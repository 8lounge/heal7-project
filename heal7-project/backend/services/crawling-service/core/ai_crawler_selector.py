#!/usr/bin/env python3
"""
ğŸ§  AI ê¸°ë°˜ í¬ë¡¤ëŸ¬ ì„ íƒ ì—”ì§„ (Gemini 2.0 í†µí•©)
URL ë¶„ì„ì„ í†µí•œ ìµœì  í¬ë¡¤ë§ ë„êµ¬ ìë™ ì„ íƒ

ğŸ¯ í•µì‹¬ ê¸°ëŠ¥:
- URL íŒ¨í„´ ë° ì½˜í…ì¸  íƒ€ì… AI ë¶„ì„  
- 3ë‹¨ê³„ ë„êµ¬ ì²´ê³„ ìµœì í™” (httpx â†’ httpx+beautifulsoup â†’ playwright)
- ì‹¤ì‹œê°„ í’ˆì§ˆ í‰ê°€ ë° ë„êµ¬ ì „í™˜
- í•™ìŠµ ê¸°ë°˜ ì„±ëŠ¥ ìµœì í™”

Author: HEAL7 Development Team  
Version: 1.0.0 (Gemini 2.0 Integration)
Date: 2025-09-03
"""

import asyncio
import json
import logging
import time
import re
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
from urllib.parse import urlparse, urljoin

# AI ë¶„ì„ê¸° import
try:
    from ..multimodal.ai_analyzer import MultimodalAnalyzer, AIModel
except ImportError:
    # í´ë°± êµ¬í˜„
    class AIModel:
        GEMINI_FLASH = "gemini_flash"
    
    class MultimodalAnalyzer:
        async def analyze_text(self, text, model, analysis_type):
            return {"success": False, "error": "AI ë¶„ì„ê¸° ë¯¸ì„¤ì¹˜"}

from .crawlers.base_crawler import CrawlerType

logger = logging.getLogger(__name__)


class CrawlComplexity(Enum):
    """í¬ë¡¤ë§ ë³µì¡ë„ ë ˆë²¨"""
    SIMPLE = "simple"       # API ë˜ëŠ” ì •ì  HTML (httpx)
    MODERATE = "moderate"   # HTML íŒŒì‹± í•„ìš” (httpx + beautifulsoup)
    COMPLEX = "complex"     # JavaScript ë Œë”ë§ í•„ìš” (playwright)


class AIConfidence(Enum):
    """AI ë¶„ì„ ì‹ ë¢°ë„"""
    HIGH = "high"       # 90%+ ì‹ ë¢°ë„
    MEDIUM = "medium"   # 70-89% ì‹ ë¢°ë„  
    LOW = "low"         # 50-69% ì‹ ë¢°ë„
    FALLBACK = "fallback"  # 50% ë¯¸ë§Œ, ê¸°ë³¸ ê·œì¹™ ì‚¬ìš©


@dataclass
class CrawlerRecommendation:
    """í¬ë¡¤ëŸ¬ ì¶”ì²œ ê²°ê³¼"""
    primary_crawler: CrawlerType
    fallback_crawler: CrawlerType
    complexity_level: CrawlComplexity
    confidence_score: float
    reasoning: str
    analysis_time_ms: float
    ai_analysis: Dict[str, Any] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "primary_crawler": self.primary_crawler.value,
            "fallback_crawler": self.fallback_crawler.value,
            "complexity_level": self.complexity_level.value,
            "confidence_score": self.confidence_score,
            "reasoning": self.reasoning,
            "analysis_time_ms": self.analysis_time_ms,
            "ai_analysis": self.ai_analysis
        }


@dataclass
class URLAnalysisContext:
    """URL ë¶„ì„ ì»¨í…ìŠ¤íŠ¸"""
    url: str
    domain: str
    path: str
    params: str
    initial_content: Optional[str] = None
    response_headers: Optional[Dict[str, str]] = None
    status_code: Optional[int] = None


class AICrawlerSelector:
    """ğŸ§  AI ê¸°ë°˜ í¬ë¡¤ëŸ¬ ì„ íƒ ì—”ì§„"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.AICrawlerSelector")
        self.ai_analyzer = None
        self._initialize_ai()
        
        # ì„±ëŠ¥ í†µê³„
        self.selection_stats = {
            "total_selections": 0,
            "ai_selections": 0,
            "fallback_selections": 0,
            "accuracy_scores": []
        }
        
        # í•™ìŠµ ë°ì´í„° (ë„ë©”ì¸ë³„ ì„±ê³µ íŒ¨í„´)
        self.learning_patterns = {}
    
    def _initialize_ai(self):
        """AI ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        try:
            self.ai_analyzer = MultimodalAnalyzer()
            self.logger.info("âœ… AI í¬ë¡¤ëŸ¬ ì„ íƒê¸° ì´ˆê¸°í™” ì™„ë£Œ (Gemini 2.0)")
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨, í´ë°± ëª¨ë“œ: {e}")
            self.ai_analyzer = None
    
    async def select_optimal_crawler(
        self, 
        url: str, 
        context: Optional[URLAnalysisContext] = None,
        use_ai: bool = True
    ) -> CrawlerRecommendation:
        """ìµœì  í¬ë¡¤ëŸ¬ ì„ íƒ (AI + íœ´ë¦¬ìŠ¤í‹± í•˜ì´ë¸Œë¦¬ë“œ)"""
        start_time = time.time()
        
        try:
            # 1ë‹¨ê³„: URL ê¸°ë³¸ ë¶„ì„
            url_context = context or self._create_url_context(url)
            
            # 2ë‹¨ê³„: AI ë¶„ì„ (í™œì„±í™”ëœ ê²½ìš°)
            ai_result = None
            if use_ai and self.ai_analyzer:
                ai_result = await self._ai_url_analysis(url_context)
            
            # 3ë‹¨ê³„: ìµœì¢… ì¶”ì²œ ê²°ì •
            recommendation = await self._make_final_recommendation(
                url_context, ai_result
            )
            
            recommendation.analysis_time_ms = (time.time() - start_time) * 1000
            
            # 4ë‹¨ê³„: í†µê³„ ì—…ë°ì´íŠ¸
            self._update_selection_stats(recommendation, ai_result is not None)
            
            self.logger.info(
                f"ğŸ¯ í¬ë¡¤ëŸ¬ ì„ íƒ ì™„ë£Œ: {recommendation.primary_crawler.value} "
                f"(ì‹ ë¢°ë„: {recommendation.confidence_score:.1f}%, "
                f"ë¶„ì„ì‹œê°„: {recommendation.analysis_time_ms:.0f}ms)"
            )
            
            return recommendation
            
        except Exception as e:
            self.logger.error(f"âŒ í¬ë¡¤ëŸ¬ ì„ íƒ ì‹¤íŒ¨: {e}")
            return self._create_fallback_recommendation(url, str(e))
    
    def _create_url_context(self, url: str) -> URLAnalysisContext:
        """URL ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        parsed = urlparse(url)
        return URLAnalysisContext(
            url=url,
            domain=parsed.netloc,
            path=parsed.path,
            params=parsed.query
        )
    
    async def _ai_url_analysis(self, context: URLAnalysisContext) -> Optional[Dict[str, Any]]:
        """AI ê¸°ë°˜ URL ë¶„ì„"""
        try:
            # Gemini Flash ëª¨ë¸ë¡œ URL íŒ¨í„´ ë¶„ì„
            analysis_prompt = f"""
ì›¹ í¬ë¡¤ë§ì„ ìœ„í•œ URL ë¶„ì„ì„ í•´ì£¼ì„¸ìš”. 3ë‹¨ê³„ ë„êµ¬ ì²´ê³„ ì¤‘ ìµœì  ì„ íƒì„ ìœ„í•´:

ğŸ¯ ë¶„ì„ ëŒ€ìƒ URL: {context.url}
- ë„ë©”ì¸: {context.domain} 
- ê²½ë¡œ: {context.path}
- íŒŒë¼ë¯¸í„°: {context.params}

ğŸ” íŒë‹¨ ê¸°ì¤€:
1. HTTPX (1ë‹¨ê³„): API ì—”ë“œí¬ì¸íŠ¸, ì •ì  íŒŒì¼, ë‹¨ìˆœ HTML
2. HTTPX + BeautifulSoup (2ë‹¨ê³„): HTML íŒŒì‹± í•„ìš”, ì •ì  ì½˜í…ì¸ 
3. Playwright (3ë‹¨ê³„): JavaScript ë Œë”ë§, ë™ì  ì½˜í…ì¸ , ë³µì¡í•œ ìƒí˜¸ì‘ìš©

ğŸ’¡ ë¶„ì„ ìš”ì²­:
- complexity_level: "simple", "moderate", "complex" ì¤‘ ì„ íƒ
- confidence_score: 0-100 ì ìˆ˜ 
- reasoning: ì„ íƒ ì´ìœ  (í•œ ë¬¸ì¥)
- requires_js: JavaScript í•„ìš” ì—¬ë¶€ (boolean)
- is_api: API ì—”ë“œí¬ì¸íŠ¸ ì—¬ë¶€ (boolean)
- content_type_prediction: ì˜ˆìƒ ì½˜í…ì¸  íƒ€ì…

JSON í˜•íƒœë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
"""
            
            result = await self.ai_analyzer.analyze_text(
                text=analysis_prompt,
                model=AIModel.GEMINI_FLASH,
                analysis_type="url_crawler_selection"
            )
            
            if result and result.get('success'):
                content = result.get('content', {})
                # JSON íŒŒì‹± ì‹œë„
                if isinstance(content, str):
                    try:
                        content = json.loads(content)
                    except json.JSONDecodeError:
                        # JSONì´ ì•„ë‹Œ ê²½ìš° í‚¤ì›Œë“œ ì¶”ì¶œ
                        content = self._extract_analysis_keywords(content)
                
                self.logger.debug(f"ğŸ¤– AI ë¶„ì„ ê²°ê³¼: {content}")
                return content
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        return None
    
    def _extract_analysis_keywords(self, text: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ì—ì„œ ë¶„ì„ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        analysis = {
            "complexity_level": "moderate",
            "confidence_score": 60.0,
            "reasoning": "í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„",
            "requires_js": False,
            "is_api": False
        }
        
        text_lower = text.lower()
        
        # ë³µì¡ë„ íŒë‹¨
        if any(word in text_lower for word in ['api', 'json', 'simple', 'static']):
            analysis["complexity_level"] = "simple"
            analysis["confidence_score"] = 75.0
        elif any(word in text_lower for word in ['javascript', 'dynamic', 'complex', 'spa']):
            analysis["complexity_level"] = "complex"
            analysis["requires_js"] = True
            analysis["confidence_score"] = 80.0
        
        # API íŒë‹¨
        if any(word in text_lower for word in ['api', 'rest', 'json', 'graphql']):
            analysis["is_api"] = True
            
        return analysis
    
    async def _make_final_recommendation(
        self, 
        context: URLAnalysisContext,
        ai_result: Optional[Dict[str, Any]]
    ) -> CrawlerRecommendation:
        """ìµœì¢… í¬ë¡¤ëŸ¬ ì¶”ì²œ ê²°ì •"""
        
        # AI ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°
        if ai_result:
            return self._ai_based_recommendation(context, ai_result)
        
        # íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ í´ë°±
        return self._heuristic_based_recommendation(context)
    
    def _ai_based_recommendation(
        self, 
        context: URLAnalysisContext, 
        ai_result: Dict[str, Any]
    ) -> CrawlerRecommendation:
        """AI ê²°ê³¼ ê¸°ë°˜ ì¶”ì²œ"""
        
        complexity = ai_result.get("complexity_level", "moderate")
        confidence = ai_result.get("confidence_score", 60.0)
        reasoning = ai_result.get("reasoning", "AI ê¸°ë°˜ ë¶„ì„")
        requires_js = ai_result.get("requires_js", False)
        is_api = ai_result.get("is_api", False)
        
        # í¬ë¡¤ëŸ¬ ì„ íƒ ë¡œì§
        if is_api or complexity == "simple":
            primary = CrawlerType.HTTPX
            fallback = CrawlerType.PLAYWRIGHT
            complexity_level = CrawlComplexity.SIMPLE
        elif requires_js or complexity == "complex":
            primary = CrawlerType.PLAYWRIGHT
            fallback = CrawlerType.HTTPX
            complexity_level = CrawlComplexity.COMPLEX
        else:  # moderate
            primary = CrawlerType.HTTPX  # BeautifulSoupëŠ” HTTPXì— í¬í•¨ë¨
            fallback = CrawlerType.PLAYWRIGHT
            complexity_level = CrawlComplexity.MODERATE
        
        return CrawlerRecommendation(
            primary_crawler=primary,
            fallback_crawler=fallback,
            complexity_level=complexity_level,
            confidence_score=confidence,
            reasoning=f"AI ë¶„ì„: {reasoning}",
            analysis_time_ms=0,  # ë‚˜ì¤‘ì— ì„¤ì •ë¨
            ai_analysis=ai_result
        )
    
    def _heuristic_based_recommendation(
        self, 
        context: URLAnalysisContext
    ) -> CrawlerRecommendation:
        """íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ì¶”ì²œ (AI í´ë°±)"""
        
        url_lower = context.url.lower()
        domain_lower = context.domain.lower()
        path_lower = context.path.lower()
        
        # API íŒ¨í„´ ê°ì§€
        if any(pattern in url_lower for pattern in [
            'api', '.json', '/rest/', '/graphql', '/v1/', '/v2/'
        ]):
            return CrawlerRecommendation(
                primary_crawler=CrawlerType.HTTPX,
                fallback_crawler=CrawlerType.PLAYWRIGHT,
                complexity_level=CrawlComplexity.SIMPLE,
                confidence_score=85.0,
                reasoning="API ì—”ë“œí¬ì¸íŠ¸ íŒ¨í„´ ê°ì§€",
                analysis_time_ms=0
            )
        
        # ì •ë¶€ ì‚¬ì´íŠ¸ (ë³µì¡í•¨)
        if '.go.kr' in domain_lower or '.gov' in domain_lower:
            return CrawlerRecommendation(
                primary_crawler=CrawlerType.PLAYWRIGHT,
                fallback_crawler=CrawlerType.HTTPX,
                complexity_level=CrawlComplexity.COMPLEX,
                confidence_score=80.0,
                reasoning="ì •ë¶€ ì‚¬ì´íŠ¸ - JavaScript ë Œë”ë§ í•„ìš”",
                analysis_time_ms=0
            )
        
        # ë™ì  ì½˜í…ì¸  íŒ¨í„´
        if any(pattern in url_lower for pattern in [
            'spa', 'app', 'react', 'vue', 'angular', 'ajax'
        ]):
            return CrawlerRecommendation(
                primary_crawler=CrawlerType.PLAYWRIGHT,
                fallback_crawler=CrawlerType.HTTPX,
                complexity_level=CrawlComplexity.COMPLEX,
                confidence_score=75.0,
                reasoning="ë™ì  ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ íŒ¨í„´ ê°ì§€",
                analysis_time_ms=0
            )
        
        # ê¸°ë³¸ê°’: ì¤‘ê°„ ë³µì¡ë„
        return CrawlerRecommendation(
            primary_crawler=CrawlerType.HTTPX,  # BeautifulSoup í¬í•¨
            fallback_crawler=CrawlerType.PLAYWRIGHT,
            complexity_level=CrawlComplexity.MODERATE,
            confidence_score=65.0,
            reasoning="íœ´ë¦¬ìŠ¤í‹± ê¸°ë³¸ ë¶„ì„ - HTML íŒŒì‹± ì¤‘ì‹¬",
            analysis_time_ms=0
        )
    
    def _create_fallback_recommendation(self, url: str, error: str) -> CrawlerRecommendation:
        """ì—ëŸ¬ ì‹œ í´ë°± ì¶”ì²œ"""
        return CrawlerRecommendation(
            primary_crawler=CrawlerType.HTTPX,
            fallback_crawler=CrawlerType.PLAYWRIGHT,
            complexity_level=CrawlComplexity.MODERATE,
            confidence_score=30.0,
            reasoning=f"ë¶„ì„ ì˜¤ë¥˜ë¡œ ì¸í•œ ê¸°ë³¸ê°’ ì ìš©: {error}",
            analysis_time_ms=0
        )
    
    def _update_selection_stats(self, recommendation: CrawlerRecommendation, used_ai: bool):
        """ì„ íƒ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.selection_stats["total_selections"] += 1
        if used_ai:
            self.selection_stats["ai_selections"] += 1
        else:
            self.selection_stats["fallback_selections"] += 1
        
        self.selection_stats["accuracy_scores"].append(recommendation.confidence_score)
        
        # ìµœê·¼ 100ê°œ ì ìˆ˜ë§Œ ìœ ì§€
        if len(self.selection_stats["accuracy_scores"]) > 100:
            self.selection_stats["accuracy_scores"] = self.selection_stats["accuracy_scores"][-100:]
    
    async def evaluate_crawler_performance(
        self,
        url: str,
        recommended_crawler: CrawlerType,
        actual_result: Dict[str, Any]
    ) -> float:
        """í¬ë¡¤ëŸ¬ ì„±ëŠ¥ í‰ê°€ ë° í•™ìŠµ"""
        try:
            success = actual_result.get('success', False)
            response_time = actual_result.get('response_time', 0)
            content_length = len(actual_result.get('html', ''))
            
            # ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚° (0-100)
            score = 0.0
            if success:
                score += 50.0  # ê¸°ë³¸ ì„±ê³µ ì ìˆ˜
                
                # ì‘ë‹µ ì‹œê°„ í‰ê°€ (ë¹ ë¥¼ìˆ˜ë¡ ì¢‹ìŒ)
                if response_time < 2.0:
                    score += 25.0
                elif response_time < 5.0:
                    score += 15.0
                elif response_time < 10.0:
                    score += 5.0
                
                # ì½˜í…ì¸  í’ˆì§ˆ í‰ê°€
                if content_length > 1000:
                    score += 25.0
                elif content_length > 100:
                    score += 15.0
                elif content_length > 0:
                    score += 5.0
            
            # ë„ë©”ì¸ë³„ í•™ìŠµ íŒ¨í„´ ì—…ë°ì´íŠ¸
            domain = urlparse(url).netloc
            if domain not in self.learning_patterns:
                self.learning_patterns[domain] = {
                    'crawler_scores': {},
                    'total_attempts': 0,
                    'successful_attempts': 0
                }
            
            pattern = self.learning_patterns[domain]
            crawler_key = recommended_crawler.value
            
            if crawler_key not in pattern['crawler_scores']:
                pattern['crawler_scores'][crawler_key] = []
            
            pattern['crawler_scores'][crawler_key].append(score)
            pattern['total_attempts'] += 1
            if success:
                pattern['successful_attempts'] += 1
            
            # ìµœê·¼ 10ê°œ ì ìˆ˜ë§Œ ìœ ì§€
            if len(pattern['crawler_scores'][crawler_key]) > 10:
                pattern['crawler_scores'][crawler_key] = pattern['crawler_scores'][crawler_key][-10:]
            
            self.logger.debug(f"ğŸ“Š í¬ë¡¤ëŸ¬ ì„±ëŠ¥ í‰ê°€: {crawler_key} â†’ {score:.1f}ì ")
            return score
            
        except Exception as e:
            self.logger.error(f"âŒ ì„±ëŠ¥ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.0
    
    def get_selection_stats(self) -> Dict[str, Any]:
        """ì„ íƒ í†µê³„ ì¡°íšŒ"""
        accuracy_scores = self.selection_stats["accuracy_scores"]
        avg_accuracy = sum(accuracy_scores) / len(accuracy_scores) if accuracy_scores else 0.0
        
        return {
            "total_selections": self.selection_stats["total_selections"],
            "ai_selections": self.selection_stats["ai_selections"],
            "fallback_selections": self.selection_stats["fallback_selections"],
            "ai_usage_rate": (
                self.selection_stats["ai_selections"] / max(1, self.selection_stats["total_selections"]) * 100
            ),
            "average_accuracy": avg_accuracy,
            "learning_domains": len(self.learning_patterns),
            "ai_analyzer_status": "active" if self.ai_analyzer else "inactive"
        }


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_ai_selector = None

async def get_ai_crawler_selector() -> AICrawlerSelector:
    """AI í¬ë¡¤ëŸ¬ ì„ íƒê¸° ì¸ìŠ¤í„´ìŠ¤ ì¡°íšŒ"""
    global _ai_selector
    if _ai_selector is None:
        _ai_selector = AICrawlerSelector()
    return _ai_selector


# í¸ì˜ í•¨ìˆ˜ë“¤
async def select_crawler_for_url(url: str, use_ai: bool = True) -> CrawlerRecommendation:
    """URLì— ìµœì í™”ëœ í¬ë¡¤ëŸ¬ ì„ íƒ"""
    selector = await get_ai_crawler_selector()
    return await selector.select_optimal_crawler(url, use_ai=use_ai)


async def evaluate_crawl_result(
    url: str, 
    crawler: CrawlerType, 
    result: Dict[str, Any]
) -> float:
    """í¬ë¡¤ë§ ê²°ê³¼ í‰ê°€ ë° í•™ìŠµ"""
    selector = await get_ai_crawler_selector()
    return await selector.evaluate_crawler_performance(url, crawler, result)