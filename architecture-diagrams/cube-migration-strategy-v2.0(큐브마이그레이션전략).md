# íë¸Œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ v2.0 ğŸ”„ğŸ¯
> **ê¸°ì¡´ HEAL7 ì‹œìŠ¤í…œì„ íë¸Œ ëª¨ë“ˆëŸ¬ ì•„í‚¤í…ì²˜ë¡œ ì ì§„ì  ì „í™˜í•˜ëŠ” ë‹¨ê³„ë³„ ì „ëµ**
> 
> **ë¬¸ì„œ ë²„ì „**: v2.0 | **ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-08-20 | **ë‹´ë‹¹**: HEAL7 ì•„í‚¤í…ì²˜íŒ€

---

## ğŸ“‹ **ë¬¸ì„œ ê°œìš”**

### **ëª©ì **
- í˜„ì¬ HEAL7 ì‹œìŠ¤í…œì„ íë¸Œ ëª¨ë“ˆëŸ¬ ì•„í‚¤í…ì²˜ë¡œ ì•ˆì „í•˜ê²Œ ì „í™˜
- ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ì—†ëŠ” ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ ìˆ˜ë¦½
- ë¦¬ìŠ¤í¬ ìµœì†Œí™”ì™€ ë¹„ì¦ˆë‹ˆìŠ¤ ì—°ì†ì„± ë³´ì¥
- ë§ˆì´ê·¸ë ˆì´ì…˜ ê³¼ì •ì˜ ì„±ê³¼ ì¸¡ì • ë° ê²€ì¦ ë°©ì•ˆ ì œì‹œ

### **ë²”ìœ„**
- **ì „í™˜ ëŒ€ìƒ**: ì‚¬ì£¼ ì‹œìŠ¤í…œ, AI ë¶„ì„, í˜ì´í¼ì›Œí¬, í¬ë¡¤ëŸ¬, í”„ë¡ íŠ¸ì—”ë“œ
- **ë§ˆì´ê·¸ë ˆì´ì…˜ ê¸°ê°„**: 12ê°œì›” (4ë‹¨ê³„ Ã— 3ê°œì›”)
- **ìš´ì˜ ëª¨ë“œ**: Blue-Green ë°°í¬, Canary ë¦´ë¦¬ìŠ¤, Feature Flag

---

## ğŸ” **í˜„ì¬ ì‹œìŠ¤í…œ ë¶„ì„ (As-Is)**

### **ğŸ—ï¸ í˜„ì¬ ì•„í‚¤í…ì²˜ ìƒíƒœ**

```
ğŸ“Š HEAL7 í˜„ì¬ ì‹œìŠ¤í…œ êµ¬ì¡° (2025-08-20 ê¸°ì¤€)
â”œâ”€â”€ ğŸ  ë¡œì»¬ ì„œë²„ (í†µí•© í”„ë¡ íŠ¸ì—”ë“œ ì¤‘ì‹¬)
â”‚   â”œâ”€â”€ ğŸŒ Next.js Frontend (í¬íŠ¸ 3000) âœ… ìš´ì˜ ì¤‘
â”‚   â”œâ”€â”€ ğŸ”® ì‚¬ì£¼ ì„œë¹„ìŠ¤ (FastAPI 8001) âš ï¸ ë¶„ì‚° ìƒíƒœ
â”‚   â”œâ”€â”€ ğŸ§  AI ë¶„ì„ (FastAPI 8002) âš ï¸ ë ˆê±°ì‹œ ìœ„ì¹˜
â”‚   â””â”€â”€ ğŸ“„ Paperwork AI (í¬íŠ¸ 8002) âš ï¸ ì•„ì¹´ì´ë¸Œ ìœ„ì¹˜
â”‚
â”œâ”€â”€ ğŸ¢ ì›ê²© ì„œë²„ (ë„ë©”ì¸ ê¸°ë°˜ ì„œë¹„ìŠ¤)
â”‚   â”œâ”€â”€ ğŸŒ heal7.com (FastAPI 8000) âœ… ë©”ì¸ ì„œë¹„ìŠ¤
â”‚   â”œâ”€â”€ ğŸ‘‘ admin.heal7.com (FastAPI 8001) âœ… ê´€ë¦¬ì
â”‚   â””â”€â”€ ğŸ” keywords.heal7.com âœ… í‚¤ì›Œë“œ ì„œë¹„ìŠ¤
â”‚
â””â”€â”€ ğŸ—„ï¸ ê³µí†µ ì¸í”„ë¼
    â”œâ”€â”€ ğŸ˜ PostgreSQL 16 âœ… í†µí•© DB
    â”œâ”€â”€ âš¡ Redis âœ… ìºì‹œ
    â””â”€â”€ ğŸŒ Nginx âœ… ë¦¬ë²„ìŠ¤ í”„ë¡ì‹œ
```

### **ğŸ” ë¬¸ì œì  ë¶„ì„**

| ì˜ì—­ | ë¬¸ì œì  | ì˜í–¥ë„ | ìš°ì„ ìˆœìœ„ |
|------|--------|--------|----------|
| **ì•„í‚¤í…ì²˜** | ëª¨ë†€ë¦¬ì‹ êµ¬ì¡°, ì„œë¹„ìŠ¤ ê°„ ê°•ê²°í•© | ğŸ”´ ë†’ìŒ | P1 |
| **ë°°í¬** | ìˆ˜ë™ ë°°í¬, ì˜ì¡´ì„± ë³µì¡ì„± | ğŸŸ¡ ì¤‘ê°„ | P2 |
| **í™•ì¥ì„±** | ê°œë³„ í™•ì¥ ë¶ˆê°€, ë¦¬ì†ŒìŠ¤ ë¹„íš¨ìœ¨ | ğŸ”´ ë†’ìŒ | P1 |
| **ëª¨ë‹ˆí„°ë§** | í†µí•© ëª¨ë‹ˆí„°ë§ ë¶€ì¬ | ğŸŸ¡ ì¤‘ê°„ | P3 |
| **ë³´ì•ˆ** | ì„œë¹„ìŠ¤ë³„ ë³´ì•ˆ ì •ì±… ë¶ˆì¼ì¹˜ | ğŸŸ  ì¤‘ìƒ | P2 |

### **ğŸ“Š í˜„ì¬ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ìŠ¤**

```python
# í˜„ì¬ ì‹œìŠ¤í…œ ì„±ëŠ¥ ì§€í‘œ (ë² ì´ìŠ¤ë¼ì¸)
CURRENT_SYSTEM_METRICS = {
    "performance": {
        "average_response_time": "2.5s",
        "requests_per_second": 50,
        "error_rate": "2.3%",
        "uptime": "99.2%"
    },
    
    "resources": {
        "cpu_utilization": "65%",
        "memory_usage": "78%",
        "storage_usage": "42%",
        "network_bandwidth": "45 Mbps"
    },
    
    "operations": {
        "deployment_frequency": "weekly",
        "mean_time_to_recovery": "45 minutes",
        "change_failure_rate": "8%",
        "lead_time": "3 days"
    },
    
    "business": {
        "daily_active_users": 1200,
        "transaction_volume": 5000,
        "service_availability": "99.2%",
        "customer_satisfaction": 4.2
    }
}
```

---

## ğŸ¯ **íë¸Œ ë§ˆì´ê·¸ë ˆì´ì…˜ ë¡œë“œë§µ**

### **ğŸ—“ï¸ 4ë‹¨ê³„ ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš (12ê°œì›”)**

```mermaid
gantt
    title HEAL7 íë¸Œ ë§ˆì´ê·¸ë ˆì´ì…˜ ë¡œë“œë§µ
    dateFormat  YYYY-MM-DD
    section Phase 1: ê¸°ë°˜ êµ¬ì¶•
    ì¸í”„ë¼ íë¸Œ êµ¬ì¶•     :done, phase1a, 2025-08-20, 1M
    CI/CD íŒŒì´í”„ë¼ì¸    :done, phase1b, 2025-09-01, 1M
    ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ      :active, phase1c, 2025-09-15, 2w
    
    section Phase 2: í•µì‹¬ ì„œë¹„ìŠ¤
    ì‚¬ì£¼ ì—”ì§„ íë¸Œ       :phase2a, 2025-11-01, 1M
    AI ë¶„ì„ íë¸Œ        :phase2b, 2025-11-15, 1M
    ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜   :phase2c, 2025-12-01, 2w
    
    section Phase 3: ì§€ì› ì„œë¹„ìŠ¤
    í˜ì´í¼ì›Œí¬ íë¸Œ      :phase3a, 2026-02-01, 3w
    í¬ë¡¤ëŸ¬ íë¸Œ         :phase3b, 2026-02-15, 3w
    í”„ë¡ íŠ¸ì—”ë“œ í†µí•©      :phase3c, 2026-03-01, 2w
    
    section Phase 4: ìµœì í™”
    ì„±ëŠ¥ ìµœì í™”         :phase4a, 2026-05-01, 1M
    ìë™í™” ì™„ì„±         :phase4b, 2026-05-15, 1M
    ì‹œìŠ¤í…œ ì•ˆì •í™”       :phase4c, 2026-06-01, 2w
```

---

## ğŸš€ **Phase 1: ê¸°ë°˜ êµ¬ì¶• (1-3ê°œì›”)**

### **ğŸ¯ Phase 1 ëª©í‘œ**
- íë¸Œ ì•„í‚¤í…ì²˜ ê¸°ë°˜ ì¸í”„ë¼ êµ¬ì¶•
- CI/CD íŒŒì´í”„ë¼ì¸ ìë™í™”
- ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹… ì‹œìŠ¤í…œ êµ¬ì¶•
- ì„œë¹„ìŠ¤ ë©”ì‹œ êµ¬ì„±

### **ğŸ“¦ êµ¬ì¶•í•  ì¸í”„ë¼ íë¸Œë“¤**

#### **ğŸ”— 1.1 API Gateway Cube**
```yaml
# api-gateway-cube.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: api-gateway-config
data:
  nginx.conf: |
    upstream saju-service {
      server saju-cube:8001;
    }
    upstream ai-service {
      server ai-cube:8002;
    }
    upstream paperwork-service {
      server paperwork-cube:8003;
    }
    
    server {
      listen 80;
      location /api/saju/ {
        proxy_pass http://saju-service/;
        proxy_set_header X-Cube-Source "api-gateway";
      }
      location /api/ai/ {
        proxy_pass http://ai-service/;
        proxy_set_header X-Cube-Source "api-gateway";
      }
      location /api/paperwork/ {
        proxy_pass http://paperwork-service/;
        proxy_set_header X-Cube-Source "api-gateway";
      }
    }
```

#### **ğŸ“Š 1.2 Monitoring Cube**
```python
# monitoring-cube/prometheus-config.py
MONITORING_CONFIG = {
    "prometheus": {
        "scrape_interval": "15s",
        "evaluation_interval": "15s",
        "rule_files": ["cube_rules.yml"],
        "scrape_configs": [
            {
                "job_name": "saju-cube",
                "static_configs": [{"targets": ["saju-cube:8001"]}]
            },
            {
                "job_name": "ai-cube", 
                "static_configs": [{"targets": ["ai-cube:8002"]}]
            },
            {
                "job_name": "paperwork-cube",
                "static_configs": [{"targets": ["paperwork-cube:8003"]}]
            }
        ]
    },
    
    "grafana": {
        "dashboards": [
            "cube_performance_dashboard.json",
            "cube_health_dashboard.json",
            "cube_business_metrics_dashboard.json"
        ]
    },
    
    "alertmanager": {
        "routes": [
            {
                "match": {"cube_type": "core_service"},
                "receiver": "critical_alerts"
            },
            {
                "match": {"cube_type": "support_service"},
                "receiver": "warning_alerts"
            }
        ]
    }
}
```

#### **ğŸ” 1.3 Security Cube**
```python
# security-cube/auth-service.py
class CubeAuthService:
    """íë¸Œ ê°„ ì¸ì¦ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.jwt_secret = os.getenv("CUBE_JWT_SECRET")
        self.token_expiry = 3600  # 1ì‹œê°„
        
    def generate_cube_token(self, cube_id: str, permissions: List[str]) -> str:
        """íë¸Œ ê°„ í†µì‹ ìš© í† í° ìƒì„±"""
        payload = {
            "cube_id": cube_id,
            "permissions": permissions,
            "exp": datetime.utcnow() + timedelta(seconds=self.token_expiry),
            "iat": datetime.utcnow(),
            "iss": "heal7-security-cube"
        }
        return jwt.encode(payload, self.jwt_secret, algorithm="HS256")
    
    def validate_cube_token(self, token: str) -> dict:
        """íë¸Œ í† í° ê²€ì¦"""
        try:
            payload = jwt.decode(token, self.jwt_secret, algorithms=["HS256"])
            return {"valid": True, "cube_id": payload["cube_id"], "permissions": payload["permissions"]}
        except jwt.ExpiredSignatureError:
            return {"valid": False, "error": "Token expired"}
        except jwt.InvalidTokenError:
            return {"valid": False, "error": "Invalid token"}
```

### **ğŸ”„ Phase 1 ì‹¤í–‰ ë‹¨ê³„**

#### **Week 1-2: ê°œë°œ í™˜ê²½ êµ¬ì„±**
```bash
# 1. íë¸Œ ê°œë°œ í™˜ê²½ ì„¤ì •
mkdir -p /heal7-project/cubes/{infrastructure,core,interface,data}

# 2. Docker í™˜ê²½ êµ¬ì„±
cat > docker-compose.cubes-dev.yml << 'EOF'
version: '3.8'
services:
  api-gateway-cube:
    build: ./infrastructure/api-gateway/
    ports: ["80:80"]
    depends_on: [monitoring-cube, security-cube]
    
  monitoring-cube:
    build: ./infrastructure/monitoring/
    ports: ["9090:9090", "3000:3000"]
    volumes:
      - prometheus-data:/prometheus
      - grafana-data:/grafana
      
  security-cube:
    build: ./infrastructure/security/
    ports: ["8080:8080"]
    environment:
      - CUBE_JWT_SECRET=${CUBE_JWT_SECRET}
      
volumes:
  prometheus-data:
  grafana-data:
EOF

# 3. íë¸Œ ë¹Œë“œ ë° ì‹¤í–‰
docker-compose -f docker-compose.cubes-dev.yml up -d
```

#### **Week 3-4: CI/CD íŒŒì´í”„ë¼ì¸ êµ¬ì¶•**
```yaml
# .github/workflows/cube-deployment.yml
name: Cube Deployment Pipeline

on:
  push:
    branches: [main, develop]
    paths: ['cubes/**']

jobs:
  test-cubes:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Test Cube Interfaces
        run: |
          # íë¸Œ ì¸í„°í˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸
          python -m pytest cubes/tests/ -v
          
      - name: Validate Cube Configs
        run: |
          # íë¸Œ ì„¤ì • ê²€ì¦
          python cubes/tools/config-validator.py
  
  build-cubes:
    needs: test-cubes
    runs-on: ubuntu-latest
    strategy:
      matrix:
        cube: [api-gateway, monitoring, security]
    steps:
      - uses: actions/checkout@v3
      - name: Build Cube Image
        run: |
          docker build -t heal7/${{ matrix.cube }}-cube:${{ github.sha }} \
            cubes/infrastructure/${{ matrix.cube }}/
          
      - name: Push to Registry
        run: |
          echo ${{ secrets.DOCKER_PASSWORD }} | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin
          docker push heal7/${{ matrix.cube }}-cube:${{ github.sha }}
  
  deploy-cubes:
    needs: build-cubes
    runs-on: ubuntu-latest
    environment: production
    steps:
      - name: Deploy to Kubernetes
        run: |
          kubectl set image deployment/${{ matrix.cube }}-cube \
            ${{ matrix.cube }}-cube=heal7/${{ matrix.cube }}-cube:${{ github.sha }}
```

#### **Week 5-8: ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶•**
```python
# monitoring-cube/cube-metrics.py
class CubeMetricsCollector:
    """íë¸Œë³„ ë©”íŠ¸ë¦­ìŠ¤ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        self.prometheus_client = PrometheusClient()
        self.cube_registry = {}
        
    def register_cube(self, cube_id: str, cube_config: dict):
        """íë¸Œ ë“±ë¡ ë° ë©”íŠ¸ë¦­ìŠ¤ ì„¤ì •"""
        self.cube_registry[cube_id] = {
            "config": cube_config,
            "metrics": self.init_cube_metrics(cube_id),
            "health_check_url": f"http://{cube_id}:8080/health"
        }
        
    def init_cube_metrics(self, cube_id: str) -> dict:
        """íë¸Œë³„ ë©”íŠ¸ë¦­ìŠ¤ ì´ˆê¸°í™”"""
        return {
            "request_count": Counter(
                f"{cube_id}_requests_total",
                "Total requests to cube",
                ["method", "endpoint", "status"]
            ),
            "request_duration": Histogram(
                f"{cube_id}_request_duration_seconds",
                "Request duration in seconds"
            ),
            "cpu_usage": Gauge(
                f"{cube_id}_cpu_usage_percent",
                "CPU usage percentage"
            ),
            "memory_usage": Gauge(
                f"{cube_id}_memory_usage_bytes",
                "Memory usage in bytes"
            )
        }
    
    async def collect_all_metrics(self):
        """ëª¨ë“  íë¸Œ ë©”íŠ¸ë¦­ìŠ¤ ìˆ˜ì§‘"""
        for cube_id, cube_info in self.cube_registry.items():
            try:
                # Health Check
                health_status = await self.check_cube_health(cube_id)
                
                # Performance Metrics
                performance_data = await self.get_cube_performance(cube_id)
                
                # Update Prometheus metrics
                self.update_prometheus_metrics(cube_id, health_status, performance_data)
                
            except Exception as e:
                logger.error(f"Failed to collect metrics for {cube_id}: {e}")
```

### **ğŸ“Š Phase 1 ì„±ê³µ ì§€í‘œ**

| ì§€í‘œ | ëª©í‘œê°’ | ì¸¡ì • ë°©ë²• |
|------|--------|-----------|
| **ì¸í”„ë¼ ê°€ìš©ì„±** | 99.9% | íë¸Œ Health Check |
| **ë°°í¬ ìë™í™”** | 100% | CI/CD ì„±ê³µë¥  |
| **ëª¨ë‹ˆí„°ë§ ì»¤ë²„ë¦¬ì§€** | 100% | ë©”íŠ¸ë¦­ìŠ¤ ìˆ˜ì§‘ë¥  |
| **ì‘ë‹µ ì‹œê°„** | <1s | API Gateway ë©”íŠ¸ë¦­ìŠ¤ |

---

## ğŸ”® **Phase 2: í•µì‹¬ ì„œë¹„ìŠ¤ íë¸Œí™” (4-6ê°œì›”)**

### **ğŸ¯ Phase 2 ëª©í‘œ**
- ì‚¬ì£¼ ì—”ì§„ê³¼ AI ë¶„ì„ ì„œë¹„ìŠ¤ íë¸Œ ì „í™˜
- ê¸°ì¡´ ì„œë¹„ìŠ¤ì™€ ë³‘ë ¬ ìš´ì˜ (Blue-Green)
- ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ë° ë™ê¸°í™”
- ì‚¬ìš©ì ì˜í–¥ ìµœì†Œí™”

### **ğŸ”„ íë¸Œ ì „í™˜ ì „ëµ: Strangler Fig Pattern**

```mermaid
graph LR
    A[ê¸°ì¡´ ì‚¬ì£¼ ì„œë¹„ìŠ¤] --> C[API Gateway]
    B[ìƒˆ ì‚¬ì£¼ íë¸Œ] --> C
    C --> D[Feature Flag]
    D --> E[ì‚¬ìš©ì íŠ¸ë˜í”½]
    
    F[ê¸°ì¡´ AI ì„œë¹„ìŠ¤] --> C
    G[ìƒˆ AI íë¸Œ] --> C
```

### **ğŸ”® 2.1 ì‚¬ì£¼ ì—”ì§„ íë¸Œ ì „í™˜**

#### **ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ**
```python
# migration/saju-data-migration.py
class SajuDataMigration:
    """ì‚¬ì£¼ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.legacy_db = LegacyDatabase()
        self.cube_db = CubeDatabase()
        self.migration_log = MigrationLogger()
        
    async def migrate_saju_calculations(self, batch_size: int = 1000):
        """ì‚¬ì£¼ ê³„ì‚° ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜"""
        total_records = await self.legacy_db.count_saju_records()
        migrated = 0
        
        for offset in range(0, total_records, batch_size):
            batch = await self.legacy_db.get_saju_batch(offset, batch_size)
            
            for record in batch:
                try:
                    # ë°ì´í„° ë³€í™˜
                    cube_record = self.transform_saju_record(record)
                    
                    # íë¸Œ DBì— ì €ì¥
                    await self.cube_db.insert_saju_record(cube_record)
                    
                    # ê²€ì¦
                    if await self.validate_migrated_record(record, cube_record):
                        migrated += 1
                        await self.migration_log.log_success(record.id)
                    else:
                        await self.migration_log.log_failure(record.id, "Validation failed")
                        
                except Exception as e:
                    await self.migration_log.log_failure(record.id, str(e))
                    
            logger.info(f"Migrated {migrated}/{total_records} records")
    
    def transform_saju_record(self, legacy_record: dict) -> dict:
        """ë ˆê±°ì‹œ ë°ì´í„°ë¥¼ íë¸Œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        return {
            "user_id": legacy_record["user_id"],
            "birth_info": {
                "year": legacy_record["birth_year"],
                "month": legacy_record["birth_month"], 
                "day": legacy_record["birth_day"],
                "hour": legacy_record["birth_hour"],
                "solar_calendar": legacy_record["is_solar"]
            },
            "saju_chart": {
                "year_pillar": legacy_record["year_pillar"],
                "month_pillar": legacy_record["month_pillar"],
                "day_pillar": legacy_record["day_pillar"],
                "hour_pillar": legacy_record["hour_pillar"]
            },
            "interpretation": legacy_record["interpretation"],
            "created_at": legacy_record["created_at"],
            "updated_at": datetime.utcnow()
        }
```

#### **Feature Flag ê¸°ë°˜ íŠ¸ë˜í”½ ì „í™˜**
```python
# feature-flags/saju-cube-flag.py
class SajuCubeFeatureFlag:
    """ì‚¬ì£¼ íë¸Œ ì ì§„ì  ì „í™˜ì„ ìœ„í•œ Feature Flag"""
    
    def __init__(self):
        self.redis_client = redis.Redis()
        self.flag_key = "saju_cube_enabled"
        self.user_percentage_key = "saju_cube_user_percentage"
        
    def is_saju_cube_enabled_for_user(self, user_id: str) -> bool:
        """íŠ¹ì • ì‚¬ìš©ìì— ëŒ€í•´ ì‚¬ì£¼ íë¸Œ í™œì„±í™” ì—¬ë¶€ í™•ì¸"""
        # ì „ì²´ í™œì„±í™” í”Œë˜ê·¸ í™•ì¸
        global_enabled = self.redis_client.get(self.flag_key)
        if global_enabled == "false":
            return False
            
        # ì‚¬ìš©ì ë¹„ìœ¨ ê¸°ë°˜ í™œì„±í™”
        percentage = int(self.redis_client.get(self.user_percentage_key) or 0)
        user_hash = hashlib.md5(user_id.encode()).hexdigest()
        user_number = int(user_hash[:8], 16) % 100
        
        return user_number < percentage
    
    def gradually_increase_traffic(self, target_percentage: int, step: int = 10, interval: int = 3600):
        """íŠ¸ë˜í”½ì„ ì ì§„ì ìœ¼ë¡œ ì¦ê°€"""
        current_percentage = int(self.redis_client.get(self.user_percentage_key) or 0)
        
        while current_percentage < target_percentage:
            current_percentage = min(current_percentage + step, target_percentage)
            self.redis_client.set(self.user_percentage_key, current_percentage)
            
            logger.info(f"Saju cube traffic increased to {current_percentage}%")
            time.sleep(interval)
```

### **ğŸ§  2.2 AI ë¶„ì„ íë¸Œ ì „í™˜**

#### **AI ëª¨ë¸ ë§ˆì´ê·¸ë ˆì´ì…˜**
```python
# migration/ai-model-migration.py
class AIModelMigration:
    """AI ëª¨ë¸ ë° ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜"""
    
    def __init__(self):
        self.legacy_ai_service = LegacyAIService()
        self.ai_cube_service = AICubeService()
        
    async def migrate_ai_models(self):
        """AI ëª¨ë¸ íë¸Œë¡œ ì´ì „"""
        models = await self.legacy_ai_service.get_all_models()
        
        for model in models:
            try:
                # ëª¨ë¸ ê²€ì¦
                validation_result = await self.validate_model(model)
                if not validation_result.is_valid:
                    logger.warning(f"Model {model.name} validation failed: {validation_result.errors}")
                    continue
                
                # íë¸Œë¡œ ëª¨ë¸ ì´ì „
                await self.ai_cube_service.import_model(model)
                
                # ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸
                performance_diff = await self.compare_model_performance(model)
                if performance_diff.accuracy_drop > 0.05:  # 5% ì´ìƒ ì„±ëŠ¥ ì €í•˜
                    logger.error(f"Model {model.name} performance degradation: {performance_diff}")
                    await self.ai_cube_service.rollback_model(model.name)
                    continue
                
                logger.info(f"Successfully migrated model {model.name}")
                
            except Exception as e:
                logger.error(f"Failed to migrate model {model.name}: {e}")
    
    async def compare_model_performance(self, model) -> ModelPerformanceComparison:
        """ë ˆê±°ì‹œì™€ íë¸Œ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ"""
        test_dataset = await self.get_test_dataset()
        
        legacy_results = await self.legacy_ai_service.batch_predict(model.name, test_dataset)
        cube_results = await self.ai_cube_service.batch_predict(model.name, test_dataset)
        
        return ModelPerformanceComparison(
            accuracy_diff=self.calculate_accuracy_diff(legacy_results, cube_results),
            latency_diff=self.calculate_latency_diff(legacy_results, cube_results),
            throughput_diff=self.calculate_throughput_diff(legacy_results, cube_results)
        )
```

### **ğŸ“Š Phase 2 ì„±ê³µ ì§€í‘œ**

| ì§€í‘œ | ëª©í‘œê°’ | í˜„ì¬ê°’ | ë‹¬ì„±ë¥  |
|------|--------|--------|--------|
| **ì‚¬ì£¼ ê³„ì‚° ì •í™•ë„** | 99.95% | - | - |
| **AI ë¶„ì„ ì„±ëŠ¥** | ê¸°ì¡´ ëŒ€ë¹„ +10% | - | - |
| **ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜** | 100% | - | - |
| **ì„œë¹„ìŠ¤ ë‹¤ìš´íƒ€ì„** | <5ë¶„ | - | - |

---

## ğŸ“„ **Phase 3: ì§€ì› ì„œë¹„ìŠ¤ íë¸Œí™” (7-9ê°œì›”)**

### **ğŸ¯ Phase 3 ëª©í‘œ**
- í˜ì´í¼ì›Œí¬ AIì™€ í¬ë¡¤ëŸ¬ ì„œë¹„ìŠ¤ íë¸Œ ì „í™˜
- í”„ë¡ íŠ¸ì—”ë“œ í†µí•© íë¸Œ êµ¬ì„±
- ëª¨ë“  íë¸Œ ê°„ ì—°ë™ ì™„ì„±
- ì‚¬ìš©ì ê²½í—˜ í–¥ìƒ

### **ğŸ“„ 3.1 í˜ì´í¼ì›Œí¬ AI íë¸Œ ì „í™˜**

#### **íŒŒì¼ ì €ì¥ì†Œ ë§ˆì´ê·¸ë ˆì´ì…˜**
```python
# migration/paperwork-storage-migration.py
class PaperworkStorageMigration:
    """í˜ì´í¼ì›Œí¬ íŒŒì¼ ì €ì¥ì†Œ ë§ˆì´ê·¸ë ˆì´ì…˜"""
    
    def __init__(self):
        self.legacy_storage = LegacyFileStorage()
        self.cube_storage = CubeFileStorage()
        self.metadata_migrator = MetadataMigrator()
        
    async def migrate_file_storage(self):
        """íŒŒì¼ ì €ì¥ì†Œ íë¸Œ ì´ì „"""
        file_list = await self.legacy_storage.list_all_files()
        
        for file_info in file_list:
            try:
                # íŒŒì¼ ë³µì‚¬
                file_content = await self.legacy_storage.read_file(file_info.path)
                new_path = await self.cube_storage.store_file(
                    file_content, 
                    file_info.metadata
                )
                
                # ë©”íƒ€ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜
                await self.metadata_migrator.migrate_file_metadata(
                    file_info.path, 
                    new_path, 
                    file_info.metadata
                )
                
                # ì²˜ë¦¬ ê²°ê³¼ ë§ˆì´ê·¸ë ˆì´ì…˜
                processing_results = await self.legacy_storage.get_processing_results(file_info.id)
                await self.cube_storage.store_processing_results(new_path, processing_results)
                
                logger.info(f"Migrated file: {file_info.path} -> {new_path}")
                
            except Exception as e:
                logger.error(f"Failed to migrate file {file_info.path}: {e}")
```

### **ğŸ•·ï¸ 3.2 í¬ë¡¤ëŸ¬ íë¸Œ ì „í™˜**

#### **í¬ë¡¤ë§ ì‘ì—… ë§ˆì´ê·¸ë ˆì´ì…˜**
```python
# migration/crawler-jobs-migration.py
class CrawlerJobsMigration:
    """í¬ë¡¤ë§ ì‘ì—… ë° ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜"""
    
    def __init__(self):
        self.legacy_crawler = LegacyCrawler()
        self.crawler_cube = CrawlerCube()
        
    async def migrate_crawling_jobs(self):
        """í¬ë¡¤ë§ ì‘ì—… íë¸Œ ì´ì „"""
        active_jobs = await self.legacy_crawler.get_active_jobs()
        scheduled_jobs = await self.legacy_crawler.get_scheduled_jobs()
        
        # í™œì„± ì‘ì—… ì¼ì‹œ ì¤‘ì§€
        for job in active_jobs:
            await self.legacy_crawler.pause_job(job.id)
            
        # ìŠ¤ì¼€ì¤„ëœ ì‘ì—… ë§ˆì´ê·¸ë ˆì´ì…˜
        for job in scheduled_jobs:
            cube_job_config = self.convert_job_config(job)
            new_job_id = await self.crawler_cube.create_job(cube_job_config)
            
            # ì‘ì—… ë§¤í•‘ ì €ì¥
            await self.store_job_mapping(job.id, new_job_id)
            
        # í¬ë¡¤ë§ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜
        await self.migrate_crawled_data()
        
        # íë¸Œì—ì„œ ì‘ì—… ì¬ì‹œì‘
        for job in active_jobs:
            new_job_id = await self.get_mapped_job_id(job.id)
            await self.crawler_cube.start_job(new_job_id)
    
    def convert_job_config(self, legacy_job) -> dict:
        """ë ˆê±°ì‹œ ì‘ì—… ì„¤ì •ì„ íë¸Œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        return {
            "name": legacy_job.name,
            "target_urls": legacy_job.urls,
            "schedule": {
                "type": "cron",
                "expression": legacy_job.cron_expression
            },
            "extraction_rules": {
                "selectors": legacy_job.css_selectors,
                "filters": legacy_job.content_filters
            },
            "rate_limiting": {
                "requests_per_minute": legacy_job.rate_limit,
                "delay_between_requests": legacy_job.request_delay
            },
            "output_format": legacy_job.output_format
        }
```

### **ğŸŒ 3.3 í”„ë¡ íŠ¸ì—”ë“œ í†µí•© íë¸Œ**

#### **API í†µí•© ì „ëµ**
```typescript
// frontend-cube/src/services/cube-api-client.ts
class CubeAPIClient {
  private baseURL: string;
  private authToken: string;
  
  constructor() {
    this.baseURL = process.env.NEXT_PUBLIC_API_GATEWAY_URL || 'http://localhost:80';
    this.authToken = '';
  }
  
  // ğŸ”® ì‚¬ì£¼ íë¸Œ API
  async callSajuCube(endpoint: string, data: any): Promise<any> {
    return this.makeRequest(`/api/saju/${endpoint}`, data);
  }
  
  // ğŸ§  AI íë¸Œ API
  async callAICube(endpoint: string, data: any): Promise<any> {
    return this.makeRequest(`/api/ai/${endpoint}`, data);
  }
  
  // ğŸ“„ í˜ì´í¼ì›Œí¬ íë¸Œ API
  async callPaperworkCube(endpoint: string, data: any): Promise<any> {
    return this.makeRequest(`/api/paperwork/${endpoint}`, data);
  }
  
  // ğŸ•·ï¸ í¬ë¡¤ëŸ¬ íë¸Œ API
  async callCrawlerCube(endpoint: string, data: any): Promise<any> {
    return this.makeRequest(`/api/crawler/${endpoint}`, data);
  }
  
  private async makeRequest(url: string, data: any): Promise<any> {
    const response = await fetch(`${this.baseURL}${url}`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${this.authToken}`,
        'X-Cube-Client': 'frontend-cube'
      },
      body: JSON.stringify(data)
    });
    
    if (!response.ok) {
      throw new Error(`API request failed: ${response.statusText}`);
    }
    
    return await response.json();
  }
}
```

---

## ğŸš€ **Phase 4: ìµœì í™” ë° ì•ˆì •í™” (10-12ê°œì›”)**

### **ğŸ¯ Phase 4 ëª©í‘œ**
- ì „ì²´ ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™”
- ìë™í™” ì‹œìŠ¤í…œ ì™„ì„±
- ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ê³ ë„í™”
- ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ìŠ¤ ê°œì„ 

### **âš¡ 4.1 ì„±ëŠ¥ ìµœì í™”**

#### **íë¸Œ ê°„ í†µì‹  ìµœì í™”**
```python
# optimization/inter-cube-communication.py
class InterCubeOptimizer:
    """íë¸Œ ê°„ í†µì‹  ìµœì í™”"""
    
    def __init__(self):
        self.circuit_breaker = CircuitBreaker()
        self.request_cache = RequestCache()
        self.load_balancer = LoadBalancer()
        
    async def optimized_cube_call(self, target_cube: str, method: str, data: dict) -> dict:
        """ìµœì í™”ëœ íë¸Œ ê°„ í˜¸ì¶œ"""
        # 1. ìºì‹œ í™•ì¸
        cache_key = self.generate_cache_key(target_cube, method, data)
        cached_result = await self.request_cache.get(cache_key)
        if cached_result:
            return cached_result
        
        # 2. Circuit Breaker í™•ì¸
        if not self.circuit_breaker.is_call_allowed(target_cube):
            raise CubeUnavailableError(f"Circuit breaker open for {target_cube}")
        
        # 3. ë¡œë“œ ë°¸ëŸ°ì‹±
        target_instance = await self.load_balancer.get_best_instance(target_cube)
        
        try:
            # 4. ì‹¤ì œ í˜¸ì¶œ
            result = await self.make_cube_call(target_instance, method, data)
            
            # 5. ê²°ê³¼ ìºì‹±
            await self.request_cache.set(cache_key, result, ttl=300)  # 5ë¶„
            
            # 6. Circuit Breaker ì„±ê³µ ê¸°ë¡
            self.circuit_breaker.record_success(target_cube)
            
            return result
            
        except Exception as e:
            # 7. Circuit Breaker ì‹¤íŒ¨ ê¸°ë¡
            self.circuit_breaker.record_failure(target_cube)
            raise e
```

#### **ë¦¬ì†ŒìŠ¤ ìµœì í™”**
```python
# optimization/resource-optimizer.py
class CubeResourceOptimizer:
    """íë¸Œ ë¦¬ì†ŒìŠ¤ ìµœì í™”"""
    
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.auto_scaler = AutoScaler()
        
    async def optimize_cube_resources(self):
        """íë¸Œ ë¦¬ì†ŒìŠ¤ ìµœì í™”"""
        cubes = await self.get_all_cubes()
        
        for cube in cubes:
            metrics = await self.metrics_collector.get_cube_metrics(cube.id)
            
            # CPU ìµœì í™”
            if metrics.cpu_usage > 80:
                await self.auto_scaler.scale_up(cube.id, resource_type="cpu")
            elif metrics.cpu_usage < 20:
                await self.auto_scaler.scale_down(cube.id, resource_type="cpu")
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            if metrics.memory_usage > 85:
                await self.auto_scaler.scale_up(cube.id, resource_type="memory")
                
            # ì¸ìŠ¤í„´ìŠ¤ ìµœì í™”
            if metrics.request_rate > metrics.capacity * 0.8:
                await self.auto_scaler.add_instance(cube.id)
            elif metrics.request_rate < metrics.capacity * 0.2 and metrics.instance_count > 1:
                await self.auto_scaler.remove_instance(cube.id)
```

### **ğŸ¤– 4.2 ìë™í™” ì‹œìŠ¤í…œ ì™„ì„±**

#### **ìê°€ ì¹˜ìœ  ì‹œìŠ¤í…œ**
```python
# automation/self-healing.py
class SelfHealingSystem:
    """íë¸Œ ìê°€ ì¹˜ìœ  ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.health_checker = HealthChecker()
        self.failure_detector = FailureDetector()
        self.recovery_executor = RecoveryExecutor()
        
    async def monitor_and_heal(self):
        """ëª¨ë‹ˆí„°ë§ ë° ìë™ ë³µêµ¬"""
        while True:
            cubes = await self.get_all_cubes()
            
            for cube in cubes:
                try:
                    # ê±´ê°• ìƒíƒœ í™•ì¸
                    health_status = await self.health_checker.check_cube_health(cube.id)
                    
                    if not health_status.is_healthy:
                        # ì¥ì•  ë¶„ì„
                        failure_analysis = await self.failure_detector.analyze_failure(
                            cube.id, health_status
                        )
                        
                        # ë³µêµ¬ ì „ëµ ì‹¤í–‰
                        recovery_plan = self.create_recovery_plan(cube.id, failure_analysis)
                        await self.recovery_executor.execute_recovery(recovery_plan)
                        
                        logger.info(f"Self-healing completed for cube {cube.id}")
                        
                except Exception as e:
                    logger.error(f"Self-healing failed for cube {cube.id}: {e}")
                    
            await asyncio.sleep(30)  # 30ì´ˆë§ˆë‹¤ í™•ì¸
    
    def create_recovery_plan(self, cube_id: str, failure_analysis: FailureAnalysis) -> RecoveryPlan:
        """ë³µêµ¬ ê³„íš ìƒì„±"""
        plan = RecoveryPlan(cube_id=cube_id)
        
        if failure_analysis.failure_type == "memory_leak":
            plan.add_action("restart_cube")
            plan.add_action("increase_memory_limit")
            
        elif failure_analysis.failure_type == "high_cpu":
            plan.add_action("scale_out_instances")
            plan.add_action("optimize_cpu_intensive_tasks")
            
        elif failure_analysis.failure_type == "network_timeout":
            plan.add_action("check_network_connectivity")
            plan.add_action("restart_network_services")
            
        elif failure_analysis.failure_type == "database_connection":
            plan.add_action("reset_database_connections")
            plan.add_action("failover_to_backup_database")
            
        return plan
```

### **ğŸ“ˆ 4.3 ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ìŠ¤ ê°œì„ **

#### **ì„±ê³¼ ëŒ€ì‹œë³´ë“œ**
```python
# monitoring/business-metrics.py
class BusinessMetricsDashboard:
    """ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ìŠ¤ ëŒ€ì‹œë³´ë“œ"""
    
    def __init__(self):
        self.metrics_aggregator = MetricsAggregator()
        self.dashboard_updater = DashboardUpdater()
        
    async def generate_daily_report(self) -> BusinessReport:
        """ì¼ê°„ ë¹„ì¦ˆë‹ˆìŠ¤ ë¦¬í¬íŠ¸ ìƒì„±"""
        today = datetime.now().date()
        
        # ì‚¬ìš©ì ê´€ë ¨ ë©”íŠ¸ë¦­ìŠ¤
        user_metrics = await self.get_user_metrics(today)
        
        # ì„œë¹„ìŠ¤ ê´€ë ¨ ë©”íŠ¸ë¦­ìŠ¤
        service_metrics = await self.get_service_metrics(today)
        
        # íë¸Œ ì„±ëŠ¥ ë©”íŠ¸ë¦­ìŠ¤
        cube_metrics = await self.get_cube_performance_metrics(today)
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ì˜í–¥ ë¶„ì„
        business_impact = await self.analyze_business_impact(
            user_metrics, service_metrics, cube_metrics
        )
        
        return BusinessReport(
            date=today,
            user_metrics=user_metrics,
            service_metrics=service_metrics,
            cube_metrics=cube_metrics,
            business_impact=business_impact,
            recommendations=self.generate_recommendations(business_impact)
        )
    
    async def get_user_metrics(self, date: datetime.date) -> UserMetrics:
        """ì‚¬ìš©ì ê´€ë ¨ ë©”íŠ¸ë¦­ìŠ¤"""
        return UserMetrics(
            daily_active_users=await self.count_daily_active_users(date),
            new_registrations=await self.count_new_registrations(date),
            user_retention_rate=await self.calculate_retention_rate(date),
            average_session_duration=await self.calculate_avg_session_duration(date)
        )
    
    async def get_service_metrics(self, date: datetime.date) -> ServiceMetrics:
        """ì„œë¹„ìŠ¤ ê´€ë ¨ ë©”íŠ¸ë¦­ìŠ¤"""
        return ServiceMetrics(
            saju_calculations=await self.count_saju_calculations(date),
            ai_analyses=await self.count_ai_analyses(date),
            document_processing=await self.count_document_processing(date),
            crawling_jobs=await self.count_crawling_jobs(date),
            service_uptime=await self.calculate_service_uptime(date),
            average_response_time=await self.calculate_avg_response_time(date)
        )
```

---

## ğŸ“Š **ë§ˆì´ê·¸ë ˆì´ì…˜ ì„±ê³¼ ì¸¡ì •**

### **ğŸ¯ í•µì‹¬ ì„±ê³¼ ì§€í‘œ (KPIs)**

| ì˜ì—­ | ì§€í‘œ | ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ | ëª©í‘œê°’ | í˜„ì¬ê°’ |
|------|------|----------------|--------|--------|
| **ì„±ëŠ¥** | í‰ê·  ì‘ë‹µ ì‹œê°„ | 2.5s | 1.0s | - |
| **ì„±ëŠ¥** | ì²˜ë¦¬ëŸ‰ (RPS) | 50 | 200 | - |
| **ì•ˆì •ì„±** | ì„œë¹„ìŠ¤ ê°€ìš©ì„± | 99.2% | 99.9% | - |
| **ì•ˆì •ì„±** | í‰ê·  ë³µêµ¬ ì‹œê°„ | 45ë¶„ | 5ë¶„ | - |
| **í™•ì¥ì„±** | ìë™ ìŠ¤ì¼€ì¼ë§ | 0% | 100% | - |
| **í™•ì¥ì„±** | ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„± | 60% | 85% | - |
| **ìš´ì˜** | ë°°í¬ ë¹ˆë„ | ì£¼ 1íšŒ | ì¼ 3íšŒ | - |
| **ìš´ì˜** | ì¥ì• ìœ¨ | 8% | 2% | - |

### **ğŸ’° ë¹„ìš© ìµœì í™” íš¨ê³¼**

```python
# cost-analysis/migration-roi.py
class MigrationROICalculator:
    """ë§ˆì´ê·¸ë ˆì´ì…˜ ROI ê³„ì‚°ê¸°"""
    
    def calculate_migration_costs(self) -> MigrationCost:
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ë¹„ìš© ê³„ì‚°"""
        return MigrationCost(
            development_cost=150000,  # ê°œë°œ ë¹„ìš© (15ë§Œ ë‹¬ëŸ¬)
            infrastructure_cost=24000,  # ì¸í”„ë¼ ë¹„ìš© (ì—°ê°„ 2ë§Œ 4ì²œ ë‹¬ëŸ¬)
            training_cost=20000,  # êµìœ¡ ë¹„ìš© (2ë§Œ ë‹¬ëŸ¬)
            migration_period_cost=30000,  # ë§ˆì´ê·¸ë ˆì´ì…˜ ê¸°ê°„ ì¶”ê°€ ë¹„ìš©
            total_cost=224000
        )
    
    def calculate_annual_savings(self) -> AnnualSavings:
        """ì—°ê°„ ì ˆì•½ íš¨ê³¼ ê³„ì‚°"""
        return AnnualSavings(
            infrastructure_savings=60000,  # ì¸í”„ë¼ ë¹„ìš© ì ˆì•½
            operational_savings=80000,  # ìš´ì˜ ë¹„ìš© ì ˆì•½
            productivity_gains=100000,  # ìƒì‚°ì„± í–¥ìƒ
            reduced_downtime_savings=40000,  # ë‹¤ìš´íƒ€ì„ ê°ì†Œ
            total_savings=280000
        )
    
    def calculate_roi(self) -> ROIAnalysis:
        """ROI ë¶„ì„"""
        migration_cost = self.calculate_migration_costs()
        annual_savings = self.calculate_annual_savings()
        
        payback_period = migration_cost.total_cost / annual_savings.total_savings
        three_year_roi = (annual_savings.total_savings * 3 - migration_cost.total_cost) / migration_cost.total_cost
        
        return ROIAnalysis(
            payback_period_months=payback_period * 12,
            three_year_roi_percentage=three_year_roi * 100,
            break_even_date=datetime.now() + timedelta(days=payback_period * 365)
        )
```

---

## âš ï¸ **ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë° ë¡¤ë°± ì „ëµ**

### **ğŸš¨ ì£¼ìš” ë¦¬ìŠ¤í¬ ìš”ì†Œ**

| ë¦¬ìŠ¤í¬ | í™•ë¥  | ì˜í–¥ë„ | ì™„í™” ì „ëµ |
|--------|------|--------|-----------|
| **ë°ì´í„° ì†ì‹¤** | ë‚®ìŒ | ì¹˜ëª…ì  | ì‹¤ì‹œê°„ ë°±ì—…, ê²€ì¦ ì‹œìŠ¤í…œ |
| **ì„œë¹„ìŠ¤ ì¤‘ë‹¨** | ì¤‘ê°„ | ë†’ìŒ | Blue-Green ë°°í¬, Canary ë¦´ë¦¬ìŠ¤ |
| **ì„±ëŠ¥ ì €í•˜** | ì¤‘ê°„ | ì¤‘ê°„ | ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§, ìë™ ë¡¤ë°± |
| **ë³´ì•ˆ ì·¨ì•½ì ** | ë‚®ìŒ | ë†’ìŒ | ë³´ì•ˆ í…ŒìŠ¤íŠ¸, ì ‘ê·¼ ì œì–´ |

### **ğŸ”„ ë¡¤ë°± ì „ëµ**

```python
# rollback/rollback-strategy.py
class CubeRollbackStrategy:
    """íë¸Œ ë¡¤ë°± ì „ëµ"""
    
    def __init__(self):
        self.backup_manager = BackupManager()
        self.traffic_manager = TrafficManager()
        self.health_monitor = HealthMonitor()
        
    async def emergency_rollback(self, cube_id: str, target_version: str):
        """ì‘ê¸‰ ë¡¤ë°± ì‹¤í–‰"""
        logger.critical(f"Emergency rollback initiated for {cube_id}")
        
        try:
            # 1. íŠ¸ë˜í”½ ì¤‘ë‹¨
            await self.traffic_manager.stop_traffic_to_cube(cube_id)
            
            # 2. ë°±ì—… ë²„ì „ìœ¼ë¡œ ë³µêµ¬
            await self.backup_manager.restore_cube_version(cube_id, target_version)
            
            # 3. ë°ì´í„° ì¼ê´€ì„± ê²€ì¦
            consistency_check = await self.verify_data_consistency(cube_id)
            if not consistency_check.is_consistent:
                raise RollbackError("Data consistency check failed")
            
            # 4. íŠ¸ë˜í”½ ì¬ê°œ
            await self.traffic_manager.resume_traffic_to_cube(cube_id)
            
            # 5. ë¡¤ë°± ì„±ê³µ í™•ì¸
            health_status = await self.health_monitor.check_cube_health(cube_id)
            if health_status.is_healthy:
                logger.info(f"Emergency rollback successful for {cube_id}")
            else:
                raise RollbackError("Health check failed after rollback")
                
        except Exception as e:
            logger.error(f"Emergency rollback failed for {cube_id}: {e}")
            await self.escalate_to_manual_intervention(cube_id)
```

---

## ğŸ“š **ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œë¶**

### **ğŸ‘¥ íŒ€ë³„ ì—­í•  ë¶„ë‹´**

| íŒ€ | ì—­í•  | ì£¼ìš” ì—…ë¬´ |
|----|------|-----------|
| **ì•„í‚¤í…ì²˜íŒ€** | ì„¤ê³„ ì´ê´„ | íë¸Œ ì„¤ê³„, ì¸í„°í˜ì´ìŠ¤ ì •ì˜ |
| **ë°±ì—”ë“œíŒ€** | íë¸Œ ê°œë°œ | ì„œë¹„ìŠ¤ ë¡œì§ êµ¬í˜„, API ê°œë°œ |
| **í”„ë¡ íŠ¸ì—”ë“œíŒ€** | UI í†µí•© | ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ íë¸Œí™” |
| **ë°ë¸Œì˜µìŠ¤íŒ€** | ì¸í”„ë¼ êµ¬ì¶• | CI/CD, ëª¨ë‹ˆí„°ë§, ë°°í¬ |
| **QAíŒ€** | í’ˆì§ˆ ë³´ì¦ | í…ŒìŠ¤íŠ¸ ìë™í™”, ì„±ëŠ¥ ê²€ì¦ |
| **ë³´ì•ˆíŒ€** | ë³´ì•ˆ ê°•í™” | ì·¨ì•½ì  ì ê²€, ë³´ì•ˆ ì •ì±… |

### **ğŸ“– ë‹¨ê³„ë³„ ì²´í¬ë¦¬ìŠ¤íŠ¸**

#### **Phase 1 ì²´í¬ë¦¬ìŠ¤íŠ¸**
- [ ] íë¸Œ ì•„í‚¤í…ì²˜ ì„¤ê³„ ì™„ë£Œ
- [ ] API Gateway êµ¬ì¶•
- [ ] ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶•
- [ ] ë³´ì•ˆ íë¸Œ êµ¬í˜„
- [ ] CI/CD íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- [ ] ê°œë°œ í™˜ê²½ ê²€ì¦

#### **Phase 2 ì²´í¬ë¦¬ìŠ¤íŠ¸**
- [ ] ì‚¬ì£¼ ì—”ì§„ íë¸Œ ê°œë°œ
- [ ] AI ë¶„ì„ íë¸Œ ê°œë°œ
- [ ] ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ
- [ ] Feature Flag êµ¬í˜„
- [ ] ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] ì‚¬ìš©ì ê²€ì¦ ì™„ë£Œ

#### **Phase 3 ì²´í¬ë¦¬ìŠ¤íŠ¸**
- [ ] í˜ì´í¼ì›Œí¬ íë¸Œ ê°œë°œ
- [ ] í¬ë¡¤ëŸ¬ íë¸Œ ê°œë°œ
- [ ] í”„ë¡ íŠ¸ì—”ë“œ í†µí•© ì™„ë£Œ
- [ ] ëª¨ë“  íë¸Œ ì—°ë™ í…ŒìŠ¤íŠ¸
- [ ] ì‚¬ìš©ì ê²½í—˜ ê²€ì¦
- [ ] ì„±ëŠ¥ ìµœì í™”

#### **Phase 4 ì²´í¬ë¦¬ìŠ¤íŠ¸**
- [ ] ìë™í™” ì‹œìŠ¤í…œ ì™„ì„±
- [ ] ìê°€ ì¹˜ìœ  ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ìŠ¤ ê°œì„ 
- [ ] ROI ëª©í‘œ ë‹¬ì„±
- [ ] ì‹œìŠ¤í…œ ì•ˆì •í™” ì™„ë£Œ
- [ ] ë¬¸ì„œí™” ì™„ë£Œ

---

## ğŸ“ **ê²°ë¡  ë° ê¸°ëŒ€íš¨ê³¼**

### **ğŸŒŸ ì£¼ìš” ì„±ê³¼ ì˜ˆìƒ**

1. **ê¸°ìˆ ì  ì„±ê³¼**
   - ì„œë¹„ìŠ¤ ì‘ë‹µ ì‹œê°„ 60% ë‹¨ì¶• (2.5s â†’ 1.0s)
   - ì‹œìŠ¤í…œ ê°€ìš©ì„± 0.7% í–¥ìƒ (99.2% â†’ 99.9%)
   - ìë™ ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„± 25% ê°œì„ 

2. **ìš´ì˜ì  ì„±ê³¼**
   - ë°°í¬ ë¹ˆë„ 21ë°° ì¦ê°€ (ì£¼ 1íšŒ â†’ ì¼ 3íšŒ)
   - ì¥ì•  ë³µêµ¬ ì‹œê°„ 90% ë‹¨ì¶• (45ë¶„ â†’ 5ë¶„)
   - ìš´ì˜ ìë™í™” 100% ë‹¬ì„±

3. **ë¹„ì¦ˆë‹ˆìŠ¤ ì„±ê³¼**
   - ì—°ê°„ 28ë§Œ ë‹¬ëŸ¬ ë¹„ìš© ì ˆì•½
   - 10ê°œì›” ë§Œì— íˆ¬ì íšŒìˆ˜
   - ì‚¬ìš©ì ë§Œì¡±ë„ 20% í–¥ìƒ

### **ğŸš€ ë¯¸ë˜ í™•ì¥ ê°€ëŠ¥ì„±**

- **Multi-Cloud ì§€ì›**: AWS, GCP, Azure ë©€í‹° í´ë¼ìš°ë“œ ë°°í¬
- **ê¸€ë¡œë²Œ í™•ì¥**: ì§€ì—­ë³„ íë¸Œ í´ëŸ¬ìŠ¤í„° êµ¬ì„±
- **AI ìµœì í™”**: ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ìë™ ìµœì í™”
- **ì„œë¹„ìŠ¤ í™•ì¥**: ìƒˆë¡œìš´ ì„œë¹„ìŠ¤ íë¸Œ ì¦‰ì‹œ ì¶”ê°€ ê°€ëŠ¥

ì´ ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµì„ í†µí•´ HEAL7ëŠ” ë”ìš± ê²¬ê³ í•˜ê³  í™•ì¥ ê°€ëŠ¥í•œ ì‹œìŠ¤í…œìœ¼ë¡œ ì§„í™”í•  ê²ƒì…ë‹ˆë‹¤.

---

**ğŸ“š ê´€ë ¨ ë¬¸ì„œ**:
- [ì„œë¹„ìŠ¤ë³„ íë¸Œ êµ¬í˜„ v2.0](./service-cube-implementation-v2.0.md)
- [íë¸Œ ì¡°ë¦½ íŒ¨í„´ v2.0](./cube-assembly-patterns-v2.0.md)
- [íë¸Œ íš¨ìš©ì„± ì¢…í•© ë¶„ì„ v2.0](./cube-efficiency-analysis-v2.0.md)

**ğŸ”— ì°¸ê³  ìë£Œ**:
- [ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ íŒ¨í„´](https://microservices.io/patterns/refactoring/)
- [Strangler Fig íŒ¨í„´](https://martinfowler.com/bliki/StranglerFigApplication.html)
- [Blue-Green ë°°í¬ ì „ëµ](https://martinfowler.com/bliki/BlueGreenDeployment.html)

*ğŸ“ ë¬¸ì„œ ê´€ë¦¬: 2025-08-20 ì‘ì„± | HEAL7 ì•„í‚¤í…ì²˜íŒ€*
*ğŸ”„ ë‹¤ìŒ ì—…ë°ì´íŠ¸: ë§ˆì´ê·¸ë ˆì´ì…˜ ì§„í–‰ ìƒí™©ì— ë”°ë¼ ì›”ê°„ ì—…ë°ì´íŠ¸*