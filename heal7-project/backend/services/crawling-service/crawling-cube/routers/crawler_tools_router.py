#!/usr/bin/env python3
"""
ğŸ› ï¸ í¬ë¡¤ë§ ë„êµ¬ ì„ íƒ ë¼ìš°í„° (3ë‹¨ê³„ ê°„ì†Œí™” ì‹œìŠ¤í…œ)
ë‹¨ê³„ë³„ í¬ë¡¤ë§ ë„êµ¬ ì¶”ì²œ ë° ê´€ë¦¬

ğŸ¯ 3ë‹¨ê³„ ë„êµ¬ ì²´ê³„:
- ë‹¨ê³„ 1: HTTPX (ë¹ ë¥¸ HTTP ìš”ì²­)
- ë‹¨ê³„ 2: HTTPX + BeautifulSoup (HTML íŒŒì‹±)
- ë‹¨ê³„ 3: Playwright (JavaScript ë Œë”ë§)

Author: HEAL7 Development Team
Version: 2.0.0 (Simplified)
Date: 2025-09-03
"""

from typing import List, Dict, Any, Optional
from fastapi import APIRouter, HTTPException, Query
from pydantic import BaseModel, Field
from enum import Enum
import logging

logger = logging.getLogger(__name__)

# ================================
# ë°ì´í„° ëª¨ë¸ë“¤ (ê°„ì†Œí™”)
# ================================

class CrawlerToolType(str, Enum):
    """3ë‹¨ê³„ í¬ë¡¤ë§ ë„êµ¬ íƒ€ì…"""
    HTTPX = "httpx"
    HTTPX_BEAUTIFULSOUP = "httpx_beautifulsoup" 
    PLAYWRIGHT = "playwright"

class UseCaseType(str, Enum):
    """ì‚¬ìš© ì‚¬ë¡€ íƒ€ì…"""
    STATIC_CONTENT = "static_content"      # ì •ì  ì½˜í…ì¸ 
    DYNAMIC_CONTENT = "dynamic_content"    # ë™ì  ì½˜í…ì¸  (JS)
    API_SCRAPING = "api_scraping"         # API ê¸°ë°˜ ìˆ˜ì§‘
    SPEED_CRITICAL = "speed_critical"     # ì†ë„ ì¤‘ì‹œ
    RELIABILITY_CRITICAL = "reliability_critical" # ì•ˆì •ì„± ì¤‘ì‹œ

class CrawlerToolSpec(BaseModel):
    """í¬ë¡¤ë§ ë„êµ¬ ì‚¬ì–‘"""
    tool_type: CrawlerToolType
    display_name: str
    description: str
    strengths: List[str] = []
    weaknesses: List[str] = []
    best_use_cases: List[UseCaseType] = []
    performance_score: float = Field(ge=0, le=10)  # 0-10 ì ìˆ˜
    reliability_score: float = Field(ge=0, le=10)
    ease_of_use_score: float = Field(ge=0, le=10)
    resource_usage: str = Field(description="low/medium/high")
    javascript_support: bool = False
    async_support: bool = False
    installation_complexity: str = Field(description="easy/medium/hard")
    recommended_fallbacks: List[CrawlerToolType] = []

# ================================
# 3ë‹¨ê³„ ë„êµ¬ ì‚¬ì–‘ ì •ì˜
# ================================

CRAWLER_TOOLS_SPECS: Dict[CrawlerToolType, CrawlerToolSpec] = {
    
    # ë‹¨ê³„ 1: HTTPX (ê¸°ë³¸)
    CrawlerToolType.HTTPX: CrawlerToolSpec(
        tool_type=CrawlerToolType.HTTPX,
        display_name="HTTPX",
        description="âš¡ ì´ˆê³ ì† ë¹„ë™ê¸° HTTP í´ë¼ì´ì–¸íŠ¸",
        strengths=[
            "ğŸš€ ë§¤ìš° ë¹ ë¥¸ ì†ë„",
            "ğŸ’¾ ë‚®ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©",
            "ğŸ”„ ë¹„ë™ê¸° ì§€ì›",
            "ğŸ“¡ HTTP/2 ì§€ì›",
            "ğŸ› ï¸ ê°„ë‹¨í•œ ì„¤ì •"
        ],
        weaknesses=[
            "âŒ JavaScript ë¯¸ì§€ì›",
            "ğŸ” HTML íŒŒì‹± ë¶ˆê°€",
            "ğŸ“ ì •ì  ì½˜í…ì¸ ë§Œ ê°€ëŠ¥"
        ],
        best_use_cases=[UseCaseType.API_SCRAPING, UseCaseType.SPEED_CRITICAL],
        performance_score=10.0,
        reliability_score=8.0,
        ease_of_use_score=9.0,
        resource_usage="low",
        javascript_support=False,
        async_support=True,
        installation_complexity="easy",
        recommended_fallbacks=[CrawlerToolType.HTTPX_BEAUTIFULSOUP, CrawlerToolType.PLAYWRIGHT]
    ),
    
    # ë‹¨ê³„ 2: HTTPX + BeautifulSoup (ì¤‘ê°„)
    CrawlerToolType.HTTPX_BEAUTIFULSOUP: CrawlerToolSpec(
        tool_type=CrawlerToolType.HTTPX_BEAUTIFULSOUP,
        display_name="HTTPX + BeautifulSoup",
        description="ğŸœ ë¹ ë¥¸ HTTP + ê°•ë ¥í•œ HTML íŒŒì‹±",
        strengths=[
            "âš¡ ë¹ ë¥¸ ì†ë„",
            "ğŸ” HTML íŒŒì‹± ì§€ì›",
            "ğŸ’¾ ë³´í†µ ë©”ëª¨ë¦¬ ì‚¬ìš©",
            "ğŸ”„ ë¹„ë™ê¸° ì§€ì›",
            "ğŸ“Š ì •í™•í•œ ë°ì´í„° ì¶”ì¶œ"
        ],
        weaknesses=[
            "âŒ JavaScript ë¯¸ì§€ì›",
            "ğŸ–±ï¸ ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ë¶ˆê°€",
            "ğŸŒ ë™ì  ì½˜í…ì¸  ì œí•œ"
        ],
        best_use_cases=[UseCaseType.STATIC_CONTENT, UseCaseType.SPEED_CRITICAL],
        performance_score=8.0,
        reliability_score=8.0,
        ease_of_use_score=8.0,
        resource_usage="medium",
        javascript_support=False,
        async_support=True,
        installation_complexity="easy",
        recommended_fallbacks=[CrawlerToolType.PLAYWRIGHT, CrawlerToolType.HTTPX]
    ),
    
    # ë‹¨ê³„ 3: Playwright (ìµœê³ ê¸‰)
    CrawlerToolType.PLAYWRIGHT: CrawlerToolSpec(
        tool_type=CrawlerToolType.PLAYWRIGHT,
        display_name="Playwright",
        description="ğŸ­ ìµœì‹  ë¸Œë¼ìš°ì € ìë™í™”ì˜ ì •ì ",
        strengths=[
            "ğŸŒ ì™„ì „í•œ JavaScript ì§€ì›",
            "ğŸ“± ëª¨ë“  ë¸Œë¼ìš°ì € ì§€ì›",
            "ğŸ–±ï¸ ì‚¬ìš©ì ìƒí˜¸ì‘ìš©",
            "ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì§€ì›",
            "ğŸ›¡ï¸ ì•ˆí‹°ë´‡ ìš°íšŒ",
            "âš¡ ë¹ ë¥¸ ì‹¤í–‰ ì†ë„"
        ],
        weaknesses=[
            "ğŸ’¾ ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©",
            "ğŸ”§ ë³µì¡í•œ ì„¤ì •",
            "â±ï¸ ìƒëŒ€ì  ëŠë¦° ì‹œì‘"
        ],
        best_use_cases=[UseCaseType.DYNAMIC_CONTENT, UseCaseType.RELIABILITY_CRITICAL],
        performance_score=7.0,
        reliability_score=9.0,
        ease_of_use_score=7.0,
        resource_usage="high",
        javascript_support=True,
        async_support=True,
        installation_complexity="medium",
        recommended_fallbacks=[CrawlerToolType.HTTPX_BEAUTIFULSOUP, CrawlerToolType.HTTPX]
    )
}

# ================================
# ë¼ìš°í„° ì„¤ì •
# ================================

router = APIRouter(prefix="/api/crawler-tools", tags=["crawler-tools"])

# ================================
# API ì—”ë“œí¬ì¸íŠ¸ë“¤
# ================================

@router.get("/", response_model=Dict[str, Any])
async def get_all_tools():
    """ëª¨ë“  í¬ë¡¤ë§ ë„êµ¬ ì‚¬ì–‘ ì¡°íšŒ"""
    try:
        tools_data = {}
        for tool_type, spec in CRAWLER_TOOLS_SPECS.items():
            tools_data[tool_type.value] = spec.dict()
        
        return {
            "success": True,
            "data": tools_data,
            "system_info": {
                "total_tools": len(CRAWLER_TOOLS_SPECS),
                "system_type": "3ë‹¨ê³„ ê°„ì†Œí™” ì‹œìŠ¤í…œ",
                "available_stages": ["httpx", "httpx_beautifulsoup", "playwright"]
            }
        }
    except Exception as e:
        logger.error(f"ë„êµ¬ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/recommend", response_model=Dict[str, Any])
async def recommend_tool(
    url: str = Query(..., description="ëŒ€ìƒ URL"),
    use_case: Optional[UseCaseType] = Query(None, description="ì‚¬ìš© ì‚¬ë¡€"),
    priority: str = Query("balanced", description="ìš°ì„ ìˆœìœ„: speed, reliability, balanced")
):
    """URLê³¼ ì‚¬ìš© ì‚¬ë¡€ì— ë”°ë¥¸ ë„êµ¬ ì¶”ì²œ"""
    try:
        # URL ë¶„ì„
        url_analysis = _analyze_url(url)
        
        # ë„êµ¬ ì¶”ì²œ ë¡œì§
        recommended_tool = _get_recommended_tool(url_analysis, use_case, priority)
        
        # í´ë°± ì²´ì¸ ìƒì„±
        fallback_chain = _create_fallback_chain(recommended_tool)
        
        return {
            "success": True,
            "data": {
                "primary_recommendation": recommended_tool.value,
                "fallback_chain": [tool.value for tool in fallback_chain],
                "url_analysis": url_analysis,
                "reasoning": _get_recommendation_reasoning(recommended_tool, url_analysis)
            }
        }
    except Exception as e:
        logger.error(f"ë„êµ¬ ì¶”ì²œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/performance", response_model=Dict[str, Any])
async def get_performance_comparison():
    """ë„êµ¬ë³„ ì„±ëŠ¥ ë¹„êµ ë°ì´í„°"""
    try:
        performance_data = {}
        
        for tool_type, spec in CRAWLER_TOOLS_SPECS.items():
            performance_data[tool_type.value] = {
                "performance_score": spec.performance_score,
                "reliability_score": spec.reliability_score,
                "ease_of_use_score": spec.ease_of_use_score,
                "resource_usage": spec.resource_usage,
                "async_support": spec.async_support,
                "javascript_support": spec.javascript_support,
                "overall_score": (spec.performance_score + spec.reliability_score + spec.ease_of_use_score) / 3
            }
        
        return {
            "success": True,
            "data": {
                "tools": performance_data,
                "recommendations": {
                    "fastest": "httpx",
                    "most_reliable": "playwright", 
                    "most_versatile": "httpx_beautifulsoup",
                    "least_resources": "httpx"
                }
            }
        }
    except Exception as e:
        logger.error(f"ì„±ëŠ¥ ë¹„êµ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ================================
# ë‚´ë¶€ í—¬í¼ í•¨ìˆ˜ë“¤
# ================================

def _analyze_url(url: str) -> Dict[str, Any]:
    """URL ë¶„ì„"""
    url_lower = url.lower()
    
    analysis = {
        "is_api": any(keyword in url_lower for keyword in ['api', '.json', '/rest/', '/graphql']),
        "is_dynamic": any(keyword in url_lower for keyword in ['spa', 'app', 'react', 'vue', 'angular']),
        "requires_js": False,  # ê¸°ë³¸ê°’, ë‚˜ì¤‘ì— AIê°€ íŒë‹¨
        "complexity_level": "low"
    }
    
    # ë³µì¡ë„ íŒë‹¨
    if analysis["is_api"]:
        analysis["complexity_level"] = "low"
    elif analysis["is_dynamic"]:
        analysis["complexity_level"] = "high"
    else:
        analysis["complexity_level"] = "medium"
    
    return analysis

def _get_recommended_tool(url_analysis: Dict[str, Any], use_case: Optional[UseCaseType], priority: str) -> CrawlerToolType:
    """ì¶”ì²œ ë„êµ¬ ê²°ì •"""
    
    # API ìš”ì²­ì¸ ê²½ìš°
    if url_analysis["is_api"]:
        return CrawlerToolType.HTTPX
    
    # ë™ì  ì½˜í…ì¸ ì¸ ê²½ìš°
    if url_analysis["is_dynamic"] or url_analysis.get("requires_js", False):
        return CrawlerToolType.PLAYWRIGHT
    
    # ì‚¬ìš© ì‚¬ë¡€ ê¸°ë°˜ íŒë‹¨
    if use_case:
        if use_case == UseCaseType.SPEED_CRITICAL:
            return CrawlerToolType.HTTPX
        elif use_case == UseCaseType.DYNAMIC_CONTENT:
            return CrawlerToolType.PLAYWRIGHT
        elif use_case == UseCaseType.STATIC_CONTENT:
            return CrawlerToolType.HTTPX_BEAUTIFULSOUP
    
    # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ íŒë‹¨
    if priority == "speed":
        return CrawlerToolType.HTTPX
    elif priority == "reliability":
        return CrawlerToolType.PLAYWRIGHT
    else:  # balanced
        return CrawlerToolType.HTTPX_BEAUTIFULSOUP

def _create_fallback_chain(primary_tool: CrawlerToolType) -> List[CrawlerToolType]:
    """í´ë°± ì²´ì¸ ìƒì„±"""
    spec = CRAWLER_TOOLS_SPECS[primary_tool]
    fallback_chain = [primary_tool] + spec.recommended_fallbacks
    return fallback_chain

def _get_recommendation_reasoning(tool: CrawlerToolType, analysis: Dict[str, Any]) -> str:
    """ì¶”ì²œ ì‚¬ìœ  ì„¤ëª…"""
    if tool == CrawlerToolType.HTTPX:
        if analysis["is_api"]:
            return "API ì—”ë“œí¬ì¸íŠ¸ì´ë¯€ë¡œ ë¹ ë¥¸ HTTPXê°€ ìµœì ì…ë‹ˆë‹¤."
        else:
            return "ë‹¨ìˆœí•œ ìš”ì²­ì´ë¯€ë¡œ ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ HTTPXë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤."
    elif tool == CrawlerToolType.HTTPX_BEAUTIFULSOUP:
        return "HTML íŒŒì‹±ì´ í•„ìš”í•˜ì§€ë§Œ JavaScriptëŠ” ë¶ˆìš”í•˜ë¯€ë¡œ HTTPX + BeautifulSoupì´ ì í•©í•©ë‹ˆë‹¤."
    elif tool == CrawlerToolType.PLAYWRIGHT:
        return "ë™ì  ì½˜í…ì¸  ë˜ëŠ” JavaScript ì²˜ë¦¬ê°€ í•„ìš”í•˜ë¯€ë¡œ Playwrightê°€ í•„ìš”í•©ë‹ˆë‹¤."
    
    return "ê· í˜•ì¡íŒ ì ‘ê·¼ì„ ìœ„í•´ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤."

# ================================
# ì‹œìŠ¤í…œ ì •ë³´
# ================================

@router.get("/system-info")
async def get_system_info():
    """3ë‹¨ê³„ ì‹œìŠ¤í…œ ì •ë³´"""
    return {
        "system_version": "2.0.0",
        "system_type": "3ë‹¨ê³„ ê°„ì†Œí™” ì‹œìŠ¤í…œ",
        "available_tools": [tool.value for tool in CrawlerToolType],
        "total_tools": len(CrawlerToolType),
        "removed_tools": ["selenium", "scrapy", "requests_beautifulsoup", "mechanicalsoup"],
        "optimization_focus": ["speed", "memory_efficiency", "maintainability"]
    }