#!/usr/bin/env python3
"""
ğŸ¯ Smart Crawler ì‹œìŠ¤í…œ ë°ëª¨
3-Tier í¬ë¡¤ë§ ì‹œìŠ¤í…œê³¼ ë©€í‹°ëª¨ë‹¬ AI ë¶„ì„ í…ŒìŠ¤íŠ¸

Author: HEAL7 Development Team
Version: 1.0.0
Date: 2025-08-30
"""

import asyncio
import logging
import time
from pathlib import Path

from core.smart_crawler import SmartCrawler, CrawlStrategy
from multimodal.ai_analyzer import MultimodalAnalyzer
from multimodal.document_processor import DocumentProcessor


# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def demo_basic_crawling():
    """ê¸°ë³¸ í¬ë¡¤ë§ ë°ëª¨"""
    logger.info("ğŸ¯ ê¸°ë³¸ í¬ë¡¤ë§ ë°ëª¨ ì‹œì‘")
    
    crawler = SmartCrawler()
    
    try:
        await crawler.initialize()
        
        # í…ŒìŠ¤íŠ¸í•  URLë“¤
        test_urls = [
            "https://httpbin.org/json",  # JSON API (httpx í…ŒìŠ¤íŠ¸)
            "https://example.com",       # ì •ì  ì‚¬ì´íŠ¸ (httpx/playwright)
            "https://www.google.com"     # ë™ì  ì‚¬ì´íŠ¸ (playwright)
        ]
        
        for url in test_urls:
            logger.info(f"ğŸ“¡ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸: {url}")
            
            result = await crawler.crawl(url, strategy=CrawlStrategy.AUTO)
            
            if result.success:
                logger.info(f"âœ… ì„±ê³µ - {result.crawler_used.value}")
                logger.info(f"   ì‘ë‹µ í¬ê¸°: {len(result.html) if result.html else 0} bytes")
                logger.info(f"   ì‘ë‹µ ì‹œê°„: {result.response_time:.2f}ì´ˆ")
            else:
                logger.error(f"âŒ ì‹¤íŒ¨ - {result.error}")
            
            print("-" * 60)
    
    finally:
        await crawler.cleanup()


async def demo_batch_crawling():
    """ë°°ì¹˜ í¬ë¡¤ë§ ë°ëª¨"""
    logger.info("ğŸš€ ë°°ì¹˜ í¬ë¡¤ë§ ë°ëª¨ ì‹œì‘")
    
    crawler = SmartCrawler()
    
    try:
        await crawler.initialize()
        
        # ì—¬ëŸ¬ URL ë™ì‹œ í¬ë¡¤ë§
        urls = [
            "https://httpbin.org/status/200",
            "https://httpbin.org/json",
            "https://httpbin.org/html",
            "https://httpbin.org/xml",
            "https://httpbin.org/delay/1"
        ]
        
        start_time = time.time()
        results = await crawler.batch_crawl(urls, max_concurrent=3)
        end_time = time.time()
        
        # ê²°ê³¼ ë¶„ì„
        successful = [r for r in results if r.success]
        failed = [r for r in results if not r.success]
        
        logger.info(f"ğŸ“Š ë°°ì¹˜ í¬ë¡¤ë§ ì™„ë£Œ:")
        logger.info(f"   ì´ ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
        logger.info(f"   ì„±ê³µ: {len(successful)}/{len(urls)}")
        logger.info(f"   ì‹¤íŒ¨: {len(failed)}/{len(urls)}")
        
        # í¬ë¡¤ëŸ¬ë³„ ì‚¬ìš© í†µê³„
        crawler_usage = {}
        for result in successful:
            crawler_type = result.crawler_used.value
            crawler_usage[crawler_type] = crawler_usage.get(crawler_type, 0) + 1
        
        logger.info(f"   í¬ë¡¤ëŸ¬ ì‚¬ìš©: {crawler_usage}")
    
    finally:
        await crawler.cleanup()


async def demo_strategy_comparison():
    """í¬ë¡¤ë§ ì „ëµ ë¹„êµ ë°ëª¨"""
    logger.info("âš–ï¸ í¬ë¡¤ë§ ì „ëµ ë¹„êµ ë°ëª¨")
    
    crawler = SmartCrawler()
    test_url = "https://httpbin.org/html"
    
    try:
        await crawler.initialize()
        
        strategies = [
            CrawlStrategy.FAST,
            CrawlStrategy.RENDER,
            CrawlStrategy.AUTO
        ]
        
        for strategy in strategies:
            logger.info(f"ğŸ¯ ì „ëµ í…ŒìŠ¤íŠ¸: {strategy.value}")
            
            start_time = time.time()
            result = await crawler.crawl(test_url, strategy=strategy)
            end_time = time.time()
            
            if result.success:
                logger.info(f"   âœ… ì„±ê³µ - {result.crawler_used.value}")
                logger.info(f"   ì‘ë‹µ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
            else:
                logger.info(f"   âŒ ì‹¤íŒ¨ - {result.error}")
    
    finally:
        await crawler.cleanup()


async def demo_health_check():
    """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ë°ëª¨"""
    logger.info("ğŸ¥ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ë°ëª¨")
    
    crawler = SmartCrawler()
    
    try:
        await crawler.initialize()
        
        health = await crawler.health_check()
        
        logger.info(f"ì‹œìŠ¤í…œ ìƒíƒœ: {'âœ… ê±´ê°•' if health['system_healthy'] else 'âŒ ë¬¸ì œ'}")
        logger.info(f"ì´ˆê¸°í™” ìƒíƒœ: {health['initialized']}")
        logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ í¬ë¡¤ëŸ¬: {health['available_crawlers']}")
        
        # í¬ë¡¤ëŸ¬ë³„ ìƒì„¸ ì •ë³´
        for crawler_name, crawler_health in health['crawler_health'].items():
            status = "âœ…" if crawler_health['healthy'] else "âŒ"
            logger.info(f"  {status} {crawler_name}: {crawler_health}")
        
        # ì„±ëŠ¥ ìš”ì•½
        performance = health['performance_summary']
        logger.info(f"ì„±ëŠ¥ ìš”ì•½:")
        logger.info(f"  ì „ì²´ ì„±ê³µë¥ : {performance['overall_success_rate']:.1%}")
        logger.info(f"  ì´ ìš”ì²­ ìˆ˜: {performance['total_requests']}")
        logger.info(f"  ìµœê³  ì„±ëŠ¥: {performance['best_performer']}")
    
    finally:
        await crawler.cleanup()


async def demo_multimodal_ai():
    """ë©€í‹°ëª¨ë‹¬ AI ë¶„ì„ ë°ëª¨"""
    logger.info("ğŸ¤– ë©€í‹°ëª¨ë‹¬ AI ë¶„ì„ ë°ëª¨")
    
    analyzer = MultimodalAnalyzer()
    
    try:
        await analyzer.initialize()
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸
        stats = analyzer.get_usage_stats()
        logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸: {stats['available_models']}")
        
        # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì´ë¯¸ì§€ ë¶„ì„ (API í‚¤ê°€ ìˆëŠ” ê²½ìš°)
        if stats['available_models']:
            logger.info("ğŸ¨ í…ìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„± ë° ë¶„ì„ í…ŒìŠ¤íŠ¸")
            
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„± (PIL ì‚¬ìš©)
            from PIL import Image, ImageDraw, ImageFont
            
            # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
            img = Image.new('RGB', (400, 200), 'white')
            draw = ImageDraw.Draw(img)
            
            try:
                font = ImageFont.load_default()
            except:
                font = None
            
            draw.text((20, 50), "HEAL7 í¬ë¡¤ë§ ì‹œìŠ¤í…œ", fill='black', font=font)
            draw.text((20, 100), "Smart Crawler Test", fill='blue', font=font)
            
            # ì´ë¯¸ì§€ë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜
            import io
            img_bytes = io.BytesIO()
            img.save(img_bytes, format='JPEG')
            img_bytes = img_bytes.getvalue()
            
            # AI ë¶„ì„
            result = await analyzer.analyze_image(
                img_bytes, 
                "ì´ ì´ë¯¸ì§€ì—ì„œ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”."
            )
            
            if result['success']:
                logger.info(f"âœ… AI ë¶„ì„ ì„±ê³µ ({result['model_used']})")
                logger.info(f"   ì¶”ì¶œëœ í…ìŠ¤íŠ¸: {result['content'][:100]}...")
            else:
                logger.warning(f"âŒ AI ë¶„ì„ ì‹¤íŒ¨: {result.get('error')}")
        
        else:
            logger.info("âš ï¸ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ AI ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤")
    
    except Exception as e:
        logger.error(f"AI ë¶„ì„ ë°ëª¨ ì˜¤ë¥˜: {e}")


async def demo_document_processing():
    """ë¬¸ì„œ ì²˜ë¦¬ ë°ëª¨"""
    logger.info("ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬ ë°ëª¨")
    
    processor = DocumentProcessor()
    
    try:
        await processor.initialize()
        
        # ì§€ì›í•˜ëŠ” ë¬¸ì„œ í˜•ì‹
        formats = processor.get_supported_formats()
        logger.info(f"ì§€ì›í•˜ëŠ” ë¬¸ì„œ í˜•ì‹: {formats}")
        
        # í…ŒìŠ¤íŠ¸ìš© í…ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
        test_file = Path("/tmp/test_document.txt")
        with open(test_file, 'w', encoding='utf-8') as f:
            f.write("""HEAL7 í¬ë¡¤ë§ ì‹œìŠ¤í…œ ë¬¸ì„œ

ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ë¬¸ì„œì…ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. 3-Tier í¬ë¡¤ë§ ì‹œìŠ¤í…œ
2. ë©€í‹°ëª¨ë‹¬ AI ë¶„ì„
3. ìë™ í´ë°± ì‹œìŠ¤í…œ

ê²°ë¡ :
ê°•ë ¥í•˜ê³  ì•ˆì •ì ì¸ í¬ë¡¤ë§ ì†”ë£¨ì…˜ì…ë‹ˆë‹¤.
""")
        
        # ë¬¸ì„œ ì²˜ë¦¬ (AI ë¶„ì„ ì œì™¸ - API í‚¤ê°€ ì—†ì„ ìˆ˜ ìˆìŒ)
        result = await processor.process_document(str(test_file), include_ai_analysis=False)
        
        if result.success:
            logger.info(f"âœ… ë¬¸ì„œ ì²˜ë¦¬ ì„±ê³µ")
            logger.info(f"   ë¬¸ì„œ íƒ€ì…: {result.document_type.value}")
            logger.info(f"   ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.2f}ì´ˆ")
            logger.info(f"   í…ìŠ¤íŠ¸ ë¸”ë¡ ìˆ˜: {len(result.text_content)}")
            logger.info(f"   í…Œì´ë¸” ìˆ˜: {len(result.tables)}")
        else:
            logger.error(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {result.error}")
        
        # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬
        test_file.unlink()
    
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ì²˜ë¦¬ ë°ëª¨ ì˜¤ë¥˜: {e}")


async def main():
    """ë©”ì¸ ë°ëª¨ ì‹¤í–‰"""
    logger.info("ğŸ‰ HEAL7 Smart Crawler ì‹œìŠ¤í…œ ë°ëª¨ ì‹œì‘")
    
    demos = [
        ("ê¸°ë³¸ í¬ë¡¤ë§", demo_basic_crawling),
        ("ë°°ì¹˜ í¬ë¡¤ë§", demo_batch_crawling),
        ("ì „ëµ ë¹„êµ", demo_strategy_comparison),
        ("ìƒíƒœ í™•ì¸", demo_health_check),
        ("ë©€í‹°ëª¨ë‹¬ AI", demo_multimodal_ai),
        ("ë¬¸ì„œ ì²˜ë¦¬", demo_document_processing)
    ]
    
    for demo_name, demo_func in demos:
        try:
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸš€ {demo_name} ë°ëª¨ ì‹œì‘")
            logger.info(f"{'='*60}")
            
            await demo_func()
            
            logger.info(f"âœ… {demo_name} ë°ëª¨ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ {demo_name} ë°ëª¨ ì‹¤íŒ¨: {e}")
        
        # ë°ëª¨ ê°„ ì ì‹œ ëŒ€ê¸°
        await asyncio.sleep(1)
    
    logger.info(f"\n{'='*60}")
    logger.info("ğŸŠ ëª¨ë“  ë°ëª¨ ì™„ë£Œ!")
    logger.info(f"{'='*60}")


if __name__ == "__main__":
    asyncio.run(main())