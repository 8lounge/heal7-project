#!/usr/bin/env python3
"""
ì •ë¶€ í¬í„¸ ìŠ¤í¬ë˜í•‘ í”„ë¡œí† íƒ€ì… í…ŒìŠ¤íŠ¸
ì‹¤ì œ ì‚¬ì´íŠ¸ êµ¬ì¡° ê¸°ë°˜ ë°ì´í„° ìˆ˜ì§‘ ê²€ì¦

Author: Paperwork AI Team
Date: 2025-08-23
"""

import asyncio
import aiohttp
from bs4 import BeautifulSoup
import logging
from typing import List, Dict, Optional
from urllib.parse import urljoin
import time

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PortalScraperPrototype:
    """ì •ë¶€ í¬í„¸ ìŠ¤í¬ë˜í¼ í”„ë¡œí† íƒ€ì…"""
    
    def __init__(self):
        self.session = None
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (compatible; PaperworkAI-Test/1.0)',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive'
        }
        
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        connector = aiohttp.TCPConnector(limit=3, ttl_dns_cache=300)
        timeout = aiohttp.ClientTimeout(total=30)
        
        self.session = aiohttp.ClientSession(
            headers=self.headers,
            connector=connector,
            timeout=timeout
        )
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.session:
            await self.session.close()

    async def test_bizinfo_scraping(self) -> List[Dict]:
        """ê¸°ì—…ë§ˆë‹¹ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ¢ ê¸°ì—…ë§ˆë‹¹ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        url = "https://www.bizinfo.go.kr/web/lay1/bbs/S1T122C128/AS/74/list.do"
        programs = []
        
        try:
            # ì²« í˜ì´ì§€ë§Œ í…ŒìŠ¤íŠ¸
            params = {"cpage": 1}
            
            async with self.session.get(url, params=params) as response:
                if response.status != 200:
                    logger.error(f"âŒ HTTP ì˜¤ë¥˜: {response.status}")
                    return []
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                logger.info(f"âœ… í˜ì´ì§€ ë¡œë“œ ì„±ê³µ: {len(html)} bytes")
                
                # í…Œì´ë¸” êµ¬ì¡° ë¶„ì„
                table = soup.select_one('table')
                if not table:
                    logger.error("âŒ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    return []
                
                logger.info("âœ… í…Œì´ë¸” ìš”ì†Œ ë°œê²¬")
                
                # í…Œì´ë¸” í–‰ ì¶”ì¶œ
                rows = table.select('tbody tr')
                if not rows:
                    # tbodyê°€ ì—†ëŠ” ê²½ìš° ì§ì ‘ tr ì°¾ê¸°
                    rows = table.select('tr')
                    if rows:
                        rows = rows[1:]  # í—¤ë” í–‰ ì œì™¸
                
                logger.info(f"ğŸ“‹ í…Œì´ë¸” í–‰ ë°œê²¬: {len(rows)}ê°œ")
                
                for i, row in enumerate(rows[:5]):  # ì²˜ìŒ 5ê°œë§Œ í…ŒìŠ¤íŠ¸
                    cells = row.select('td')
                    if len(cells) >= 6:
                        program = self.extract_bizinfo_program(cells, i + 1)
                        if program:
                            programs.append(program)
                            logger.info(f"âœ… í”„ë¡œê·¸ë¨ {i+1}: {program['title'][:50]}...")
                
        except Exception as e:
            logger.error(f"âŒ ê¸°ì—…ë§ˆë‹¹ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {str(e)}")
        
        return programs

    def extract_bizinfo_program(self, cells: List, index: int) -> Optional[Dict]:
        """ê¸°ì—…ë§ˆë‹¹ í”„ë¡œê·¸ë¨ ì •ë³´ ì¶”ì¶œ"""
        try:
            # ì»¬ëŸ¼: ë²ˆí˜¸, ì§€ì›ë¶„ì•¼, ì§€ì›ì‚¬ì—…ëª…, ì‹ ì²­ê¸°ê°„, ì†Œê´€ë¶€ì²˜, ì‚¬ì—…ìˆ˜í–‰ê¸°ê´€, ë“±ë¡ì¼, ì¡°íšŒìˆ˜
            if len(cells) < 6:
                return None
                
            program = {
                'index': index,
                'support_field': self.clean_text(cells[1].get_text()) if len(cells) > 1 else 'N/A',
                'title': self.clean_text(cells[2].get_text()) if len(cells) > 2 else 'N/A', 
                'application_period': self.clean_text(cells[3].get_text()) if len(cells) > 3 else 'N/A',
                'jurisdiction': self.clean_text(cells[4].get_text()) if len(cells) > 4 else 'N/A',
                'implementing_agency': self.clean_text(cells[5].get_text()) if len(cells) > 5 else 'N/A',
                'registration_date': self.clean_text(cells[6].get_text()) if len(cells) > 6 else 'N/A',
                'view_count': self.clean_text(cells[7].get_text()) if len(cells) > 7 else 'N/A'
            }
            
            # ìƒì„¸ ë§í¬ ì¶”ì¶œ
            link_elem = cells[2].select_one('a[href]') if len(cells) > 2 else None
            if link_elem:
                href = link_elem.get('href')
                program['detail_url'] = urljoin('https://www.bizinfo.go.kr', href)
            else:
                program['detail_url'] = None
                
            return program
            
        except Exception as e:
            logger.error(f"âŒ í”„ë¡œê·¸ë¨ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨ {index}: {str(e)}")
            return None

    async def test_kstartup_scraping(self) -> List[Dict]:
        """K-Startup ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸš€ K-Startup ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        url = "https://www.k-startup.go.kr/web/contents/bizpbanc-ongoing.do"
        programs = []
        
        try:
            async with self.session.get(url) as response:
                if response.status != 200:
                    logger.error(f"âŒ HTTP ì˜¤ë¥˜: {response.status}")
                    return []
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                logger.info(f"âœ… í˜ì´ì§€ ë¡œë“œ ì„±ê³µ: {len(html)} bytes")
                
                # ë‹¤ì–‘í•œ ì…€ë ‰í„°ë¡œ ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ì°¾ê¸°
                selectors = [
                    '.list-item',
                    '.announcement-item', 
                    '.business-item',
                    '.program-item',
                    'ul li',
                    '.content li'
                ]
                
                items = []
                for selector in selectors:
                    items = soup.select(selector)
                    if items and len(items) > 5:  # ì˜ë¯¸ ìˆëŠ” ìˆ˜ì˜ ì•„ì´í…œì´ ìˆëŠ” ê²½ìš°
                        logger.info(f"âœ… ì…€ë ‰í„° '{selector}' ì‚¬ìš©: {len(items)}ê°œ ì•„ì´í…œ")
                        break
                
                if not items:
                    logger.warning("âš ï¸ ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. í˜ì´ì§€ êµ¬ì¡° ë¶„ì„...")
                    # í˜ì´ì§€ êµ¬ì¡° ë¶„ì„ì„ ìœ„í•´ ì£¼ìš” ìš”ì†Œë“¤ ì¶œë ¥
                    self.analyze_page_structure(soup)
                    return []
                
                # ì²˜ìŒ 5ê°œ ì•„ì´í…œë§Œ ë¶„ì„
                for i, item in enumerate(items[:5]):
                    program = self.extract_kstartup_program(item, i + 1)
                    if program:
                        programs.append(program)
                        logger.info(f"âœ… í”„ë¡œê·¸ë¨ {i+1}: {program['title'][:50]}...")
                
        except Exception as e:
            logger.error(f"âŒ K-Startup ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {str(e)}")
        
        return programs

    def extract_kstartup_program(self, item, index: int) -> Optional[Dict]:
        """K-Startup í”„ë¡œê·¸ë¨ ì •ë³´ ì¶”ì¶œ"""
        try:
            # ì œëª© ì¶”ì¶œ
            title_selectors = ['.title', 'h3', 'h4', '.subject', 'strong', 'a']
            title = 'N/A'
            
            for selector in title_selectors:
                title_elem = item.select_one(selector)
                if title_elem:
                    title = self.clean_text(title_elem.get_text())
                    if title and len(title) > 5:  # ì˜ë¯¸ ìˆëŠ” ì œëª©
                        break
            
            # ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì œëª©ì„ ì°¾ì§€ ëª»í•œ ê²½ìš°)
            if title == 'N/A' or len(title) <= 5:
                title = self.clean_text(item.get_text())
                if len(title) > 100:  # ë„ˆë¬´ ê¸´ ê²½ìš° ì•ë¶€ë¶„ë§Œ
                    title = title[:100] + '...'
            
            program = {
                'index': index,
                'title': title,
                'raw_html': str(item)[:200] + '...',  # ë””ë²„ê¹…ìš©
            }
            
            # ì¶”ê°€ ì •ë³´ ì¶”ì¶œ ì‹œë„
            for attr, selectors in {
                'status': ['.status', '.state', '.badge', '.label'],
                'organization': ['.org', '.agency', '.department'],
                'period': ['.period', '.date', '.deadline'],
                'category': ['.category', '.tag', '.type']
            }.items():
                
                value = 'N/A'
                for selector in selectors:
                    elem = item.select_one(selector)
                    if elem:
                        value = self.clean_text(elem.get_text())
                        if value and len(value) > 2:
                            break
                
                program[attr] = value
            
            return program if title != 'N/A' and len(title) > 5 else None
            
        except Exception as e:
            logger.error(f"âŒ K-Startup í”„ë¡œê·¸ë¨ ì¶”ì¶œ ì‹¤íŒ¨ {index}: {str(e)}")
            return None

    def analyze_page_structure(self, soup: BeautifulSoup):
        """í˜ì´ì§€ êµ¬ì¡° ë¶„ì„ (ë””ë²„ê¹…ìš©)"""
        logger.info("ğŸ” í˜ì´ì§€ êµ¬ì¡° ë¶„ì„ ì‹œì‘")
        
        # ì£¼ìš” ì»¨í…Œì´ë„ˆ ìš”ì†Œë“¤ í™•ì¸
        containers = [
            'main', '.main', '#main',
            '.content', '#content', 
            '.container', '.wrapper',
            '.list', '.items', '.announcements'
        ]
        
        for container in containers:
            elem = soup.select_one(container)
            if elem:
                logger.info(f"ğŸ“¦ ì»¨í…Œì´ë„ˆ ë°œê²¬: {container}")
                # í•˜ìœ„ ìš”ì†Œë“¤ ë¶„ì„
                children = elem.find_all(['ul', 'ol', 'div', 'section'], limit=10)
                for child in children:
                    if child.get('class'):
                        logger.info(f"  â””â”€ í•˜ìœ„ ìš”ì†Œ: {child.name}.{'.'.join(child.get('class'))}")

    def clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not text:
            return 'N/A'
        
        # ê³µë°± ë¬¸ì ì •ë¦¬
        cleaned = ' '.join(text.strip().split())
        
        # ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬
        return cleaned if cleaned else 'N/A'

async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    logger.info("ğŸš€ ì •ë¶€ í¬í„¸ ìŠ¤í¬ë˜í•‘ í”„ë¡œí† íƒ€ì… í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    async with PortalScraperPrototype() as scraper:
        # ê¸°ì—…ë§ˆë‹¹ í…ŒìŠ¤íŠ¸
        bizinfo_programs = await scraper.test_bizinfo_scraping()
        logger.info(f"ğŸ¢ ê¸°ì—…ë§ˆë‹¹ ìˆ˜ì§‘ ê²°ê³¼: {len(bizinfo_programs)}ê°œ í”„ë¡œê·¸ë¨")
        
        if bizinfo_programs:
            logger.info("ğŸ“‹ ê¸°ì—…ë§ˆë‹¹ ìƒ˜í”Œ ë°ì´í„°:")
            for i, program in enumerate(bizinfo_programs[:3]):
                logger.info(f"  {i+1}. {program['title']}")
                logger.info(f"     ê¸°ê´€: {program['implementing_agency']}")
                logger.info(f"     ê¸°ê°„: {program['application_period']}")
        
        await asyncio.sleep(2)  # ìš”ì²­ ê°„ê²©
        
        # K-Startup í…ŒìŠ¤íŠ¸
        kstartup_programs = await scraper.test_kstartup_scraping()
        logger.info(f"ğŸš€ K-Startup ìˆ˜ì§‘ ê²°ê³¼: {len(kstartup_programs)}ê°œ í”„ë¡œê·¸ë¨")
        
        if kstartup_programs:
            logger.info("ğŸ“‹ K-Startup ìƒ˜í”Œ ë°ì´í„°:")
            for i, program in enumerate(kstartup_programs[:3]):
                logger.info(f"  {i+1}. {program['title']}")
                if program.get('organization') != 'N/A':
                    logger.info(f"     ê¸°ê´€: {program['organization']}")
                if program.get('period') != 'N/A':
                    logger.info(f"     ê¸°ê°„: {program['period']}")
    
    logger.info("âœ… í”„ë¡œí† íƒ€ì… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

if __name__ == "__main__":
    asyncio.run(main())