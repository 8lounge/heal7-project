#!/usr/bin/env python3
"""
Production Government Portal Scraper
ì •ì‹ ì •ë¶€ í¬í„¸ ìŠ¤í¬ë˜í•‘ ì‹œìŠ¤í…œ

Version: 1.0.0
Author: HEAL7 Team
"""

import asyncio
import aiohttp
import asyncpg
import logging
import json
import hashlib
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
import time

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/tmp/production_scraper.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ProductionScraper:
    """í”„ë¡œë•ì…˜ ì •ë¶€ í¬í„¸ ìŠ¤í¬ë˜í¼"""
    
    def __init__(self):
        self.db_pool = None
        self.session = None
        self.download_base_path = "/tmp/downloads"
        
        # ìŠ¤í¬ë˜í•‘ ì„¤ì •
        self.config = {
            'bizinfo': {
                'base_url': 'https://www.bizinfo.go.kr',
                'list_url': 'https://www.bizinfo.go.kr/web/lay1/biz/S1T122C128/AS/main.do?searchCondition=1&searchKeyword=&crtfcKey=',
                'max_pages': 5,
                'delay': 2.0
            },
            'kstartup': {
                'base_url': 'https://www.k-startup.go.kr',
                'list_url': 'https://www.k-startup.go.kr/main.do',
                'max_pages': 3,
                'delay': 3.0
            }
        }
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (compatible; HEAL7-Scraper/1.0; +https://paperwork.heal7.com)',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive'
        }

    async def initialize(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        logger.info("ğŸš€ Production Scraper ì´ˆê¸°í™” ì‹œì‘")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        try:
            self.db_pool = await asyncpg.create_pool(
                host='localhost',
                database='paperworkdb',
                user='postgres',
                password='postgres',
                min_size=2,
                max_size=10
            )
            logger.info("âœ… PostgreSQL ì—°ê²° ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
        
        # HTTP ì„¸ì…˜ ì´ˆê¸°í™”
        connector = aiohttp.TCPConnector(limit=10, ttl_dns_cache=300, use_dns_cache=True)
        timeout = aiohttp.ClientTimeout(total=30)
        
        self.session = aiohttp.ClientSession(
            headers=self.headers,
            connector=connector,
            timeout=timeout
        )
        
        # ë‹¤ìš´ë¡œë“œ í´ë” ìƒì„±
        os.makedirs(self.download_base_path, exist_ok=True)
        logger.info("âœ… Production Scraper ì´ˆê¸°í™” ì™„ë£Œ")

    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.session:
            await self.session.close()
        if self.db_pool:
            await self.db_pool.close()
        logger.info("ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

    async def scrape_all_portals(self) -> Dict:
        """ëª¨ë“  í¬í„¸ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"""
        logger.info("ğŸŒ ì „ì²´ í¬í„¸ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
        
        results = {
            'total_scraped': 0,
            'total_new': 0,
            'total_updated': 0,
            'portals': {}
        }
        
        # ìŠ¤í¬ë˜í•‘ ì„¸ì…˜ ì‹œì‘
        session_id = await self.create_scraping_session()
        
        try:
            # ê° í¬í„¸ ìŠ¤í¬ë˜í•‘
            for portal_id in ['bizinfo', 'kstartup']:
                logger.info(f"ğŸ“¡ {portal_id} í¬í„¸ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
                
                portal_result = await self.scrape_single_portal(portal_id, session_id)
                results['portals'][portal_id] = portal_result
                results['total_scraped'] += portal_result['scraped_count']
                results['total_new'] += portal_result['new_count']
                results['total_updated'] += portal_result['updated_count']
                
                # í¬í„¸ê°„ ë”œë ˆì´
                await asyncio.sleep(self.config[portal_id]['delay'])
            
            # ì„¸ì…˜ ì™„ë£Œ
            await self.complete_scraping_session(session_id, results)
            
        except Exception as e:
            logger.error(f"âŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            await self.fail_scraping_session(session_id, str(e))
            raise
        
        logger.info(f"âœ… ì „ì²´ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {results['total_scraped']}ê°œ ìˆ˜ì§‘, {results['total_new']}ê°œ ì‹ ê·œ")
        return results

    async def create_scraping_session(self) -> str:
        """ìŠ¤í¬ë˜í•‘ ì„¸ì…˜ ìƒì„±"""
        async with self.db_pool.acquire() as conn:
            session_id = await conn.fetchval("""
                INSERT INTO scraping_sessions (portal_id, session_type, started_at, status)
                VALUES ('multi', 'production', CURRENT_TIMESTAMP, 'running')
                RETURNING id
            """)
            return str(session_id)

    async def complete_scraping_session(self, session_id: str, results: Dict):
        """ìŠ¤í¬ë˜í•‘ ì„¸ì…˜ ì™„ë£Œ"""
        async with self.db_pool.acquire() as conn:
            await conn.execute("""
                UPDATE scraping_sessions 
                SET status = 'completed', 
                    completed_at = CURRENT_TIMESTAMP,
                    items_found = $2,
                    items_processed = $3,
                    items_migrated = $4
                WHERE id = $1
            """, session_id, results['total_scraped'], results['total_new'], results['total_updated'])

    async def fail_scraping_session(self, session_id: str, error_msg: str):
        """ìŠ¤í¬ë˜í•‘ ì„¸ì…˜ ì‹¤íŒ¨ ì²˜ë¦¬"""
        async with self.db_pool.acquire() as conn:
            await conn.execute("""
                UPDATE scraping_sessions 
                SET status = 'failed', 
                    completed_at = CURRENT_TIMESTAMP,
                    error_details = $2
                WHERE id = $1
            """, session_id, json.dumps({'error': error_msg}))

    async def scrape_single_portal(self, portal_id: str, session_id: str) -> Dict:
        """ê°œë³„ í¬í„¸ ìŠ¤í¬ë˜í•‘"""
        portal_config = self.config[portal_id]
        scraped_programs = []
        
        if portal_id == 'bizinfo':
            scraped_programs = await self.scrape_bizinfo()
        elif portal_id == 'kstartup':
            scraped_programs = await self.scrape_kstartup()
        
        # Raw ë°ì´í„° ì €ì¥
        raw_ids = []
        for program in scraped_programs:
            raw_id = await self.save_raw_data(portal_id, program, session_id)
            raw_ids.append(raw_id)
        
        # ì •í˜• ë°ì´í„°ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
        migration_result = await self.migrate_to_structured(raw_ids, portal_id)
        
        return {
            'scraped_count': len(scraped_programs),
            'new_count': migration_result['new_count'],
            'updated_count': migration_result['updated_count'],
            'error_count': migration_result['error_count']
        }

    async def scrape_bizinfo(self) -> List[Dict]:
        """ê¸°ì—…ë§ˆë‹¹ ìŠ¤í¬ë˜í•‘"""
        logger.info("ğŸ¢ ê¸°ì—…ë§ˆë‹¹ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
        
        try:
            url = self.config['bizinfo']['list_url']
            async with self.session.get(url) as response:
                if response.status != 200:
                    logger.error(f"âŒ ê¸°ì—…ë§ˆë‹¹ ì ‘ê·¼ ì‹¤íŒ¨: {response.status}")
                    return []
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                programs = []
                
                # í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì¶”ì¶œ
                table = soup.find('table')
                if not table:
                    logger.warning("âš ï¸ ê¸°ì—…ë§ˆë‹¹ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    return []
                
                rows = table.find_all('tr')[1:]  # í—¤ë” ì œì™¸
                logger.info(f"ğŸ“‹ ê¸°ì—…ë§ˆë‹¹ í–‰ ë°œê²¬: {len(rows)}ê°œ")
                
                for i, row in enumerate(rows[:20]):  # ìµœëŒ€ 20ê°œê¹Œì§€
                    try:
                        cells = row.find_all('td')
                        if len(cells) >= 4:
                            
                            # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
                            title_cell = cells[0]
                            title_link = title_cell.find('a')
                            title = title_link.get_text(strip=True) if title_link else title_cell.get_text(strip=True)
                            detail_url = urljoin(self.config['bizinfo']['base_url'], title_link.get('href', '')) if title_link else ''
                            
                            # ê¸°ê´€ëª…
                            agency = cells[1].get_text(strip=True)
                            
                            # ê¸°ê°„
                            period = cells[2].get_text(strip=True)
                            
                            # ìƒíƒœ
                            status = cells[3].get_text(strip=True)
                            
                            # í”„ë¡œê·¸ë¨ ID ìƒì„±
                            program_id = f"BIZ_{datetime.now().strftime('%Y%m%d')}_{i+1:03d}"
                            
                            program = {
                                'program_id': program_id,
                                'title': title,
                                'implementing_agency': agency,
                                'application_period': period,
                                'application_status': 'active' if 'ì ‘ìˆ˜ì¤‘' in status else 'closed',
                                'detail_url': detail_url,
                                'scraped_at': datetime.now().isoformat(),
                                'portal_name': 'ì •ë¶€ì§€ì›ì‚¬ì—…í†µí•©ì •ë³´ì‹œìŠ¤í…œ',
                                'quality_score': self.calculate_quality_score(title, agency, period)
                            }
                            
                            programs.append(program)
                            logger.info(f"âœ… ê¸°ì—…ë§ˆë‹¹ í”„ë¡œê·¸ë¨ {i+1}: {title[:50]}...")
                    
                    except Exception as e:
                        logger.error(f"âŒ ê¸°ì—…ë§ˆë‹¹ í–‰ ì²˜ë¦¬ ì‹¤íŒ¨ {i+1}: {e}")
                        continue
                
                logger.info(f"ğŸ¢ ê¸°ì—…ë§ˆë‹¹ ìˆ˜ì§‘ ì™„ë£Œ: {len(programs)}ê°œ")
                return programs
                
        except Exception as e:
            logger.error(f"âŒ ê¸°ì—…ë§ˆë‹¹ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return []

    async def scrape_kstartup(self) -> List[Dict]:
        """K-Startup ìŠ¤í¬ë˜í•‘"""
        logger.info("ğŸš€ K-Startup ìŠ¤í¬ë˜í•‘ ì‹œì‘")
        
        try:
            url = self.config['kstartup']['list_url']
            async with self.session.get(url) as response:
                if response.status != 200:
                    logger.error(f"âŒ K-Startup ì ‘ê·¼ ì‹¤íŒ¨: {response.status}")
                    return []
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                programs = []
                
                # ë‹¤ì–‘í•œ ì…€ë ‰í„°ë¡œ ì‹œë„
                selectors = [
                    'ul li a',
                    '.list-item a',
                    '.board-list a',
                    'table tr td a'
                ]
                
                items = []
                for selector in selectors:
                    items = soup.select(selector)
                    if len(items) > 5:
                        logger.info(f"âœ… K-Startup ì…€ë ‰í„° '{selector}' ì‚¬ìš©: {len(items)}ê°œ ì•„ì´í…œ")
                        break
                
                if not items:
                    logger.warning("âš ï¸ K-Startup ì•„ì´í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    return []
                
                for i, item in enumerate(items[:15]):  # ìµœëŒ€ 15ê°œ
                    try:
                        title = item.get_text(strip=True)
                        # ì˜ë¯¸ìˆëŠ” ì œëª©ë§Œ í•„í„°ë§ (SNS ë§í¬, ì™¸ë¶€ ë§í¬ ì œì™¸)
                        if (title and len(title) > 10 and 
                            not any(skip_word in title for skip_word in 
                                   ['ìƒˆì°½ìœ¼ë¡œ ì—´ê¸°', 'í˜ì´ìŠ¤ë¶', 'ë¸”ë¡œê·¸', 'ìœ íŠœë¸Œ', 'ì¸ìŠ¤íƒ€ê·¸ë¨', 
                                    'facebook', 'blog', 'youtube', 'instagram']) and
                            not title.startswith(('http', 'www.')) and
                            ('ì°½ì—…' in title or 'ì§€ì›' in title or 'ì‚¬ì—…' in title or 'ê³µê³ ' in title)):
                            detail_url = urljoin(self.config['kstartup']['base_url'], item.get('href', ''))
                            
                            # í”„ë¡œê·¸ë¨ ID ìƒì„±
                            program_id = f"KST_{datetime.now().strftime('%Y%m%d')}_{i+1:03d}"
                            
                            program = {
                                'program_id': program_id,
                                'title': title,
                                'implementing_agency': 'ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€',
                                'application_period': f"{datetime.now().strftime('%Y-%m-%d')} ~ ìƒì‹œ",
                                'application_status': 'active',
                                'detail_url': detail_url,
                                'scraped_at': datetime.now().isoformat(),
                                'portal_name': 'K-Startup',
                                'quality_score': self.calculate_quality_score(title, 'ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€', 'ìƒì‹œ')
                            }
                            
                            programs.append(program)
                            logger.info(f"âœ… K-Startup í”„ë¡œê·¸ë¨ {i+1}: {title[:50]}...")
                    
                    except Exception as e:
                        logger.error(f"âŒ K-Startup ì•„ì´í…œ ì²˜ë¦¬ ì‹¤íŒ¨ {i+1}: {e}")
                        continue
                
                logger.info(f"ğŸš€ K-Startup ìˆ˜ì§‘ ì™„ë£Œ: {len(programs)}ê°œ")
                return programs
                
        except Exception as e:
            logger.error(f"âŒ K-Startup ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return []

    def calculate_quality_score(self, title: str, agency: str, period: str) -> float:
        """ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        score = 5.0  # ê¸°ë³¸ ì ìˆ˜
        
        # ì œëª© í’ˆì§ˆ ê²€ì‚¬
        if title and len(title) > 10:
            score += 1.5
        if title and any(keyword in title for keyword in ['ì§€ì›', 'ì‚¬ì—…', 'ëª¨ì§‘', 'ê³µê³ ']):
            score += 1.0
            
        # ê¸°ê´€ëª… í’ˆì§ˆ ê²€ì‚¬
        if agency and len(agency) > 3:
            score += 1.0
        if agency and any(keyword in agency for keyword in ['ë¶€', 'ì²­', 'ì›', 'ë‹¨ì²´']):
            score += 0.5
            
        # ê¸°ê°„ í’ˆì§ˆ ê²€ì‚¬
        if period and '~' in period:
            score += 1.0
        if period and re.search(r'\d{4}-\d{2}-\d{2}', period):
            score += 0.5
        
        return min(10.0, score)

    async def save_raw_data(self, portal_id: str, program: Dict, session_id: str) -> int:
        """Raw ë°ì´í„° ì €ì¥"""
        async with self.db_pool.acquire() as conn:
            raw_id = await conn.fetchval("""
                INSERT INTO raw_scraped_data (
                    portal_id, url, scraping_session_id, raw_data, 
                    processing_status, quality_score, scraped_at
                ) VALUES ($1, $2, $3, $4, $5, $6, $7)
                RETURNING id
            """, 
            portal_id, 
            program.get('detail_url', ''), 
            session_id,
            json.dumps(program),
            'pending',
            program.get('quality_score', 5.0),
            datetime.now()
            )
            return raw_id

    async def migrate_to_structured(self, raw_ids: List[int], portal_id: str) -> Dict:
        """ì •í˜• ë°ì´í„°ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜"""
        logger.info(f"ğŸ”„ {portal_id} ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘: {len(raw_ids)}ê°œ")
        
        new_count = 0
        updated_count = 0
        error_count = 0
        
        async with self.db_pool.acquire() as conn:
            for raw_id in raw_ids:
                try:
                    # Raw ë°ì´í„° ì¡°íšŒ
                    raw_row = await conn.fetchrow("""
                        SELECT raw_data FROM raw_scraped_data WHERE id = $1
                    """, raw_id)
                    
                    if not raw_row:
                        continue
                    
                    # JSON ë°ì´í„° íŒŒì‹± (ì´ë¯¸ dictì¸ ê²½ìš°ì™€ stringì¸ ê²½ìš° ëª¨ë‘ ì²˜ë¦¬)
                    raw_data = raw_row['raw_data']
                    if isinstance(raw_data, str):
                        program_data = json.loads(raw_data)
                    elif isinstance(raw_data, dict):
                        program_data = raw_data
                    else:
                        logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ë°ì´í„° íƒ€ì…: {type(raw_data)}")
                        continue
                    
                    # ì¤‘ë³µ ì²´í¬
                    existing = await conn.fetchval("""
                        SELECT id FROM support_programs WHERE program_id = $1
                    """, program_data['program_id'])
                    
                    if existing:
                        # ì—…ë°ì´íŠ¸
                        await conn.execute("""
                            UPDATE support_programs SET
                                title = $2,
                                implementing_agency = $3,
                                application_period = $4,
                                application_status = $5,
                                detail_url = $6,
                                portal_id = $7,
                                data_quality_score = $8,
                                updated_at = CURRENT_TIMESTAMP
                            WHERE program_id = $1
                        """, 
                        program_data['program_id'],
                        program_data.get('title'),
                        program_data.get('implementing_agency'),
                        program_data.get('application_period'),
                        program_data.get('application_status', 'active'),
                        program_data.get('detail_url'),
                        portal_id,
                        program_data.get('quality_score', 5.0)
                        )
                        updated_count += 1
                    else:
                        # ì‹ ê·œ ì¶”ê°€
                        await conn.execute("""
                            INSERT INTO support_programs (
                                program_id, portal_id, original_raw_id, title,
                                implementing_agency, application_period, application_status,
                                detail_url, data_quality_score, created_at, updated_at
                            ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
                        """,
                        program_data['program_id'],
                        portal_id,
                        raw_id,
                        program_data.get('title'),
                        program_data.get('implementing_agency'),
                        program_data.get('application_period'),
                        program_data.get('application_status', 'active'),
                        program_data.get('detail_url'),
                        program_data.get('quality_score', 5.0)
                        )
                        new_count += 1
                    
                    # Raw ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ
                    await conn.execute("""
                        UPDATE raw_scraped_data 
                        SET processing_status = 'completed', processed_at = CURRENT_TIMESTAMP
                        WHERE id = $1
                    """, raw_id)
                    
                except Exception as e:
                    logger.error(f"âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨ raw_id={raw_id}: {e}")
                    error_count += 1
                    
                    # ì˜¤ë¥˜ ìƒíƒœ ì €ì¥
                    await conn.execute("""
                        UPDATE raw_scraped_data 
                        SET processing_status = 'failed', 
                            validation_errors = $2,
                            processed_at = CURRENT_TIMESTAMP
                        WHERE id = $1
                    """, raw_id, json.dumps({'error': str(e)}))
        
        logger.info(f"âœ… {portal_id} ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ: ì‹ ê·œ {new_count}, ì—…ë°ì´íŠ¸ {updated_count}, ì˜¤ë¥˜ {error_count}")
        
        return {
            'new_count': new_count,
            'updated_count': updated_count,
            'error_count': error_count
        }

    async def get_current_stats(self) -> Dict:
        """í˜„ì¬ í†µê³„ ì¡°íšŒ"""
        async with self.db_pool.acquire() as conn:
            stats = await conn.fetchrow("""
                SELECT 
                    COUNT(*) as total_programs,
                    COUNT(*) FILTER (WHERE portal_id = 'bizinfo') as bizinfo_count,
                    COUNT(*) FILTER (WHERE portal_id = 'kstartup') as kstartup_count,
                    AVG(data_quality_score) as avg_quality,
                    COUNT(*) FILTER (WHERE application_status = 'active') as active_count,
                    COUNT(*) FILTER (WHERE created_at::date = CURRENT_DATE) as today_count
                FROM support_programs
            """)
            return dict(stats)

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    scraper = ProductionScraper()
    
    try:
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        await scraper.initialize()
        
        # ì´ì „ í†µê³„ í™•ì¸
        before_stats = await scraper.get_current_stats()
        logger.info(f"ğŸ“Š ìŠ¤í¬ë˜í•‘ ì „ í†µê³„: {before_stats}")
        
        # ì „ì²´ í¬í„¸ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
        results = await scraper.scrape_all_portals()
        
        # ì´í›„ í†µê³„ í™•ì¸
        after_stats = await scraper.get_current_stats()
        logger.info(f"ğŸ“ˆ ìŠ¤í¬ë˜í•‘ í›„ í†µê³„: {after_stats}")
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*60)
        print("ğŸ‰ PRODUCTION SCRAPING COMPLETE")
        print("="*60)
        print(f"ğŸ“Š ì´ ìˆ˜ì§‘: {results['total_scraped']}ê°œ")
        print(f"ğŸ†• ì‹ ê·œ ì¶”ê°€: {results['total_new']}ê°œ") 
        print(f"ğŸ”„ ì—…ë°ì´íŠ¸: {results['total_updated']}ê°œ")
        print(f"ğŸ“ˆ ì „ì²´ í”„ë¡œê·¸ë¨: {before_stats['total_programs']} â†’ {after_stats['total_programs']}")
        print(f"â­ í‰ê·  í’ˆì§ˆì ìˆ˜: {after_stats['avg_quality']:.2f}")
        print("="*60)
        
        # í¬í„¸ë³„ ê²°ê³¼
        for portal_id, portal_result in results['portals'].items():
            print(f"ğŸŒ {portal_id.upper()}: {portal_result['scraped_count']}ê°œ ìˆ˜ì§‘, {portal_result['new_count']}ê°œ ì‹ ê·œ")
        
    except Exception as e:
        logger.error(f"ğŸ’¥ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise
    finally:
        await scraper.cleanup()

if __name__ == "__main__":
    asyncio.run(main())