# ğŸ§  AI ê¸°ë°˜ íŒ¨í„´ ë¶„ì„ ì—”ì§„ ì„¤ê³„ì„œ v2.0

> **í”„ë¡œì íŠ¸**: Government Portal Intelligence System - AI Pattern Analysis  
> **ë²„ì „**: v2.0.0  
> **ì‘ì„±ì¼**: 2025-08-23  
> **ëª©ì **: ì •ë¶€ í¬í„¸ ë°ì´í„°ì—ì„œ ì–‘ì‹ íŒ¨í„´ì„ ìë™ í•™ìŠµí•˜ê³  í…œí”Œë¦¿ì„ ìƒì„±í•˜ëŠ” AI ì—”ì§„  

---

## ğŸ¯ **í•µì‹¬ ë¯¸ì…˜**

### ğŸ’¡ **AI íŒ¨í„´ ë¶„ì„ì˜ ëª©í‘œ**
```
ğŸ“Š ì…ë ¥: ìˆ˜ì²œ ê°œì˜ ì •ë¶€ ì§€ì›ì‚¬ì—… ë°ì´í„°
     â¬‡ï¸ AI ë¶„ì„ ì—”ì§„
ğŸ§  ì²˜ë¦¬: íŒ¨í„´ ì¸ì‹, êµ¬ì¡° ë¶„ì„, í…œí”Œë¦¿ ì¶”ì¶œ
     â¬‡ï¸ ìë™ ìƒì„±
ğŸ“‹ ì¶œë ¥: ê¸°ê´€ë³„ ë§ì¶¤ ë¬¸ì„œ í…œí”Œë¦¿
```

### ğŸš€ **AI ì—”ì§„ì˜ í•µì‹¬ ëŠ¥ë ¥**
1. **íŒ¨í„´ ì¸ì‹**: ìˆ˜ë§ì€ ì§€ì›ì‚¬ì—…ì—ì„œ ê³µí†µ êµ¬ì¡° ë°œê²¬
2. **ê¸°ê´€ë³„ íŠ¹í™”**: ê° ê¸°ê´€ì˜ ê³ ìœ í•œ ìš”êµ¬ì‚¬í•­ í•™ìŠµ
3. **ìë™ í…œí”Œë¦¿í™”**: íŒ¨í„´ì„ ë°”íƒ•ìœ¼ë¡œ ì‹¤ìš©ì ì¸ í…œí”Œë¦¿ ìƒì„±
4. **í’ˆì§ˆ ê²€ì¦**: ìƒì„±ëœ í…œí”Œë¦¿ì˜ ì‹¤ìš©ì„± í‰ê°€
5. **ì§€ì† í•™ìŠµ**: ìƒˆë¡œìš´ ë°ì´í„°ë¡œ íŒ¨í„´ ì •í™•ë„ í–¥ìƒ

---

## ğŸ—ï¸ **AI ì—”ì§„ ì•„í‚¤í…ì²˜**

### ğŸ“Š **5ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸**
```mermaid
graph TB
    subgraph "ğŸ“¥ ë°ì´í„° ìˆ˜ì§‘ì¸µ"
        A1[ì •ë¶€í¬í„¸ ë°ì´í„°]
        A2[ê¸°ì¡´ ì–‘ì‹ ë°ì´í„°]
        A3[ì‚¬ìš©ì í”¼ë“œë°±]
    end
    
    subgraph "ğŸ” íŒ¨í„´ ì¸ì‹ì¸µ"
        B1[ë¬¸ì„œêµ¬ì¡° ë¶„ì„ê¸°]
        B2[ì„¹ì…˜ íŒ¨í„´ ì¶”ì¶œê¸°]
        B3[ê¸°ê´€ë³„ íŠ¹í™” ê°ì§€ê¸°]
        B4[ì‹œê°„ì  ë³€í™” ì¶”ì ê¸°]
    end
    
    subgraph "ğŸ§  AI í•™ìŠµì¸µ"
        C1[êµ¬ì¡° ë¶„ë¥˜ ëª¨ë¸]
        C2[ìš”êµ¬ì‚¬í•­ ì˜ˆì¸¡ ëª¨ë¸]
        C3[í…œí”Œë¦¿ ìƒì„± ëª¨ë¸]
        C4[í’ˆì§ˆ í‰ê°€ ëª¨ë¸]
    end
    
    subgraph "âš™ï¸ í…œí”Œë¦¿ ìƒì„±ì¸µ"
        D1[ìë™ í…œí”Œë¦¿ ìƒì„±ê¸°]
        D2[ê¸°ê´€ë³„ ì»¤ìŠ¤í„°ë§ˆì´ì €]
        D3[í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ]
        D4[ë²„ì „ ê´€ë¦¬ì]
    end
    
    subgraph "ğŸ“¡ ì¶œë ¥ì¸µ"
        E1[Paperwork AI ì—°ë™]
        E2[í…œí”Œë¦¿ ë¼ì´ë¸ŒëŸ¬ë¦¬]
        E3[ë¶„ì„ ë¦¬í¬íŠ¸]
        E4[í”¼ë“œë°± ìˆ˜ì§‘ê¸°]
    end
    
    A1 --> B1
    A2 --> B2
    A3 --> B3
    
    B1 --> C1
    B2 --> C2
    B3 --> C3
    B4 --> C4
    
    C1 --> D1
    C2 --> D2
    C3 --> D3
    C4 --> D4
    
    D1 --> E1
    D2 --> E2
    D3 --> E3
    D4 --> E4
    
    E4 --> A3
```

### ğŸ¤– **í•µì‹¬ AI ëª¨ë¸ êµ¬ì„±**

#### **1. ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ëª¨ë¸**
```python
class DocumentStructureAnalyzer:
    """ì •ë¶€ ë¬¸ì„œ êµ¬ì¡° ìë™ ë¶„ì„ AI"""
    
    def __init__(self):
        self.models = {
            'section_detector': 'gpt-4o',           # ì„¹ì…˜ êµ¬ë¶„ ê°ì§€
            'hierarchy_analyzer': 'claude-3.5',     # ê³„ì¸µ êµ¬ì¡° ë¶„ì„
            'content_classifier': 'gemini-pro',     # ë‚´ìš© ë¶„ë¥˜
            'pattern_extractor': 'gpt-4.1'          # íŒ¨í„´ ì¶”ì¶œ
        }
        
        # í•œêµ­ ì •ë¶€ ë¬¸ì„œ íŠ¹í™” íŒ¨í„´
        self.korean_document_patterns = {
            'common_sections': [
                'ì‚¬ì—…ê°œìš”', 'ì¶”ì§„ë°°ê²½', 'ì‚¬ì—…ëª©ì ', 'ì¶”ì§„ê·¼ê±°',
                'ì‚¬ì—…ë‚´ìš©', 'ì¶”ì§„ì²´ê³„', 'ì¶”ì§„ì¼ì •', 'ì†Œìš”ì˜ˆì‚°',
                'ê¸°ëŒ€íš¨ê³¼', 'í–¥í›„ê³„íš', 'ì²¨ë¶€ì„œë¥˜'
            ],
            'institution_prefixes': {
                'sba': ['í˜ì‹ ', 'ì°½ì—…', 'ë²¤ì²˜', 'ê¸€ë¡œë²Œ'],
                'kosmes': ['ì‹¤ìš©í™”', 'ìƒìš©í™”', 'ê¸°ìˆ ì‚¬ì—…í™”', 'ì¤‘ì†Œê¸°ì—…'],
                'nipa': ['IT', 'ë””ì§€í„¸', 'í”Œë«í¼', 'ë°ì´í„°'],
                'techno': ['ê¸°ìˆ ', 'ê²€ì¦', 'ì‹ ë¢°ì„±', 'ì•ˆì •ì„±']
            }
        }
    
    async def analyze_document_structure(self, documents: List[Dict]) -> Dict:
        """ë¬¸ì„œ êµ¬ì¡° ì¢…í•© ë¶„ì„"""
        
        analysis_result = {
            'total_documents': len(documents),
            'structure_patterns': {},
            'section_frequency': {},
            'hierarchy_analysis': {},
            'institution_patterns': {},
            'template_recommendations': []
        }
        
        # 1. ì „ì²´ ë¬¸ì„œì˜ ì„¹ì…˜ íŒ¨í„´ ë¶„ì„
        section_patterns = await self.extract_section_patterns(documents)
        analysis_result['structure_patterns'] = section_patterns
        
        # 2. ì„¹ì…˜ë³„ ì¶œí˜„ ë¹ˆë„ ê³„ì‚°
        section_frequency = self.calculate_section_frequency(documents)
        analysis_result['section_frequency'] = section_frequency
        
        # 3. ë¬¸ì„œ ê³„ì¸µ êµ¬ì¡° ë¶„ì„
        hierarchy_analysis = await self.analyze_hierarchy_patterns(documents)
        analysis_result['hierarchy_analysis'] = hierarchy_analysis
        
        # 4. ê¸°ê´€ë³„ íŠ¹í™” íŒ¨í„´ ì‹ë³„
        institution_patterns = await self.identify_institution_patterns(documents)
        analysis_result['institution_patterns'] = institution_patterns
        
        # 5. í…œí”Œë¦¿ ìƒì„± ê¶Œì¥ì‚¬í•­
        recommendations = await self.generate_template_recommendations(analysis_result)
        analysis_result['template_recommendations'] = recommendations
        
        return analysis_result
    
    async def extract_section_patterns(self, documents: List[Dict]) -> Dict:
        """ì„¹ì…˜ íŒ¨í„´ ì¶”ì¶œ"""
        
        patterns = {
            'universal_sections': [],  # ëª¨ë“  ê¸°ê´€ì—ì„œ ê³µí†µ
            'frequent_sections': [],   # 80% ì´ìƒì—ì„œ ë“±ì¥
            'optional_sections': [],   # 50% ì´ìƒì—ì„œ ë“±ì¥
            'rare_sections': []        # 20% ì´ìƒì—ì„œ ë“±ì¥
        }
        
        # ëª¨ë“  ë¬¸ì„œì—ì„œ ì„¹ì…˜ ì¶”ì¶œ
        all_sections = []
        for doc in documents:
            doc_sections = await self.extract_sections_from_document(doc)
            all_sections.extend(doc_sections)
        
        # ë¹ˆë„ ê³„ì‚°
        from collections import Counter
        section_counts = Counter(all_sections)
        total_docs = len(documents)
        
        for section, count in section_counts.items():
            frequency = count / total_docs
            
            if frequency >= 0.95:
                patterns['universal_sections'].append({
                    'section': section,
                    'frequency': frequency,
                    'count': count
                })
            elif frequency >= 0.8:
                patterns['frequent_sections'].append({
                    'section': section,
                    'frequency': frequency,
                    'count': count
                })
            elif frequency >= 0.5:
                patterns['optional_sections'].append({
                    'section': section,
                    'frequency': frequency,
                    'count': count
                })
            elif frequency >= 0.2:
                patterns['rare_sections'].append({
                    'section': section,
                    'frequency': frequency,
                    'count': count
                })
        
        return patterns
    
    async def extract_sections_from_document(self, document: Dict) -> List[str]:
        """ê°œë³„ ë¬¸ì„œì—ì„œ ì„¹ì…˜ ì¶”ì¶œ"""
        
        content = document.get('detailed_description', '') or document.get('title', '')
        
        extraction_prompt = f"""
        ë‹¤ìŒ ì •ë¶€ ì§€ì›ì‚¬ì—… ë¬¸ì„œì—ì„œ ì„¹ì…˜ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ê³  í‘œì¤€í™”ëœ ì„¹ì…˜ëª…ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”:
        
        ë¬¸ì„œ ë‚´ìš©:
        {content}
        
        í•œêµ­ ì •ë¶€ ë¬¸ì„œì˜ ì¼ë°˜ì ì¸ ì„¹ì…˜ë“¤:
        - ì‚¬ì—…ê°œìš” (ì‚¬ì—…ëª…, ëª©ì , ë°°ê²½)
        - ì‚¬ì—…ë‚´ìš© (ì¶”ì§„ë‚´ìš©, ì„¸ë¶€ê³„íš)
        - ì§€ì›ëŒ€ìƒ (ì‹ ì²­ìê²©, ì„ ì •ê¸°ì¤€)
        - ì§€ì›ì¡°ê±´ (ì§€ì›ê·œëª¨, ì§€ì›ë°©ì‹, ì§€ì›ê¸°ê°„)
        - ì‹ ì²­ë°©ë²• (ì ‘ìˆ˜ë°©ë²•, ì œì¶œì„œë¥˜, ì‹¬ì‚¬ì ˆì°¨)
        - í‰ê°€ê¸°ì¤€ (ì‹¬ì‚¬ê¸°ì¤€, ë°°ì )
        - ì‚¬í›„ê´€ë¦¬ (ì„±ê³¼ê´€ë¦¬, í™˜ìˆ˜ì¡°ê±´)
        
        ì¶”ì¶œ ì§€ì‹œì‚¬í•­:
        1. ë¬¸ì„œì— ëª…ì‹œë˜ê±°ë‚˜ ì•”ì‹œëœ ì„¹ì…˜ë“¤ì„ ì‹ë³„í•˜ì„¸ìš”
        2. í‘œì¤€í™”ëœ ì„¹ì…˜ëª…ìœ¼ë¡œ ì •ê·œí™”í•˜ì„¸ìš”
        3. JSON ë°°ì—´ í˜•íƒœë¡œ ë°˜í™˜í•˜ì„¸ìš”
        
        ì˜ˆì‹œ: ["ì‚¬ì—…ê°œìš”", "ì§€ì›ëŒ€ìƒ", "ì‹ ì²­ë°©ë²•"]
        """
        
        response = await self.models['section_detector'].generate(extraction_prompt)
        
        # JSON íŒŒì‹± ì‹œë„
        try:
            import json
            sections = json.loads(response.strip())
            return sections if isinstance(sections, list) else []
        except:
            # íŒŒì‹± ì‹¤íŒ¨ì‹œ ìˆ˜ë™ ì¶”ì¶œ
            return self.manual_section_extraction(content)
    
    def manual_section_extraction(self, content: str) -> List[str]:
        """ìˆ˜ë™ ì„¹ì…˜ ì¶”ì¶œ (ë°±ì—… ë°©ë²•)"""
        sections = []
        
        for standard_section in self.korean_document_patterns['common_sections']:
            if standard_section in content:
                sections.append(standard_section)
        
        return sections
```

#### **2. ê¸°ê´€ë³„ íŠ¹í™” íŒ¨í„´ í•™ìŠµ ëª¨ë¸**
```python
class InstitutionPatternLearner:
    """ê¸°ê´€ë³„ íŠ¹í™” íŒ¨í„´ í•™ìŠµ AI"""
    
    def __init__(self):
        self.learning_models = {
            'preference_analyzer': 'claude-3.5',    # ê¸°ê´€ ì„ í˜¸ë„ ë¶„ì„
            'style_detector': 'gpt-4o',            # ë¬¸ì²´/ìŠ¤íƒ€ì¼ ê°ì§€
            'requirement_predictor': 'gemini-pro',  # ìš”êµ¬ì‚¬í•­ ì˜ˆì¸¡
            'trend_analyzer': 'gpt-4.1'            # íŠ¸ë Œë“œ ë¶„ì„
        }
        
        self.institution_profiles = {}
    
    async def learn_institution_patterns(self, institution_id: str, documents: List[Dict]) -> Dict:
        """íŠ¹ì • ê¸°ê´€ì˜ íŒ¨í„´ í•™ìŠµ"""
        
        learning_result = {
            'institution_id': institution_id,
            'total_samples': len(documents),
            'learned_patterns': {},
            'confidence_scores': {},
            'recommendations': {}
        }
        
        # 1. ë¬¸ì²´ ë° í†¤ ë¶„ì„
        style_analysis = await self.analyze_writing_style(documents)
        learning_result['learned_patterns']['writing_style'] = style_analysis
        
        # 2. êµ¬ì¡°ì  ì„ í˜¸ë„ ë¶„ì„
        structure_preference = await self.analyze_structure_preference(documents)
        learning_result['learned_patterns']['structure_preference'] = structure_preference
        
        # 3. í‚¤ì›Œë“œ ì„ í˜¸ë„ ë¶„ì„
        keyword_analysis = await self.analyze_keyword_preferences(documents)
        learning_result['learned_patterns']['keyword_preferences'] = keyword_analysis
        
        # 4. í‰ê°€ ê¸°ì¤€ íŒ¨í„´ ë¶„ì„
        evaluation_patterns = await self.analyze_evaluation_patterns(documents)
        learning_result['learned_patterns']['evaluation_criteria'] = evaluation_patterns
        
        # 5. ì‹œê°„ì  ë³€í™” íŠ¸ë Œë“œ ë¶„ì„
        trend_analysis = await self.analyze_temporal_trends(documents)
        learning_result['learned_patterns']['trends'] = trend_analysis
        
        # 6. ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        confidence_scores = self.calculate_confidence_scores(learning_result['learned_patterns'])
        learning_result['confidence_scores'] = confidence_scores
        
        # 7. ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = await self.generate_improvement_recommendations(learning_result)
        learning_result['recommendations'] = recommendations
        
        return learning_result
    
    async def analyze_writing_style(self, documents: List[Dict]) -> Dict:
        """ë¬¸ì²´ ë° í†¤ ë¶„ì„"""
        
        # ë¬¸ì„œë“¤ì˜ ë‚´ìš© ì¶”ì¶œ
        contents = []
        for doc in documents:
            content = doc.get('detailed_description', '') or doc.get('title', '')
            if content and len(content) > 100:  # ì¶©ë¶„í•œ ê¸¸ì´ì˜ ë‚´ìš©ë§Œ
                contents.append(content)
        
        if not contents:
            return {'style': 'unknown', 'confidence': 0}
        
        analysis_prompt = f"""
        ë‹¤ìŒ ì •ë¶€ ê¸°ê´€ì˜ ì§€ì›ì‚¬ì—… ë¬¸ì„œë“¤ì„ ë¶„ì„í•˜ì—¬ ë¬¸ì²´ì  íŠ¹ì§•ì„ íŒŒì•…í•´ì£¼ì„¸ìš”:
        
        ë¬¸ì„œ ìƒ˜í”Œë“¤:
        {' | '.join(contents[:5])}  # ì²˜ìŒ 5ê°œ ìƒ˜í”Œë§Œ ì‚¬ìš©
        
        ë¶„ì„í•  ìš”ì†Œë“¤:
        1. ë¬¸ì²´ ìŠ¤íƒ€ì¼ (ê°œì¡°ì‹ vs ì„œìˆ í˜•)
        2. í†¤ì•¤ë§¤ë„ˆ (ê³µì‹ì /ì¹œê·¼í•¨/ì „ë¬¸ì  ë“±)
        3. ê°•ì¡° íŒ¨í„´ (ì–´ë–¤ ë¶€ë¶„ì„ ê°•ì¡°í•˜ëŠ” ê²½í–¥)
        4. ìš©ì–´ ì‚¬ìš© íŠ¹ì§• (ì „ë¬¸ìš©ì–´/ì¼ë°˜ìš©ì–´ ë¹„ìœ¨)
        5. ë¬¸ì¥ ê¸¸ì´ ë° ë³µì¡ë„
        
        ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”:
        {{
            "style_type": "ê°œì¡°ì‹/ì„œìˆ í˜•/í˜¼ìš©",
            "tone": "ê³µì‹ì /ì¹œê·¼í•¨/ì „ë¬¸ì ",
            "emphasis_pattern": "í˜ì‹ ì„±/ì‹¤ìš©ì„±/ì•ˆì •ì„± ë“±",
            "terminology_level": "ê³ ê¸‰/ì¤‘ê¸‰/ì´ˆê¸‰",
            "sentence_complexity": "ë³µì¡/ë³´í†µ/ë‹¨ìˆœ",
            "key_characteristics": ["íŠ¹ì§•1", "íŠ¹ì§•2", "íŠ¹ì§•3"]
        }}
        """
        
        response = await self.learning_models['style_detector'].generate(analysis_prompt)
        
        try:
            import json
            return json.loads(response.strip())
        except:
            return {'style': 'analysis_failed', 'confidence': 0}
```

#### **3. ìë™ í…œí”Œë¦¿ ìƒì„± ëª¨ë¸**
```python
class AutoTemplateGenerator:
    """í•™ìŠµëœ íŒ¨í„´ì„ ë°”íƒ•ìœ¼ë¡œ í…œí”Œë¦¿ ìë™ ìƒì„±"""
    
    def __init__(self):
        self.generation_models = {
            'template_architect': 'gpt-4o',         # í…œí”Œë¦¿ êµ¬ì¡° ì„¤ê³„
            'section_generator': 'claude-3.5',     # ì„¹ì…˜ë³„ ìƒì„¸ ìƒì„±
            'validator': 'gemini-pro',              # í’ˆì§ˆ ê²€ì¦
            'optimizer': 'gpt-4.1'                 # ìµœì í™”
        }
        
        self.quality_thresholds = {
            'minimum_quality': 7.0,   # ìµœì†Œ í’ˆì§ˆ ì ìˆ˜
            'approval_quality': 8.5,  # ìë™ ìŠ¹ì¸ ì ìˆ˜
            'excellent_quality': 9.5  # ìš°ìˆ˜ í…œí”Œë¦¿ ì ìˆ˜
        }
    
    async def generate_template_from_patterns(
        self, 
        pattern_analysis: Dict, 
        institution_id: str,
        template_type: str = 'support_business'
    ) -> Dict:
        """íŒ¨í„´ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í…œí”Œë¦¿ ìƒì„±"""
        
        generation_result = {
            'template_id': f"auto_{institution_id}_{template_type}_{int(time.time())}",
            'institution_id': institution_id,
            'template_type': template_type,
            'generated_template': {},
            'quality_assessment': {},
            'generation_metadata': {}
        }
        
        # 1. í…œí”Œë¦¿ êµ¬ì¡° ì„¤ê³„
        template_structure = await self.design_template_structure(
            pattern_analysis, institution_id, template_type
        )
        
        # 2. ì„¹ì…˜ë³„ ìƒì„¸ ìƒì„±
        detailed_sections = await self.generate_detailed_sections(
            template_structure, pattern_analysis
        )
        
        # 3. ê¸°ê´€ë³„ ì»¤ìŠ¤í„°ë§ˆì´ì§• ì ìš©
        customized_template = await self.apply_institution_customization(
            detailed_sections, institution_id, pattern_analysis
        )
        
        # 4. í’ˆì§ˆ í‰ê°€
        quality_assessment = await self.assess_template_quality(customized_template)
        
        # 5. ìµœì í™” (í•„ìš”ì‹œ)
        if quality_assessment['score'] < self.quality_thresholds['approval_quality']:
            optimized_template = await self.optimize_template(
                customized_template, quality_assessment
            )
            final_quality = await self.assess_template_quality(optimized_template)
        else:
            optimized_template = customized_template
            final_quality = quality_assessment
        
        generation_result['generated_template'] = optimized_template
        generation_result['quality_assessment'] = final_quality
        generation_result['generation_metadata'] = {
            'patterns_used': len(pattern_analysis.get('structure_patterns', {})),
            'customization_applied': bool(pattern_analysis.get('institution_patterns')),
            'optimization_performed': quality_assessment['score'] < self.quality_thresholds['approval_quality'],
            'generation_time': datetime.now().isoformat()
        }
        
        return generation_result
    
    async def design_template_structure(
        self, 
        pattern_analysis: Dict, 
        institution_id: str, 
        template_type: str
    ) -> Dict:
        """í…œí”Œë¦¿ ê¸°ë³¸ êµ¬ì¡° ì„¤ê³„"""
        
        structure_patterns = pattern_analysis.get('structure_patterns', {})
        institution_patterns = pattern_analysis.get('institution_patterns', {})
        
        design_prompt = f"""
        ë‹¤ìŒ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ {institution_id} ê¸°ê´€ì˜ {template_type} í…œí”Œë¦¿ êµ¬ì¡°ë¥¼ ì„¤ê³„í•´ì£¼ì„¸ìš”:
        
        íŒ¨í„´ ë¶„ì„ ê²°ê³¼:
        - ë²”ìš© ì„¹ì…˜: {structure_patterns.get('universal_sections', [])}
        - ë¹ˆë²ˆí•œ ì„¹ì…˜: {structure_patterns.get('frequent_sections', [])}
        - ì„ íƒì  ì„¹ì…˜: {structure_patterns.get('optional_sections', [])}
        
        ê¸°ê´€ë³„ íŠ¹í™”ì‚¬í•­:
        {json.dumps(institution_patterns.get(institution_id, {}), ensure_ascii=False, indent=2)}
        
        í…œí”Œë¦¿ êµ¬ì¡° ì„¤ê³„ ìš”êµ¬ì‚¬í•­:
        1. ë…¼ë¦¬ì  ìˆœì„œë¡œ ì„¹ì…˜ ë°°ì¹˜
        2. ê° ì„¹ì…˜ì˜ í•„ìˆ˜/ì„ íƒ ì—¬ë¶€ ê²°ì •
        3. ì„¹ì…˜ë³„ ì˜ˆìƒ ê¸¸ì´ ë° ë³µì¡ë„ ì„¤ì •
        4. ê¸°ê´€ íŠ¹í™” ìš”ì†Œ ë°˜ì˜
        5. ì‚¬ìš©ì í¸ì˜ì„± ê³ ë ¤
        
        ê²°ê³¼ í˜•ì‹:
        {{
            "template_name": "í…œí”Œë¦¿ëª…",
            "sections": [
                {{
                    "id": "section_id",
                    "name": "ì„¹ì…˜ëª…",
                    "order": 1,
                    "required": true/false,
                    "description": "ì„¹ì…˜ ì„¤ëª…",
                    "expected_length": "ì˜ˆìƒ ê¸¸ì´",
                    "complexity": "ë‚®ìŒ/ë³´í†µ/ë†’ìŒ",
                    "institution_specific": true/false
                }}
            ],
            "overall_structure": "ê°œì¡°ì‹/ì„œìˆ í˜•/í˜¼ìš©",
            "emphasis_points": ["ê°•ì¡°ì 1", "ê°•ì¡°ì 2"]
        }}
        """
        
        response = await self.generation_models['template_architect'].generate(design_prompt)
        
        try:
            import json
            return json.loads(response.strip())
        except:
            # ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜
            return self.get_default_template_structure(template_type)
```

---

## ğŸ“Š **í•™ìŠµ ë°ì´í„° ê´€ë¦¬**

### ğŸ—„ï¸ **íŒ¨í„´ í•™ìŠµ ë°ì´í„°ë² ì´ìŠ¤**
```sql
-- AI í•™ìŠµìš© íŒ¨í„´ ë°ì´í„° í…Œì´ë¸”
CREATE TABLE ai_pattern_data (
    id SERIAL PRIMARY KEY,
    institution_id VARCHAR(20) NOT NULL,
    document_type VARCHAR(50) NOT NULL,
    pattern_category VARCHAR(30) NOT NULL,     -- 'structure', 'style', 'requirement'
    pattern_data JSONB NOT NULL,               -- íŒ¨í„´ ìƒì„¸ ë°ì´í„°
    confidence_score FLOAT DEFAULT 0.0,       -- íŒ¨í„´ ì‹ ë¢°ë„
    sample_count INTEGER DEFAULT 0,           -- í•™ìŠµ ìƒ˜í”Œ ìˆ˜
    last_updated TIMESTAMP DEFAULT NOW(),
    version VARCHAR(20) DEFAULT '1.0'
);

-- í…œí”Œë¦¿ ìƒì„± ì´ë ¥ í…Œì´ë¸”
CREATE TABLE template_generation_history (
    id SERIAL PRIMARY KEY,
    template_id VARCHAR(100) NOT NULL,
    institution_id VARCHAR(20) NOT NULL,
    generation_method VARCHAR(30) NOT NULL,   -- 'ai_auto', 'ai_assisted', 'manual'
    input_patterns JSONB,                     -- ì…ë ¥ëœ íŒ¨í„´ ë°ì´í„°
    generated_template JSONB NOT NULL,        -- ìƒì„±ëœ í…œí”Œë¦¿
    quality_score FLOAT NOT NULL,             -- í’ˆì§ˆ ì ìˆ˜
    user_feedback JSONB,                      -- ì‚¬ìš©ì í”¼ë“œë°±
    approval_status VARCHAR(20) DEFAULT 'pending', -- 'pending', 'approved', 'rejected'
    created_at TIMESTAMP DEFAULT NOW()
);

-- AI ëª¨ë¸ ì„±ëŠ¥ ì¶”ì  í…Œì´ë¸”
CREATE TABLE ai_model_performance (
    id SERIAL PRIMARY KEY,
    model_name VARCHAR(50) NOT NULL,
    task_type VARCHAR(30) NOT NULL,           -- 'structure_analysis', 'template_generation'
    input_data_hash VARCHAR(64),              -- ì…ë ¥ ë°ì´í„° í•´ì‹œ
    output_quality_score FLOAT,               -- ì¶œë ¥ í’ˆì§ˆ ì ìˆ˜
    processing_time_ms INTEGER,               -- ì²˜ë¦¬ ì‹œê°„
    success BOOLEAN DEFAULT true,
    error_message TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);
```

---

## ğŸš€ **êµ¬í˜„ ìš°ì„ ìˆœìœ„**

### ğŸ“… **Phase 1: ê¸°ë³¸ íŒ¨í„´ ì¸ì‹ (2ì£¼)**
- [ ] ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ AI ëª¨ë¸ êµ¬í˜„
- [ ] í•œêµ­ ì •ë¶€ ë¬¸ì„œ íŒ¨í„´ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
- [ ] ê¸°ë³¸ ì„¹ì…˜ ì¶”ì¶œ ë° ë¶„ë¥˜ ì‹œìŠ¤í…œ
- [ ] íŒ¨í„´ ì‹ ë¢°ë„ í‰ê°€ ë©”ì»¤ë‹ˆì¦˜

### ğŸ“… **Phase 2: ê¸°ê´€ë³„ íŠ¹í™” í•™ìŠµ (2ì£¼)**  
- [ ] ê¸°ê´€ë³„ ë¬¸ì²´/ìŠ¤íƒ€ì¼ ë¶„ì„ ëª¨ë¸
- [ ] ìš”êµ¬ì‚¬í•­ ì˜ˆì¸¡ ë° ë¶„ë¥˜ ì‹œìŠ¤í…œ
- [ ] ì‹œê°„ì  ë³€í™” ì¶”ì  ë° íŠ¸ë Œë“œ ë¶„ì„
- [ ] ì ì‘í˜• í•™ìŠµ ì•Œê³ ë¦¬ì¦˜

### ğŸ“… **Phase 3: ìë™ í…œí”Œë¦¿ ìƒì„± (2ì£¼)**
- [ ] í…œí”Œë¦¿ ìë™ ìƒì„± AI ëª¨ë¸
- [ ] í’ˆì§ˆ í‰ê°€ ë° ê²€ì¦ ì‹œìŠ¤í…œ
- [ ] ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ê°œì„  ë£¨í”„
- [ ] ëŒ€ëŸ‰ í…œí”Œë¦¿ ìƒì„± ë° ê´€ë¦¬

---

**ğŸ’¡ í•µì‹¬ ê°€ì¹˜**: "AIê°€ í•™ìŠµí•˜ëŠ” ì •ë¶€ ì–‘ì‹" - ìˆ˜ì²œ ê°œì˜ ì§€ì›ì‚¬ì—… ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ ìë™ í•™ìŠµí•˜ì—¬ ê° ê¸°ê´€ì— ìµœì í™”ëœ í…œí”Œë¦¿ì„ ì‹¤ì‹œê°„ ìƒì„±í•˜ëŠ” ì§€ëŠ¥í˜• ì‹œìŠ¤í…œ

*ğŸ“ ì´ì œ ì •ë¶€ê°€ ìƒˆë¡œìš´ ì§€ì›ì‚¬ì—…ì„ ê³µê³ í•˜ë©´, AIê°€ ì¦‰ì‹œ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ë§ì¶¤í˜• ì‹ ì²­ì„œ í…œí”Œë¦¿ì„ ìë™ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!*