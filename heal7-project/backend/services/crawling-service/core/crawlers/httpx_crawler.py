#!/usr/bin/env python3
"""
ğŸš€ HTTPX í¬ë¡¤ëŸ¬ (Tier 1)
ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ì •ì  ì‚¬ì´íŠ¸ ë° API í¬ë¡¤ë§

Features:
- ë¹„ë™ê¸° HTTP/2 ì§€ì›
- ìë™ ë¦¬ë‹¤ì´ë ‰íŠ¸ ì²˜ë¦¬
- ì„¸ì…˜ ë° ì¿ í‚¤ ê´€ë¦¬
- ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° ì§€ì›

Author: HEAL7 Development Team
Version: 1.0.0
Date: 2025-08-30
"""

import asyncio
import time
import json
from typing import Optional, Dict, Any, List
from urllib.parse import urljoin, urlparse

import httpx
from bs4 import BeautifulSoup

from .base_crawler import (
    BaseCrawler, CrawlResult, CrawlConfig, CrawlerType,
    CrawlingError, TimeoutError, create_default_headers, is_valid_url
)


class HttpxCrawler(BaseCrawler):
    """ğŸš€ HTTPX ê¸°ë°˜ ê³ ì„±ëŠ¥ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        super().__init__("httpx")
        self.client: Optional[httpx.AsyncClient] = None
        self.default_limits = httpx.Limits(
            max_keepalive_connections=20,
            max_connections=100,
            keepalive_expiry=30
        )
        
    async def initialize(self):
        """HTTPX í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        if self.is_initialized:
            return
            
        try:
            self.client = httpx.AsyncClient(
                limits=self.default_limits,
                timeout=httpx.Timeout(30.0),
                follow_redirects=True,
                http2=True,
                headers=create_default_headers()
            )
            
            self.is_initialized = True
            self.logger.info("âœ… HTTPX í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ HTTPX ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise CrawlingError(f"HTTPX ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.client:
            await self.client.aclose()
            self.client = None
        self.is_initialized = False
        self.logger.info("ğŸ›‘ HTTPX í¬ë¡¤ëŸ¬ ì •ë¦¬ ì™„ë£Œ")
    
    async def crawl(self, config: CrawlConfig) -> CrawlResult:
        """ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜"""
        if not self.is_initialized:
            await self.initialize()
        
        start_time = time.time()
        
        try:
            # URL ìœ íš¨ì„± ê²€ì‚¬
            if not is_valid_url(config.url):
                raise CrawlingError(f"ì˜ëª»ëœ URL: {config.url}")
            
            # ìš”ì²­ í—¤ë” ì„¤ì •
            headers = {**create_default_headers(), **config.headers}
            if config.user_agent:
                headers['User-Agent'] = config.user_agent
            
            # HTTP ìš”ì²­ ì‹¤í–‰
            response = await self._make_request(config.url, headers, config.timeout)
            
            # ì‘ë‹µ ì²˜ë¦¬
            result = await self._process_response(response, config, start_time)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.update_stats(result)
            
            return result
            
        except httpx.TimeoutException:
            error_msg = f"ìš”ì²­ íƒ€ì„ì•„ì›ƒ: {config.url}"
            self.logger.warning(error_msg)
            return CrawlResult(
                success=False,
                error=error_msg,
                error_type="timeout",
                crawler_used=CrawlerType.HTTPX,
                url=config.url,
                response_time=time.time() - start_time
            )
            
        except httpx.RequestError as e:
            error_msg = f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}"
            self.logger.error(error_msg)
            return CrawlResult(
                success=False,
                error=error_msg,
                error_type="network_error",
                crawler_used=CrawlerType.HTTPX,
                url=config.url,
                response_time=time.time() - start_time
            )
            
        except Exception as e:
            error_msg = f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}"
            self.logger.error(error_msg)
            return CrawlResult(
                success=False,
                error=error_msg,
                error_type="unexpected_error",
                crawler_used=CrawlerType.HTTPX,
                url=config.url,
                response_time=time.time() - start_time
            )
    
    async def _make_request(self, url: str, headers: Dict[str, str], timeout: int) -> httpx.Response:
        """HTTP ìš”ì²­ ì‹¤í–‰"""
        try:
            response = await self.client.get(
                url,
                headers=headers,
                timeout=timeout
            )
            
            # ìƒíƒœ ì½”ë“œ í™•ì¸
            if response.status_code >= 400:
                self.logger.warning(f"HTTP {response.status_code}: {url}")
            
            return response
            
        except httpx.TimeoutException:
            raise
        except Exception as e:
            raise CrawlingError(f"ìš”ì²­ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    async def _process_response(self, response: httpx.Response, config: CrawlConfig, start_time: float) -> CrawlResult:
        """ì‘ë‹µ ì²˜ë¦¬"""
        response_time = time.time() - start_time
        
        try:
            # ì½˜í…ì¸  íƒ€ì… í™•ì¸
            content_type = response.headers.get('content-type', '').lower()
            
            # JSON ì‘ë‹µ ì²˜ë¦¬
            if 'application/json' in content_type:
                content = await self._process_json_response(response)
            else:
                # HTML/í…ìŠ¤íŠ¸ ì‘ë‹µ ì²˜ë¦¬
                content = response.text
            
            # ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
            metadata = {
                'content_type': content_type,
                'content_length': len(content) if content else 0,
                'headers': dict(response.headers),
                'cookies': dict(response.cookies),
                'final_url': str(response.url),
                'redirect_count': len(response.history),
                'encoding': response.encoding
            }
            
            # ì„±ê³µ ê²°ê³¼ ë°˜í™˜
            return CrawlResult(
                success=True,
                html=content,
                status_code=response.status_code,
                metadata=metadata,
                crawler_used=CrawlerType.HTTPX,
                url=config.url,
                response_time=response_time
            )
            
        except Exception as e:
            self.logger.error(f"ì‘ë‹µ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return CrawlResult(
                success=False,
                error=f"ì‘ë‹µ ì²˜ë¦¬ ì‹¤íŒ¨: {e}",
                error_type="response_processing_error",
                status_code=response.status_code,
                crawler_used=CrawlerType.HTTPX,
                url=config.url,
                response_time=response_time
            )
    
    async def _process_json_response(self, response: httpx.Response) -> str:
        """JSON ì‘ë‹µ ì²˜ë¦¬"""
        try:
            json_data = response.json()
            # JSONì„ ì˜ˆì˜ê²Œ í¬ë§·íŒ…í•´ì„œ ë°˜í™˜
            return json.dumps(json_data, indent=2, ensure_ascii=False)
        except json.JSONDecodeError:
            # JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜
            return response.text
    
    async def can_handle(self, url: str) -> bool:
        """URL ì²˜ë¦¬ ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨"""
        if not is_valid_url(url):
            return False
        
        url_lower = url.lower()
        
        # API ì—”ë“œí¬ì¸íŠ¸ (ìµœìš°ì„ )
        if any(keyword in url_lower for keyword in [
            'api', '.json', '/rest/', '/graphql', '/v1/', '/v2/'
        ]):
            return True
        
        # ì •ì  íŒŒì¼
        if any(ext in url_lower for ext in [
            '.html', '.htm', '.xml', '.txt', '.css', '.js'
        ]):
            return True
        
        # ë¬¸ì„œ íŒŒì¼ (ë‹¤ìš´ë¡œë“œ)
        if any(ext in url_lower for ext in [
            '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.zip'
        ]):
            return True
        
        # ê¸°ë³¸ì ìœ¼ë¡œ ëŒ€ë¶€ë¶„ì˜ HTTP ìš”ì²­ ì²˜ë¦¬ ê°€ëŠ¥
        return True
    
    async def download_file(self, url: str, save_path: str = None) -> CrawlResult:
        """íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì „ìš© í•¨ìˆ˜"""
        if not self.is_initialized:
            await self.initialize()
        
        start_time = time.time()
        
        try:
            async with self.client.stream('GET', url) as response:
                if response.status_code >= 400:
                    raise CrawlingError(f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ HTTP {response.status_code}")
                
                content = b''
                async for chunk in response.aiter_bytes():
                    content += chunk
                
                # íŒŒì¼ ì €ì¥ (ì„ íƒì‚¬í•­)
                if save_path:
                    with open(save_path, 'wb') as f:
                        f.write(content)
                
                metadata = {
                    'file_size': len(content),
                    'content_type': response.headers.get('content-type'),
                    'filename': save_path or url.split('/')[-1]
                }
                
                return CrawlResult(
                    success=True,
                    html=f"Downloaded {len(content)} bytes",
                    status_code=response.status_code,
                    metadata=metadata,
                    crawler_used=CrawlerType.HTTPX,
                    url=url,
                    response_time=time.time() - start_time
                )
                
        except Exception as e:
            return CrawlResult(
                success=False,
                error=f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}",
                crawler_used=CrawlerType.HTTPX,
                url=url,
                response_time=time.time() - start_time
            )
    
    async def get_page_info(self, url: str) -> Dict[str, Any]:
        """í˜ì´ì§€ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘ (HEAD ìš”ì²­)"""
        if not self.is_initialized:
            await self.initialize()
        
        try:
            response = await self.client.head(url, timeout=10)
            
            return {
                'status_code': response.status_code,
                'content_type': response.headers.get('content-type'),
                'content_length': response.headers.get('content-length'),
                'last_modified': response.headers.get('last-modified'),
                'server': response.headers.get('server'),
                'final_url': str(response.url),
                'success': response.status_code < 400
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
    
    async def batch_crawl(self, urls: List[str], max_concurrent: int = 10) -> List[CrawlResult]:
        """ë°°ì¹˜ í¬ë¡¤ë§ (ì—¬ëŸ¬ URL ë™ì‹œ ì²˜ë¦¬)"""
        if not self.is_initialized:
            await self.initialize()
        
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def crawl_single(url: str) -> CrawlResult:
            async with semaphore:
                config = CrawlConfig(url=url)
                return await self.crawl(config)
        
        tasks = [crawl_single(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì˜ˆì™¸ ì²˜ë¦¬
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append(CrawlResult(
                    success=False,
                    error=str(result),
                    crawler_used=CrawlerType.HTTPX,
                    url=urls[i]
                ))
            else:
                processed_results.append(result)
        
        return processed_results
    
    def get_supported_features(self) -> List[str]:
        """ì§€ì›í•˜ëŠ” ê¸°ëŠ¥ ëª©ë¡"""
        return [
            "static_html",
            "json_api", 
            "file_download",
            "batch_crawling",
            "http2_support",
            "auto_redirect",
            "cookie_management",
            "streaming_download"
        ]


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜

async def quick_crawl(url: str, **kwargs) -> CrawlResult:
    """ë¹ ë¥¸ í¬ë¡¤ë§ (ì›ìƒ· í•¨ìˆ˜)"""
    crawler = HttpxCrawler()
    try:
        await crawler.initialize()
        config = CrawlConfig(url=url, **kwargs)
        return await crawler.crawl(config)
    finally:
        await crawler.cleanup()


async def is_url_accessible(url: str, timeout: int = 5) -> bool:
    """URL ì ‘ê·¼ ê°€ëŠ¥ì„± ë¹ ë¥¸ ì²´í¬"""
    crawler = HttpxCrawler()
    try:
        await crawler.initialize()
        info = await crawler.get_page_info(url)
        return info.get('success', False)
    except:
        return False
    finally:
        await crawler.cleanup()