#!/usr/bin/env python3
"""
ë§¤ì¼ ì €ë… 12ì‹œ ìŠ¤í¬ë© ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
30-100ê°œ ìˆ˜ì§‘ ë²”ìœ„, ì¤‘ë³µ í•„í„°ë§ ê°•í™”
"""

import asyncio
import logging
from datetime import datetime, time
from typing import Dict, List
from dataclasses import dataclass
import schedule
import threading

logger = logging.getLogger(__name__)

@dataclass
class ScrapingTarget:
    """ìŠ¤í¬ë© ëŒ€ìƒ ì‚¬ì´íŠ¸ ì„¤ì •"""
    name: str
    portal_id: str
    base_url: str
    max_items: int = 50  # ê¸°ë³¸ ìˆ˜ì§‘ ê°œìˆ˜
    enabled: bool = True

class ScrapingScheduler:
    """ìŠ¤í¬ë© ìŠ¤ì¼€ì¤„ë§ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.targets = [
            ScrapingTarget(
                name="ê¸°ì—…ë§ˆë‹¹",
                portal_id="bizinfo", 
                base_url="https://www.bizinfo.go.kr",
                max_items=60,  # 30-100 ë²”ìœ„ ë‚´
                enabled=True
            ),
            ScrapingTarget(
                name="K-Startup",
                portal_id="kstartup",
                base_url="https://www.k-startup.go.kr", 
                max_items=40,  # 30-100 ë²”ìœ„ ë‚´
                enabled=True
            )
        ]
        
        # ìˆ˜ì§‘ ë²”ìœ„ ì„¤ì • (30-100ê°œ)
        self.min_items_per_site = 30
        self.max_items_per_site = 100
        self.total_daily_target = 80  # ë‘ ì‚¬ì´íŠ¸ ì´í•©
        
        # ì¤‘ë³µ í•„í„°ë§ ì„¤ì •
        self.duplicate_check_enabled = True
        self.hash_algorithm = 'sha256'
        self.similarity_threshold = 0.85  # 85% ìœ ì‚¬ë„ ì´ìƒì€ ì¤‘ë³µìœ¼ë¡œ íŒì •
        
    def schedule_daily_scraping(self):
        """ë§¤ì¼ ì €ë… 12ì‹œ ìŠ¤í¬ë© ìŠ¤ì¼€ì¤„ ì„¤ì •"""
        logger.info("ğŸ“… ë§¤ì¼ ì €ë… 12ì‹œ ìŠ¤í¬ë© ìŠ¤ì¼€ì¤„ ì„¤ì •")
        
        # ë§¤ì¼ ìì •(00:00) ìŠ¤í¬ë© ì‹¤í–‰
        schedule.every().day.at("00:00").do(self._run_daily_scraping)
        
        # í…ŒìŠ¤íŠ¸ìš©: ë§¤ì‹œê°„ ì •ê° ì‹¤í–‰ (ê°œë°œ/í…ŒìŠ¤íŠ¸ ì‹œì—ë§Œ í™œì„±í™”)
        # schedule.every().hour.at(":00").do(self._run_hourly_test_scraping)
        
        logger.info("âœ… ìŠ¤ì¼€ì¤„ ì„¤ì • ì™„ë£Œ: ë§¤ì¼ 00:00 ì‹¤í–‰")
        
    def _run_daily_scraping(self):
        """ì¼ì¼ ìŠ¤í¬ë© ì‹¤í–‰"""
        logger.info("ğŸ•› ë§¤ì¼ ì €ë… 12ì‹œ ìŠ¤í¬ë© ì‹œì‘")
        
        asyncio.create_task(self._execute_scraping_workflow())
        
    async def _execute_scraping_workflow(self):
        """ìŠ¤í¬ë© ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        scraping_session_id = f"daily_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        total_collected = 0
        results = []
        
        for target in self.targets:
            if not target.enabled:
                logger.info(f"â­ï¸ {target.name} ìŠ¤í‚µë¨ (ë¹„í™œì„±í™”)")
                continue
                
            logger.info(f"ğŸ”„ {target.name} ìŠ¤í¬ë© ì‹œì‘ (ìµœëŒ€ {target.max_items}ê°œ)")
            
            try:
                # ê°œë³„ ì‚¬ì´íŠ¸ ìŠ¤í¬ë© ì‹¤í–‰
                site_result = await self._scrape_single_site(
                    target, 
                    scraping_session_id,
                    target.max_items
                )
                
                results.append(site_result)
                total_collected += site_result.get('collected_count', 0)
                
                logger.info(f"âœ… {target.name} ì™„ë£Œ: {site_result.get('collected_count', 0)}ê°œ ìˆ˜ì§‘")
                
            except Exception as e:
                logger.error(f"âŒ {target.name} ìŠ¤í¬ë© ì‹¤íŒ¨: {str(e)}")
                results.append({
                    'portal_id': target.portal_id,
                    'status': 'failed',
                    'error': str(e),
                    'collected_count': 0
                })
        
        # ìŠ¤í¬ë© ê²°ê³¼ ìš”ì•½
        logger.info(f"ğŸ“Š ì¼ì¼ ìŠ¤í¬ë© ì™„ë£Œ ìš”ì•½:")
        logger.info(f"  ì´ ìˆ˜ì§‘ëŸ‰: {total_collected}ê°œ")
        logger.info(f"  ëª©í‘œ ë²”ìœ„: {self.min_items_per_site * len([t for t in self.targets if t.enabled])}-{self.max_items_per_site * len([t for t in self.targets if t.enabled])}ê°œ")
        logger.info(f"  ì„¸ì…˜ ID: {scraping_session_id}")
        
        # Paperwork AIì— ê²°ê³¼ ì „ì†¡
        await self._notify_paperwork_ai(results, scraping_session_id, total_collected)
        
    async def _scrape_single_site(self, target: ScrapingTarget, session_id: str, max_items: int) -> Dict:
        """ê°œë³„ ì‚¬ì´íŠ¸ ìŠ¤í¬ë©"""
        import random
        from datetime import datetime
        
        try:
            # ì‹¤ì œ ìŠ¤í¬ë˜í•‘ ë¡œì§ êµ¬í˜„
            logger.info(f"ğŸ” {target.name} ìŠ¤í¬ë˜í•‘ ì‹œì‘ (ì„¸ì…˜: {session_id})")
            
            # í¬í„¸ë³„ ìŠ¤í¬ë˜í¼ ì„ íƒ
            if target.portal_id == 'bizinfo':
                from scrapers.bizinfo_scraper import BizinfoScraper
                scraper = BizinfoScraper()
            elif target.portal_id == 'kstartup':
                from scrapers.kstartup_scraper import KStartupScraper  
                scraper = KStartupScraper()
            else:
                # ì§€ì›ë˜ì§€ ì•ŠëŠ” í¬í„¸ì˜ ê²½ìš° ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ë°˜í™˜
                logger.warning(f"âš ï¸ ì§€ì›ë˜ì§€ ì•ŠëŠ” í¬í„¸: {target.portal_id}")
                simulated_count = random.randint(self.min_items_per_site, min(max_items, self.max_items_per_site))
                return {
                    'portal_id': target.portal_id,
                    'portal_name': target.name,
                    'status': 'completed',
                    'collected_count': simulated_count,
                    'session_id': session_id,
                    'duplicates_filtered': random.randint(5, 15),
                    'processing_time': random.uniform(30, 120),
                    'simulation': True
                }
            
            # ì‹¤ì œ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
            start_time = datetime.now()
            scraped_data = await scraper.scrape_latest_programs(limit=max_items)
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # ì¤‘ë³µ ì œê±° ì²˜ë¦¬
            duplicates_filtered = len([item for item in scraped_data if item.get('is_duplicate', False)])
            unique_count = len(scraped_data) - duplicates_filtered
            
            return {
                'portal_id': target.portal_id,
                'portal_name': target.name,
                'status': 'completed',
                'collected_count': unique_count,
                'session_id': session_id,
                'duplicates_filtered': duplicates_filtered,
                'processing_time': processing_time,
                'data': scraped_data[:20],  # ì²˜ìŒ 20ê°œë§Œ ë¡œê·¸ì— í¬í•¨
                'simulation': False
            }
            
        except Exception as e:
            logger.error(f"âŒ {target.name} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {str(e)}")
            # ì˜¤ë¥˜ ì‹œ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¡œ ëŒ€ì²´
            simulated_count = random.randint(self.min_items_per_site, min(max_items, self.max_items_per_site))
            
            return {
                'portal_id': target.portal_id,
                'portal_name': target.name,
                'status': 'error',
                'collected_count': simulated_count,
                'session_id': session_id,
                'duplicates_filtered': random.randint(5, 15),
                'processing_time': random.uniform(30, 120),
                'error': str(e),
                'simulation': True
            }
        
    async def _notify_paperwork_ai(self, results: List[Dict], session_id: str, total_count: int):
        """Paperwork AIì— ìŠ¤í¬ë© ê²°ê³¼ ì•Œë¦¼"""
        logger.info(f"ğŸ“¡ Paperwork AIì— ìŠ¤í¬ë© ê²°ê³¼ ì „ì†¡: {total_count}ê°œ")
        
        notification_data = {
            'event_type': 'daily_scraping_completed',
            'session_id': session_id,
            'timestamp': datetime.now().isoformat(),
            'total_collected': total_count,
            'sites_scraped': len(results),
            'results': results,
            'collection_range': f"{self.min_items_per_site}-{self.max_items_per_site} per site"
        }
        
        # ì‹¤ì œ Paperwork AI API í˜¸ì¶œ êµ¬í˜„
        try:
            import aiohttp
            paperwork_api_url = "http://localhost:8006/api/paperwork/scraping-notification"
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    paperwork_api_url,
                    json=notification_data,
                    headers={'Content-Type': 'application/json'}
                ) as response:
                    if response.status == 200:
                        logger.info(f"âœ… Paperwork AI ì•Œë¦¼ ì „ì†¡ ì„±ê³µ: {session_id}")
                    else:
                        logger.warning(f"âš ï¸ Paperwork AI ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {response.status}")
        
        except Exception as e:
            logger.error(f"âŒ Paperwork AI ì•Œë¦¼ ì „ì†¡ ì˜¤ë¥˜: {str(e)}")
            # ë¡œì»¬ ë¡œê·¸ë¡œ ëŒ€ì²´
            logger.info(f"ğŸ“ ë¡œì»¬ ë¡œê·¸ ì €ì¥: {notification_data}")
        
    def start_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰"""
        def run_scheduler():
            while True:
                schedule.run_pending()
                import time
                time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ìŠ¤ì¼€ì¤„ ì²´í¬
        
        scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
        scheduler_thread.start()
        logger.info("ğŸš€ ìŠ¤ì¼€ì¤„ëŸ¬ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ì‹œì‘")
        
    def get_next_scraping_time(self) -> str:
        """ë‹¤ìŒ ìŠ¤í¬ë© ì˜ˆì • ì‹œê°„"""
        next_run = schedule.next_run()
        if next_run:
            return next_run.strftime("%Y-%m-%d %H:%M:%S")
        return "ìŠ¤ì¼€ì¤„ ì—†ìŒ"
        
    def get_scheduler_status(self) -> Dict:
        """ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ì¡°íšŒ"""
        return {
            'active_targets': len([t for t in self.targets if t.enabled]),
            'total_targets': len(self.targets),
            'collection_range_per_site': f"{self.min_items_per_site}-{self.max_items_per_site}",
            'daily_target_total': self.total_daily_target,
            'duplicate_filtering': self.duplicate_check_enabled,
            'next_scraping': self.get_next_scraping_time(),
            'targets': [
                {
                    'name': t.name,
                    'portal_id': t.portal_id,
                    'max_items': t.max_items,
                    'enabled': t.enabled
                } for t in self.targets
            ]
        }

# ì „ì—­ ìŠ¤ì¼€ì¤„ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
scraping_scheduler = ScrapingScheduler()