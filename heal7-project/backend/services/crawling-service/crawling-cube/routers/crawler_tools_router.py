#!/usr/bin/env python3
"""
ğŸ”§ í¬ë¡¤ë§ ë„êµ¬ ì„ íƒ ë° ì¶”ì²œ ì‹œìŠ¤í…œ
ëª©ì ì— ë§ëŠ” í¬ë¡¤ë§ ë„êµ¬ ì¡°í•© ì¶”ì²œ ë° í´ë°± ì‹œìŠ¤í…œ

Created: 2025-09-01
Author: HEAL7 Development Team
"""

import logging
from typing import Dict, List, Optional, Any
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field
from enum import Enum
import json

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# API ë¼ìš°í„° ìƒì„±
router = APIRouter(prefix="/api/crawler-tools", tags=["crawler-tools"])

# ================================
# ë°ì´í„° ëª¨ë¸ë“¤
# ================================

class CrawlerToolType(str, Enum):
    HTTPX = "httpx"
    PLAYWRIGHT = "playwright"
    SELENIUM = "selenium"
    HTTPX_BEAUTIFULSOUP = "httpx_beautifulsoup"
    REQUESTS_BEAUTIFULSOUP = "requests_beautifulsoup"
    SCRAPY = "scrapy"
    HTTPX_LXML = "httpx_lxml"
    AIOHTTP_BEAUTIFULSOUP = "aiohttp_beautifulsoup"
    MECHANICALSOUP = "mechanicalsoup"

class UseCaseType(str, Enum):
    STATIC_CONTENT = "static_content"      # ì •ì  ì½˜í…ì¸ 
    DYNAMIC_CONTENT = "dynamic_content"    # ë™ì  ì½˜í…ì¸  (JS)
    LARGE_SCALE = "large_scale"           # ëŒ€ìš©ëŸ‰ ìŠ¤í¬ë˜í•‘
    FORM_INTERACTION = "form_interaction" # í¼ ìƒí˜¸ì‘ìš©
    API_SCRAPING = "api_scraping"         # API ê¸°ë°˜ ìˆ˜ì§‘
    SPEED_CRITICAL = "speed_critical"     # ì†ë„ ì¤‘ì‹œ
    RELIABILITY_CRITICAL = "reliability_critical" # ì•ˆì •ì„± ì¤‘ì‹œ

class CrawlerToolSpec(BaseModel):
    """í¬ë¡¤ë§ ë„êµ¬ ì‚¬ì–‘"""
    tool_type: CrawlerToolType
    display_name: str
    description: str
    strengths: List[str] = []
    weaknesses: List[str] = []
    best_use_cases: List[UseCaseType] = []
    performance_score: float = Field(ge=0, le=10)  # 0-10 ì ìˆ˜
    reliability_score: float = Field(ge=0, le=10)
    ease_of_use_score: float = Field(ge=0, le=10)
    resource_usage: str = Field(description="low/medium/high")
    javascript_support: bool = False
    async_support: bool = False
    installation_complexity: str = Field(description="easy/medium/hard")
    recommended_fallbacks: List[CrawlerToolType] = []

class ToolRecommendation(BaseModel):
    """ë„êµ¬ ì¶”ì²œ ê²°ê³¼"""
    primary_tool: CrawlerToolType
    confidence_score: float = Field(ge=0, le=100)
    reasoning: str
    fallback_tools: List[CrawlerToolType] = []
    setup_steps: List[str] = []
    estimated_performance: Dict[str, Any] = {}

class CrawlingRequest(BaseModel):
    """í¬ë¡¤ë§ ìš”ì²­ ì‚¬ì–‘"""
    target_urls: List[str] = []
    expected_data_types: List[str] = []  # text, images, tables, forms
    javascript_required: bool = False
    form_interaction_required: bool = False
    expected_volume: str = Field(default="small", description="small/medium/large")
    priority: str = Field(default="balanced", description="speed/reliability/balanced")
    budget_constraint: str = Field(default="medium", description="low/medium/high")

# ================================
# í¬ë¡¤ë§ ë„êµ¬ ì‚¬ì–‘ ë°ì´í„°
# ================================

CRAWLER_TOOLS_SPECS: Dict[CrawlerToolType, CrawlerToolSpec] = {
    CrawlerToolType.HTTPX: CrawlerToolSpec(
        tool_type=CrawlerToolType.HTTPX,
        display_name="HTTPX",
        description="í˜„ëŒ€ì ì´ê³  ë¹ ë¥¸ HTTP í´ë¼ì´ì–¸íŠ¸",
        strengths=[
            "âš¡ ë§¤ìš° ë¹ ë¥¸ ì„±ëŠ¥",
            "ğŸ”„ HTTP/2 ì§€ì›",
            "âš™ï¸ ë¹„ë™ê¸° ì²˜ë¦¬",
            "ğŸ¯ API ìŠ¤í¬ë˜í•‘ì— ìµœì í™”",
            "ğŸ“¦ ê°€ë²¼ìš´ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©"
        ],
        weaknesses=[
            "âŒ JavaScript ë¯¸ì§€ì›",
            "âŒ ë™ì  ì½˜í…ì¸  ì²˜ë¦¬ ë¶ˆê°€",
            "âŒ ë³µì¡í•œ ì¸ì¦ ì²˜ë¦¬ ì–´ë ¤ì›€"
        ],
        best_use_cases=[UseCaseType.API_SCRAPING, UseCaseType.SPEED_CRITICAL, UseCaseType.STATIC_CONTENT],
        performance_score=9.5,
        reliability_score=8.5,
        ease_of_use_score=8.0,
        resource_usage="low",
        javascript_support=False,
        async_support=True,
        installation_complexity="easy",
        recommended_fallbacks=[CrawlerToolType.HTTPX_BEAUTIFULSOUP, CrawlerToolType.REQUESTS_BEAUTIFULSOUP]
    ),
    
    CrawlerToolType.PLAYWRIGHT: CrawlerToolSpec(
        tool_type=CrawlerToolType.PLAYWRIGHT,
        display_name="Playwright",
        description="í˜„ëŒ€ì ì¸ ë¸Œë¼ìš°ì € ìë™í™” ë„êµ¬",
        strengths=[
            "ğŸŒŸ ì™„ë²½í•œ JavaScript ì§€ì›",
            "ğŸ­ ë©€í‹° ë¸Œë¼ìš°ì € ì§€ì›",
            "ğŸ“± ëª¨ë°”ì¼ ì‹œë®¬ë ˆì´ì…˜",
            "ğŸ” ë„¤íŠ¸ì›Œí¬ ì¸í„°ì…‰ì…˜",
            "âš¡ ìƒëŒ€ì ìœ¼ë¡œ ë¹ ë¥¸ ì‹¤í–‰"
        ],
        weaknesses=[
            "ğŸ’¾ ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰",
            "ğŸ› ê°„í—ì  ì•ˆì •ì„± ì´ìŠˆ",
            "ğŸ”§ ë³µì¡í•œ ë””ë²„ê¹…",
            "ğŸ’° ë†’ì€ ë¦¬ì†ŒìŠ¤ ë¹„ìš©"
        ],
        best_use_cases=[UseCaseType.DYNAMIC_CONTENT, UseCaseType.FORM_INTERACTION],
        performance_score=8.0,
        reliability_score=7.0,
        ease_of_use_score=7.5,
        resource_usage="high",
        javascript_support=True,
        async_support=True,
        installation_complexity="medium",
        recommended_fallbacks=[CrawlerToolType.SELENIUM, CrawlerToolType.HTTPX_BEAUTIFULSOUP]
    ),
    
    CrawlerToolType.SELENIUM: CrawlerToolSpec(
        tool_type=CrawlerToolType.SELENIUM,
        display_name="Selenium",
        description="ê²€ì¦ëœ ì „í†µì ì¸ ë¸Œë¼ìš°ì € ìë™í™”",
        strengths=[
            "ğŸ›¡ï¸ ë†’ì€ í˜¸í™˜ì„±",
            "ğŸ“š í’ë¶€í•œ ë¬¸ì„œí™”",
            "ğŸ¢ ì—”í„°í”„ë¼ì´ì¦ˆ ê²€ì¦ë¨",
            "ğŸ”§ ì•ˆì •ì ì¸ ë™ì‘",
            "ğŸ‘¥ ëŒ€ê·œëª¨ ì»¤ë®¤ë‹ˆí‹°"
        ],
        weaknesses=[
            "ğŸŒ ìƒëŒ€ì ìœ¼ë¡œ ëŠë¦¼",
            "ğŸ“Š ë†’ì€ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©",
            "âš™ï¸ ë³µì¡í•œ ì„¤ì •",
            "ğŸ”„ ëŠë¦° ì—…ë°ì´íŠ¸"
        ],
        best_use_cases=[UseCaseType.RELIABILITY_CRITICAL, UseCaseType.FORM_INTERACTION],
        performance_score=6.0,
        reliability_score=9.0,
        ease_of_use_score=6.0,
        resource_usage="high",
        javascript_support=True,
        async_support=False,
        installation_complexity="medium",
        recommended_fallbacks=[CrawlerToolType.PLAYWRIGHT, CrawlerToolType.HTTPX_BEAUTIFULSOUP]
    ),
    
    CrawlerToolType.HTTPX_BEAUTIFULSOUP: CrawlerToolSpec(
        tool_type=CrawlerToolType.HTTPX_BEAUTIFULSOUP,
        display_name="HTTPX + BeautifulSoup",
        description="ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ì •ì  ìŠ¤í¬ë˜í•‘ ì¡°í•©",
        strengths=[
            "âš¡ ë§¤ìš° ë¹ ë¥¸ ì²˜ë¦¬ì†ë„",
            "ğŸ¯ ì •í™•í•œ HTML íŒŒì‹±",
            "ğŸ’¡ ì§ê´€ì ì¸ ì‚¬ìš©ë²•",
            "ğŸ’° ì €ë¹„ìš© ë¦¬ì†ŒìŠ¤",
            "ğŸ”„ ë¹„ë™ê¸° ì²˜ë¦¬"
        ],
        weaknesses=[
            "âŒ JavaScript ë¯¸ì§€ì›",
            "âŒ ë™ì  ì½˜í…ì¸  ë¶ˆê°€",
            "âŒ ë³µì¡í•œ ì¸ì¦ ì–´ë ¤ì›€"
        ],
        best_use_cases=[UseCaseType.STATIC_CONTENT, UseCaseType.SPEED_CRITICAL, UseCaseType.LARGE_SCALE],
        performance_score=9.0,
        reliability_score=8.5,
        ease_of_use_score=9.0,
        resource_usage="low",
        javascript_support=False,
        async_support=True,
        installation_complexity="easy",
        recommended_fallbacks=[CrawlerToolType.HTTPX, CrawlerToolType.PLAYWRIGHT]
    ),
    
    CrawlerToolType.REQUESTS_BEAUTIFULSOUP: CrawlerToolSpec(
        tool_type=CrawlerToolType.REQUESTS_BEAUTIFULSOUP,
        display_name="Requests + BeautifulSoup",
        description="ê°€ì¥ ì „í†µì ì´ê³  ì•ˆì •ì ì¸ ì¡°í•©",
        strengths=[
            "ğŸ›¡ï¸ ìµœê³ ì˜ ì•ˆì •ì„±",
            "ğŸ“š ë°©ëŒ€í•œ ë ˆí¼ëŸ°ìŠ¤",
            "ğŸ¯ ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë™ì‘",
            "ğŸ’¡ í•™ìŠµ ìš©ì´",
            "ğŸ”§ ê°„ë‹¨í•œ ë””ë²„ê¹…"
        ],
        weaknesses=[
            "âŒ ë¹„ë™ê¸° ë¯¸ì§€ì›",
            "ğŸŒ ìƒëŒ€ì  ì €ì„±ëŠ¥",
            "âŒ HTTP/2 ë¯¸ì§€ì›"
        ],
        best_use_cases=[UseCaseType.RELIABILITY_CRITICAL, UseCaseType.STATIC_CONTENT],
        performance_score=7.0,
        reliability_score=9.5,
        ease_of_use_score=9.5,
        resource_usage="low",
        javascript_support=False,
        async_support=False,
        installation_complexity="easy",
        recommended_fallbacks=[CrawlerToolType.HTTPX_BEAUTIFULSOUP, CrawlerToolType.SELENIUM]
    ),
    
    CrawlerToolType.SCRAPY: CrawlerToolSpec(
        tool_type=CrawlerToolType.SCRAPY,
        display_name="Scrapy",
        description="ëŒ€ìš©ëŸ‰ ìŠ¤í¬ë˜í•‘ ì „ìš© í”„ë ˆì„ì›Œí¬",
        strengths=[
            "ğŸ­ ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ íŠ¹í™”",
            "âš™ï¸ ê°•ë ¥í•œ ë¯¸ë“¤ì›¨ì–´",
            "ğŸ”„ ìë™ ì¬ì‹œë„",
            "ğŸ“Š ë‚´ì¥ í†µê³„",
            "ğŸ•·ï¸ ë¶„ì‚° ìŠ¤í¬ë˜í•‘"
        ],
        weaknesses=[
            "ğŸ“ˆ ë†’ì€ í•™ìŠµ ê³¡ì„ ",
            "âŒ JavaScript ë¯¸ì§€ì›",
            "ğŸ”§ ë³µì¡í•œ ì„¤ì •",
            "ğŸ’¾ ì˜¤ë²„í—¤ë“œ"
        ],
        best_use_cases=[UseCaseType.LARGE_SCALE, UseCaseType.STATIC_CONTENT],
        performance_score=8.5,
        reliability_score=8.0,
        ease_of_use_score=5.0,
        resource_usage="medium",
        javascript_support=False,
        async_support=True,
        installation_complexity="hard",
        recommended_fallbacks=[CrawlerToolType.HTTPX_BEAUTIFULSOUP, CrawlerToolType.AIOHTTP_BEAUTIFULSOUP]
    ),
    
    CrawlerToolType.HTTPX_LXML: CrawlerToolSpec(
        tool_type=CrawlerToolType.HTTPX_LXML,
        display_name="HTTPX + lxml",
        description="ì´ˆê³ ì„±ëŠ¥ XML/HTML íŒŒì‹± ì¡°í•©",
        strengths=[
            "ğŸš€ ìµœê³  íŒŒì‹± ì„±ëŠ¥",
            "ğŸ’¡ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ",
            "ğŸ¯ XPath ì§€ì›",
            "âš¡ C ê¸°ë°˜ ë¼ì´ë¸ŒëŸ¬ë¦¬"
        ],
        weaknesses=[
            "ğŸ”§ ì„¤ì¹˜ ë³µì¡ì„±",
            "ğŸ“š ìƒëŒ€ì  í•™ìŠµ ê³¡ì„ ",
            "âŒ JavaScript ë¯¸ì§€ì›"
        ],
        best_use_cases=[UseCaseType.LARGE_SCALE, UseCaseType.SPEED_CRITICAL],
        performance_score=9.5,
        reliability_score=8.0,
        ease_of_use_score=6.0,
        resource_usage="low",
        javascript_support=False,
        async_support=True,
        installation_complexity="hard",
        recommended_fallbacks=[CrawlerToolType.HTTPX_BEAUTIFULSOUP, CrawlerToolType.SCRAPY]
    ),
    
    CrawlerToolType.AIOHTTP_BEAUTIFULSOUP: CrawlerToolSpec(
        tool_type=CrawlerToolType.AIOHTTP_BEAUTIFULSOUP,
        display_name="aiohttp + BeautifulSoup",
        description="ë¹„ë™ê¸° HTTP í´ë¼ì´ì–¸íŠ¸ ì¡°í•©",
        strengths=[
            "âš¡ ë¹„ë™ê¸° ê³ ì„±ëŠ¥",
            "ğŸ”„ ë™ì‹œ ì²˜ë¦¬",
            "ğŸ’° íš¨ìœ¨ì  ë¦¬ì†ŒìŠ¤",
            "ğŸ“Š ì„œë²„ ì‚¬ì´ë“œ ìµœì í™”"
        ],
        weaknesses=[
            "ğŸ“ˆ ë¹„ë™ê¸° ë³µì¡ì„±",
            "âŒ JavaScript ë¯¸ì§€ì›",
            "ğŸ”§ ë””ë²„ê¹… ì–´ë ¤ì›€"
        ],
        best_use_cases=[UseCaseType.LARGE_SCALE, UseCaseType.SPEED_CRITICAL],
        performance_score=8.5,
        reliability_score=7.5,
        ease_of_use_score=6.5,
        resource_usage="medium",
        javascript_support=False,
        async_support=True,
        installation_complexity="medium",
        recommended_fallbacks=[CrawlerToolType.HTTPX_BEAUTIFULSOUP, CrawlerToolType.SCRAPY]
    ),
    
    CrawlerToolType.MECHANICALSOUP: CrawlerToolSpec(
        tool_type=CrawlerToolType.MECHANICALSOUP,
        display_name="MechanicalSoup",
        description="ë¸Œë¼ìš°ì € ì‹œë®¬ë ˆì´ì…˜ì´ ê°€ëŠ¥í•œ í¸ë¦¬í•œ ë„êµ¬",
        strengths=[
            "ğŸ­ ë¸Œë¼ìš°ì € ì‹œë®¬ë ˆì´ì…˜",
            "ğŸ“ í¼ ì²˜ë¦¬ í¸ì˜",
            "ğŸ’¡ ê°„ë‹¨í•œ ì‚¬ìš©ë²•",
            "ğŸ”§ ì¿ í‚¤/ì„¸ì…˜ ìë™"
        ],
        weaknesses=[
            "âŒ JavaScript ë¯¸ì§€ì›",
            "ğŸŒ ìƒëŒ€ì  ì €ì„±ëŠ¥",
            "âŒ ì œí•œì  ê¸°ëŠ¥"
        ],
        best_use_cases=[UseCaseType.FORM_INTERACTION, UseCaseType.STATIC_CONTENT],
        performance_score=6.5,
        reliability_score=8.0,
        ease_of_use_score=8.5,
        resource_usage="low",
        javascript_support=False,
        async_support=False,
        installation_complexity="easy",
        recommended_fallbacks=[CrawlerToolType.REQUESTS_BEAUTIFULSOUP, CrawlerToolType.SELENIUM]
    )
}

# ================================
# ì¶”ì²œ ì‹œìŠ¤í…œ ë¡œì§
# ================================

def calculate_tool_score(tool_spec: CrawlerToolSpec, request: CrawlingRequest) -> float:
    """ìš”ì²­ ì‚¬ì–‘ì— ë”°ë¥¸ ë„êµ¬ ì ìˆ˜ ê³„ì‚°"""
    score = 0.0
    max_score = 100.0
    
    # JavaScript ìš”êµ¬ì‚¬í•­ í™•ì¸ (30ì )
    if request.javascript_required:
        if tool_spec.javascript_support:
            score += 30
        else:
            return 0  # JavaScript í•„ìˆ˜ì¸ ê²½ìš° ë¯¸ì§€ì› ë„êµ¬ëŠ” ì œì™¸
    else:
        score += 20  # JavaScript ë¶ˆí•„ìš”ì‹œ ë³´ë„ˆìŠ¤
    
    # í¼ ìƒí˜¸ì‘ìš© ìš”êµ¬ì‚¬í•­ (20ì )
    if request.form_interaction_required:
        if UseCaseType.FORM_INTERACTION in tool_spec.best_use_cases:
            score += 20
        else:
            score += 5  # ë¶€ë¶„ì  ì§€ì›
    else:
        score += 10
    
    # ë³¼ë¥¨ì— ë”°ë¥¸ ì ìˆ˜ (25ì )
    volume_scores = {
        "small": {"performance": 0.3, "reliability": 0.7},
        "medium": {"performance": 0.5, "reliability": 0.5},
        "large": {"performance": 0.7, "reliability": 0.3}
    }
    volume_weight = volume_scores.get(request.expected_volume, volume_scores["medium"])
    volume_score = (tool_spec.performance_score * volume_weight["performance"] + 
                   tool_spec.reliability_score * volume_weight["reliability"]) * 2.5
    score += volume_score
    
    # ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ì ìˆ˜ (15ì )
    priority_weights = {
        "speed": {"performance": 0.8, "reliability": 0.2},
        "reliability": {"performance": 0.2, "reliability": 0.8},
        "balanced": {"performance": 0.5, "reliability": 0.5}
    }
    priority_weight = priority_weights.get(request.priority, priority_weights["balanced"])
    priority_score = (tool_spec.performance_score * priority_weight["performance"] + 
                     tool_spec.reliability_score * priority_weight["reliability"]) * 1.5
    score += priority_score
    
    # ì˜ˆì‚° ì œì•½ì— ë”°ë¥¸ ì ìˆ˜ (10ì )
    resource_scores = {"low": 10, "medium": 7, "high": 3}
    budget_penalty = {"low": 0, "medium": 3, "high": 7}
    
    if tool_spec.resource_usage == "low":
        score += resource_scores.get(request.budget_constraint, 7)
    elif tool_spec.resource_usage == "medium":
        score += max(0, 7 - budget_penalty.get(request.budget_constraint, 3))
    else:  # high
        score += max(0, 3 - budget_penalty.get(request.budget_constraint, 7))
    
    return min(score, max_score)

def get_reasoning(tool_spec: CrawlerToolSpec, request: CrawlingRequest, score: float) -> str:
    """ì¶”ì²œ ì´ìœ  ìƒì„±"""
    reasons = []
    
    if request.javascript_required and tool_spec.javascript_support:
        reasons.append("JavaScript ì²˜ë¦¬ ëŠ¥ë ¥")
    elif not request.javascript_required and not tool_spec.javascript_support:
        reasons.append("ë¹ ë¥¸ ì •ì  ì½˜í…ì¸  ì²˜ë¦¬")
    
    if request.expected_volume == "large" and tool_spec.performance_score >= 8:
        reasons.append("ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ ì„±ëŠ¥")
    
    if request.priority == "speed" and tool_spec.performance_score >= 8:
        reasons.append("ìš°ìˆ˜í•œ ì²˜ë¦¬ ì†ë„")
    elif request.priority == "reliability" and tool_spec.reliability_score >= 8:
        reasons.append("ë†’ì€ ì•ˆì •ì„±")
    
    if tool_spec.resource_usage == "low":
        reasons.append("íš¨ìœ¨ì ì¸ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©")
    
    reasoning = f"{tool_spec.display_name}ì€(ëŠ”) {', '.join(reasons)}ìœ¼ë¡œ ì´ ì‘ì—…ì— ì í•©í•©ë‹ˆë‹¤."
    reasoning += f" ì¢…í•© ì í•©ë„: {score:.1f}%"
    
    return reasoning

# ================================
# API ì—”ë“œí¬ì¸íŠ¸ë“¤
# ================================

@router.get("/available-tools")
async def get_available_tools():
    """ğŸ”§ ì‚¬ìš© ê°€ëŠ¥í•œ í¬ë¡¤ë§ ë„êµ¬ ëª©ë¡ ì¡°íšŒ"""
    try:
        logger.info("[API] í¬ë¡¤ë§ ë„êµ¬ ëª©ë¡ ìš”ì²­ ì²˜ë¦¬ ì¤‘...")
        
        tools_data = []
        for tool_type, spec in CRAWLER_TOOLS_SPECS.items():
            tool_info = spec.dict()
            tool_info["radial_data"] = {
                "performance": spec.performance_score,
                "reliability": spec.reliability_score,
                "ease_of_use": spec.ease_of_use_score,
                "resource_efficiency": 10 - (2 if spec.resource_usage == "high" else 1 if spec.resource_usage == "medium" else 0)
            }
            tools_data.append(tool_info)
        
        logger.info(f"[API] í¬ë¡¤ë§ ë„êµ¬ ëª©ë¡ ë°˜í™˜ - {len(tools_data)}ê°œ ë„êµ¬")
        return {
            "tools": tools_data,
            "total_count": len(tools_data),
            "categories": {
                "browser_automation": ["playwright", "selenium"],
                "http_clients": ["httpx", "httpx_beautifulsoup", "requests_beautifulsoup", "aiohttp_beautifulsoup"],
                "frameworks": ["scrapy"],
                "specialized": ["httpx_lxml", "mechanicalsoup"]
            }
        }
        
    except Exception as e:
        logger.error(f"[API] í¬ë¡¤ë§ ë„êµ¬ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë„êµ¬ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@router.post("/recommend")
async def recommend_tools(request: CrawlingRequest) -> Dict[str, Any]:
    """ğŸ¯ í¬ë¡¤ë§ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ë„êµ¬ ì¶”ì²œ"""
    try:
        logger.info(f"[API] ë„êµ¬ ì¶”ì²œ ìš”ì²­ ì²˜ë¦¬ ì¤‘... Volume: {request.expected_volume}, JSí•„ìš”: {request.javascript_required}")
        
        recommendations = []
        
        for tool_type, tool_spec in CRAWLER_TOOLS_SPECS.items():
            score = calculate_tool_score(tool_spec, request)
            if score > 30:  # ìµœì†Œ ì í•©ë„ 30% ì´ìƒ
                reasoning = get_reasoning(tool_spec, request, score)
                
                recommendation = ToolRecommendation(
                    primary_tool=tool_type,
                    confidence_score=score,
                    reasoning=reasoning,
                    fallback_tools=tool_spec.recommended_fallbacks[:2],  # ìƒìœ„ 2ê°œ í´ë°±
                    setup_steps=generate_setup_steps(tool_spec),
                    estimated_performance=estimate_performance(tool_spec, request)
                )
                recommendations.append(recommendation)
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        recommendations.sort(key=lambda x: x.confidence_score, reverse=True)
        
        logger.info(f"[API] ë„êµ¬ ì¶”ì²œ ì™„ë£Œ - {len(recommendations)}ê°œ ì¶”ì²œ")
        return {
            "recommendations": [r.dict() for r in recommendations[:5]],  # ìƒìœ„ 5ê°œ
            "request_summary": {
                "javascript_required": request.javascript_required,
                "volume": request.expected_volume,
                "priority": request.priority,
                "target_count": len(request.target_urls)
            }
        }
        
    except Exception as e:
        logger.error(f"[API] ë„êµ¬ ì¶”ì²œ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë„êµ¬ ì¶”ì²œ ì‹¤íŒ¨: {str(e)}")

def generate_setup_steps(tool_spec: CrawlerToolSpec) -> List[str]:
    """ë„êµ¬ë³„ ì„¤ì • ë‹¨ê³„ ìƒì„±"""
    base_steps = ["ê°€ìƒí™˜ê²½ ìƒì„±", "ì˜ì¡´ì„± ì„¤ì¹˜"]
    
    if tool_spec.tool_type == CrawlerToolType.PLAYWRIGHT:
        return base_steps + ["playwright install", "ë¸Œë¼ìš°ì € ë‹¤ìš´ë¡œë“œ", "í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì„¤ì •"]
    elif tool_spec.tool_type == CrawlerToolType.SELENIUM:
        return base_steps + ["WebDriver ì„¤ì¹˜", "ë¸Œë¼ìš°ì € ì„¤ì •", "ë“œë¼ì´ë²„ ê²½ë¡œ ì„¤ì •"]
    elif tool_spec.tool_type == CrawlerToolType.SCRAPY:
        return base_steps + ["Scrapy í”„ë¡œì íŠ¸ ìƒì„±", "Spider ìƒì„±", "ì„¤ì • íŒŒì¼ êµ¬ì„±"]
    else:
        return base_steps + ["ë¼ì´ë¸ŒëŸ¬ë¦¬ import", "ê¸°ë³¸ ì„¤ì • ì™„ë£Œ"]

def estimate_performance(tool_spec: CrawlerToolSpec, request: CrawlingRequest) -> Dict[str, Any]:
    """ì„±ëŠ¥ ì¶”ì •"""
    base_speed = tool_spec.performance_score * 10  # requests/minute
    
    if request.expected_volume == "large":
        base_speed *= 0.7
    elif request.expected_volume == "small":
        base_speed *= 1.3
    
    return {
        "estimated_speed_rpm": int(base_speed),
        "estimated_reliability": f"{tool_spec.reliability_score * 10:.0f}%",
        "resource_usage": tool_spec.resource_usage,
        "maintenance_level": "low" if tool_spec.ease_of_use_score >= 8 else "medium" if tool_spec.ease_of_use_score >= 6 else "high"
    }

@router.get("/use-cases")
async def get_use_cases():
    """ğŸ“‹ ì‚¬ìš© ì‚¬ë¡€ë³„ ë„êµ¬ ë§¤íŠ¸ë¦­ìŠ¤ ì¡°íšŒ"""
    try:
        logger.info("[API] ì‚¬ìš© ì‚¬ë¡€ ë§¤íŠ¸ë¦­ìŠ¤ ìš”ì²­ ì²˜ë¦¬ ì¤‘...")
        
        use_case_matrix = {}
        for use_case in UseCaseType:
            suitable_tools = []
            for tool_type, tool_spec in CRAWLER_TOOLS_SPECS.items():
                if use_case in tool_spec.best_use_cases:
                    suitable_tools.append({
                        "tool": tool_type.value,
                        "display_name": tool_spec.display_name,
                        "suitability_score": tool_spec.performance_score if use_case == UseCaseType.SPEED_CRITICAL else tool_spec.reliability_score
                    })
            
            # ì í•©ë„ ì ìˆ˜ìˆœ ì •ë ¬
            suitable_tools.sort(key=lambda x: x["suitability_score"], reverse=True)
            use_case_matrix[use_case.value] = suitable_tools[:3]  # ìƒìœ„ 3ê°œ
        
        logger.info(f"[API] ì‚¬ìš© ì‚¬ë¡€ ë§¤íŠ¸ë¦­ìŠ¤ ë°˜í™˜ - {len(use_case_matrix)}ê°œ ì¼€ì´ìŠ¤")
        return {
            "use_case_matrix": use_case_matrix,
            "use_case_descriptions": {
                "static_content": "ì •ì  HTML/CSS ì½˜í…ì¸  ìˆ˜ì§‘",
                "dynamic_content": "JavaScriptë¡œ ìƒì„±ë˜ëŠ” ë™ì  ì½˜í…ì¸ ",
                "large_scale": "ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ (1000+ í˜ì´ì§€)",
                "form_interaction": "ë¡œê·¸ì¸, í¼ ì œì¶œ ë“± ìƒí˜¸ì‘ìš©",
                "api_scraping": "API ì—”ë“œí¬ì¸íŠ¸ ê¸°ë°˜ ë°ì´í„° ìˆ˜ì§‘",
                "speed_critical": "ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„ê°€ ì¤‘ìš”í•œ ê²½ìš°",
                "reliability_critical": "ì•ˆì •ì„±ì´ ìµœìš°ì„ ì¸ ê²½ìš°"
            }
        }
        
    except Exception as e:
        logger.error(f"[API] ì‚¬ìš© ì‚¬ë¡€ ë§¤íŠ¸ë¦­ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì‚¬ìš© ì‚¬ë¡€ ë§¤íŠ¸ë¦­ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

logger.info("ğŸ”§ í¬ë¡¤ë§ ë„êµ¬ ì„ íƒ ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ")