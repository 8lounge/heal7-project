#!/usr/bin/env python3
"""
ğŸ¯ í†µí•© ê¿ˆí’€ì´ ìˆ˜ì§‘ ì‹œìŠ¤í…œ
- ê¸°ì¡´ ìˆ˜ì§‘ ë¬¸ì œ ì™„ì „ í•´ê²°
- subprocess ê¸°ë°˜ ì•ˆì •ì  DB ì—°ê²°
- ì—ëŸ¬ ë¡œê¹… ë° ì§„í–‰ë¥  í‘œì‹œ
- í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œ ê²€ì¦ëœ ë¡œì§
"""

import requests
import json
import hashlib
import time
import random
from datetime import datetime
from bs4 import BeautifulSoup
import logging
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import urllib.parse
import subprocess
import os

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/home/ubuntu/logs/dream_collection_integrated.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class IntegratedDreamCollector:
    def __init__(self):
        self.collected_count = 0
        self.error_count = 0
        self.duplicate_count = 0
        self.lock = threading.Lock()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # ì•ˆì „í•œ ì‚¬ì´íŠ¸ ëª©ë¡ (ì‹¤ì œ ì‘ë™ í™•ì¸ëœ ê²ƒë§Œ)
        self.sites = {
            'unse2u': {
                'base_url': 'https://www.unse2u.co.kr',
                'patterns': ['dreamview.php?c1=1&c2={}'],
                'range': range(1, 501),  # 500ê°œ ìˆ˜ì§‘
                'verified': True,
                'delay': (1, 2)  # 1-2ì´ˆ ì§€ì—°
            }
        }
    
    def save_to_db_safe(self, source_site, source_url, raw_content):
        """ì•ˆì „í•œ DB ì €ì¥ (subprocess ê¸°ë°˜)"""
        try:
            # JSON ì§ë ¬í™”
            content_str = json.dumps(raw_content, ensure_ascii=False)
            content_hash = hashlib.sha256(content_str.encode()).hexdigest()
            
            # ì„ì‹œ íŒŒì¼ì— JSON ì €ì¥ (íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬)
            temp_file = f"/tmp/dream_temp_{threading.current_thread().ident}.json"
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(raw_content, f, ensure_ascii=False)
            
            try:
                # ë§¤ê°œë³€ìˆ˜í™”ëœ SQL (psqlì—ì„œ ì‹¤í–‰)
                sql_script = f"""
                INSERT INTO dream_raw_collection 
                (source_site, source_url, scraped_at, raw_content, content_hash, collection_status)
                VALUES (
                    '{source_site.replace("'", "''")}',
                    '{source_url.replace("'", "''")}',
                    NOW(),
                    '{content_str.replace("'", "''")}'::jsonb,
                    '{content_hash}',
                    'collected'
                )
                ON CONFLICT (content_hash) DO NOTHING
                RETURNING id;
                """
                
                result = subprocess.run([
                    'sudo', '-u', 'postgres', 'psql', '-d', 'heal7', '-c', sql_script
                ], capture_output=True, text=True, timeout=30)
                
                if result.returncode == 0:
                    if 'INSERT 0 1' in result.stderr or result.stdout.strip():
                        with self.lock:
                            self.collected_count += 1
                        logger.debug(f"âœ… {source_site} DB ì €ì¥ ì„±ê³µ")
                        return True
                    else:
                        with self.lock:
                            self.duplicate_count += 1
                        logger.debug(f"âš ï¸ ì¤‘ë³µ ë°ì´í„°: {content_hash[:8]}")
                        return True  # ì¤‘ë³µë„ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
                else:
                    with self.lock:
                        self.error_count += 1
                    logger.error(f"âŒ DB ì˜¤ë¥˜: {result.stderr}")
                    return False
                    
            finally:
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                if os.path.exists(temp_file):
                    os.remove(temp_file)
                    
        except subprocess.TimeoutExpired:
            logger.error("âŒ DB ì €ì¥ íƒ€ì„ì•„ì›ƒ")
            with self.lock:
                self.error_count += 1
            return False
        except Exception as e:
            logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
            with self.lock:
                self.error_count += 1
            return False
    
    def extract_dream_content(self, soup, url):
        """ê¿ˆí’€ì´ ê´€ë ¨ ë‚´ìš© ì¶”ì¶œ ë° êµ¬ì¡°í™”"""
        try:
            # ì œëª© ì¶”ì¶œ
            title = soup.title.string if soup.title else ""
            
            # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            body_text = soup.get_text(separator=' ', strip=True)
            
            # ê¿ˆí’€ì´ í‚¤ì›Œë“œ í™•ì¸
            dream_keywords = ['ê¿ˆ', 'í•´ëª½', 'ê¸¸ëª½', 'í‰ëª½', 'íƒœëª½', 'ì˜ˆì§€', 'ìš´ì„¸', 'ì ê´˜']
            found_keywords = [word for word in dream_keywords if word in body_text]
            
            if not found_keywords or len(body_text) < 100:
                return None
            
            # êµ¬ì¡°í™”ëœ ë°ì´í„° ìƒì„±
            structured_data = {
                "url": url,
                "title": title[:200],  # ì œëª© ê¸¸ì´ ì œí•œ
                "content": body_text[:1500],  # ë‚´ìš© ê¸¸ì´ ì œí•œ
                "content_length": len(body_text),
                "found_dream_keywords": found_keywords,
                "extracted_at": datetime.now().isoformat(),
                "method": "integrated_collector_v1",
                "quality_score": len(found_keywords) * 0.2 + min(len(body_text) / 1000, 1.0) * 0.8
            }
            
            return structured_data
            
        except Exception as e:
            logger.error(f"âŒ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def collect_from_url(self, site_name, url_pattern, param):
        """URLì—ì„œ ê¿ˆí’€ì´ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            site_config = self.sites[site_name]
            url = f"{site_config['base_url']}/{url_pattern.format(param)}"
            
            # ìš”ì²­ ë° ì‘ë‹µ
            response = self.session.get(url, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ê¿ˆí’€ì´ ë‚´ìš© ì¶”ì¶œ
                dream_data = self.extract_dream_content(soup, url)
                
                if dream_data:
                    # ì‚¬ì´íŠ¸ ì •ë³´ ì¶”ê°€
                    dream_data.update({
                        "source_site": site_name,
                        "param": str(param),
                        "response_status": response.status_code
                    })
                    
                    # DB ì €ì¥
                    success = self.save_to_db_safe(site_name, url, dream_data)
                    
                    if success:
                        logger.info(f"âœ… {site_name}-{param}: {dream_data['title'][:50]}...")
                        return True
            
            # ì„œë²„ ë¶€í•˜ ë°©ì§€
            delay = random.uniform(*site_config['delay'])
            time.sleep(delay)
            
            return False
            
        except requests.RequestException as e:
            logger.warning(f"âš ï¸ {site_name}-{param} ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
            return False
        except Exception as e:
            logger.error(f"âŒ {site_name}-{param} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return False
    
    def run_collection(self):
        """ë©”ì¸ ìˆ˜ì§‘ ì‹¤í–‰"""
        logger.info("ğŸš€ í†µí•© ê¿ˆí’€ì´ ìˆ˜ì§‘ ì‹œìŠ¤í…œ ì‹œì‘!")
        logger.info(f"ğŸ“… ì‹œì‘ ì‹œê°„: {datetime.now()}")
        
        start_time = time.time()
        
        # ì‘ì—… ëª©ë¡ ìƒì„±
        tasks = []
        for site_name, config in self.sites.items():
            if config.get('verified', False):
                if 'range' in config:
                    for num in config['range']:
                        for pattern in config['patterns']:
                            tasks.append((site_name, pattern, num))
        
        total_tasks = len(tasks)
        logger.info(f"ğŸ“Š ì´ {total_tasks}ê°œ ì‘ì—… ì˜ˆì •")
        
        # ë³‘ë ¬ ìˆ˜ì§‘ ì‹¤í–‰
        max_workers = 3  # ì„œë²„ ë¶€í•˜ ìµœì†Œí™”
        success_count = 0
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ëª¨ë“  ì‘ì—… ì œì¶œ
            future_to_task = {
                executor.submit(self.collect_from_url, site_name, pattern, param): f"{site_name}-{param}"
                for site_name, pattern, param in tasks
            }
            
            # ê²°ê³¼ ì²˜ë¦¬ ë° ì§„í–‰ë¥  í‘œì‹œ
            for i, future in enumerate(as_completed(future_to_task, timeout=600), 1):
                task_name = future_to_task[future]
                
                try:
                    if future.result():
                        success_count += 1
                        
                except Exception as e:
                    logger.error(f"âŒ {task_name} ì‘ì—… ì‹¤íŒ¨: {e}")
                    with self.lock:
                        self.error_count += 1
                
                # ì§„í–‰ë¥  ì¶œë ¥ (10ê°œë§ˆë‹¤)
                if i % 10 == 0:
                    progress = (i / total_tasks) * 100
                    elapsed = time.time() - start_time
                    logger.info(f"ğŸ“ˆ ì§„í–‰ë¥ : {i}/{total_tasks} ({progress:.1f}%)")
                    logger.info(f"ğŸ“Š ì„±ê³µ: {success_count}, ì €ì¥: {self.collected_count}, ì¤‘ë³µ: {self.duplicate_count}, ì˜¤ë¥˜: {self.error_count}")
                    logger.info(f"â±ï¸ ê²½ê³¼ì‹œê°„: {elapsed/60:.1f}ë¶„")
        
        # ìµœì¢… ê²°ê³¼ ë³´ê³ 
        total_time = time.time() - start_time
        self._print_final_report(total_tasks, success_count, total_time)
        
        return self.collected_count > 0
    
    def _print_final_report(self, total_tasks, success_count, total_time):
        """ìµœì¢… ê²°ê³¼ ë³´ê³ """
        logger.info("=" * 60)
        logger.info("ğŸ í†µí•© ê¿ˆí’€ì´ ìˆ˜ì§‘ ì™„ë£Œ!")
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š ì „ì²´ ì‘ì—…: {total_tasks}ê°œ")
        logger.info(f"âœ… ìˆ˜ì§‘ ì„±ê³µ: {success_count}ê°œ")
        logger.info(f"ğŸ’¾ DB ì €ì¥: {self.collected_count}ê°œ")
        logger.info(f"ğŸ”„ ì¤‘ë³µ ìŠ¤í‚µ: {self.duplicate_count}ê°œ")
        logger.info(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {self.error_count}ê°œ")
        logger.info(f"â±ï¸ ì´ ì†Œìš”ì‹œê°„: {total_time/60:.1f}ë¶„")
        
        if self.collected_count > 0:
            rate = self.collected_count / (total_time / 60)
            logger.info(f"ğŸš€ í‰ê·  ìˆ˜ì§‘ì†ë„: {rate:.1f}ê°œ/ë¶„")
            
        success_rate = (success_count / total_tasks) * 100 if total_tasks > 0 else 0
        logger.info(f"ğŸ“ˆ ì„±ê³µë¥ : {success_rate:.1f}%")
        logger.info("=" * 60)

def test_db_connection():
    """DB ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        result = subprocess.run([
            'sudo', '-u', 'postgres', 'psql', '-d', 'heal7', '-c', 
            'SELECT COUNT(*) FROM dream_raw_collection;'
        ], capture_output=True, text=True, timeout=10)
        
        if result.returncode == 0:
            logger.info("âœ… PostgreSQL ì—°ê²° ë° í…Œì´ë¸” ì ‘ê·¼ ì„±ê³µ")
            return True
        else:
            logger.error(f"âŒ DB ì—°ê²° ì‹¤íŒ¨: {result.stderr}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ DB í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

if __name__ == "__main__":
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ í™•ì¸
    os.makedirs('/home/ubuntu/logs', exist_ok=True)
    
    # DB ì—°ê²° í…ŒìŠ¤íŠ¸
    if not test_db_connection():
        logger.error("âŒ DB ì—°ê²° ë¶ˆê°€ë¡œ ìˆ˜ì§‘ ì¤‘ë‹¨")
        exit(1)
    
    # ìˆ˜ì§‘ ì‹œìŠ¤í…œ ì‹¤í–‰
    collector = IntegratedDreamCollector()
    success = collector.run_collection()
    
    if success:
        logger.info("ğŸ‰ ìˆ˜ì§‘ ì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤!")
        exit(0)
    else:
        logger.error("ğŸ’¥ ìˆ˜ì§‘ ì‹œìŠ¤í…œ ì‹¤í–‰ ì‹¤íŒ¨")
        exit(1)