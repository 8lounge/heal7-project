#!/usr/bin/env python3
"""
ğŸ¢ ê¸°ì—…ë§ˆë‹¹(bizinfo.go.kr) ì‹¤ì œ ìˆ˜ì§‘ê¸° êµ¬í˜„
ì •ë¶€ ì§€ì›ì‚¬ì—… ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ë°ì´í„° ìˆ˜ì§‘

Author: HEAL7 Development Team
Version: 1.0.0
Date: 2025-08-29
"""

import asyncio
import logging
import re
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from urllib.parse import urljoin, urlparse, parse_qs

import aiohttp
from bs4 import BeautifulSoup

from core_collection_engine import CoreCollectionEngine, create_collection_engine


logger = logging.getLogger(__name__)


class BizinfoCollector:
    """ğŸ¢ ê¸°ì—…ë§ˆë‹¹ ì „ìš© ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, collection_engine: CoreCollectionEngine):
        self.engine = collection_engine
        self.base_url = "https://www.bizinfo.go.kr"
        
        # ê¸°ì—…ë§ˆë‹¹ íŠ¹í™” ì„¤ì •
        self.search_endpoints = {
            'support_programs': '/web/lay1/bbs/S1T122C128/AS/list.do',  # ì§€ì›ì‚¬ì—…
            'funding': '/web/lay1/bbs/S1T122C129/AS/list.do',           # ìê¸ˆì§€ì›  
            'startup': '/web/lay1/bbs/S1T122C130/AS/list.do',          # ì°½ì—…ì§€ì›
        }
        
        # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        self.category_mapping = {
            'C128': 'ì§€ì›ì‚¬ì—…',
            'C129': 'ìê¸ˆì§€ì›', 
            'C130': 'ì°½ì—…ì§€ì›',
            'C131': 'ê¸°ìˆ ê°œë°œ',
            'C132': 'í•´ì™¸ì§„ì¶œ',
            'C133': 'íŒë¡œê°œì²™'
        }
    
    async def collect_support_programs(self, max_pages: int = 10) -> List[Dict]:
        """ì§€ì›ì‚¬ì—… ë°ì´í„° ìˆ˜ì§‘"""
        logger.info(f"ğŸ¢ ê¸°ì—…ë§ˆë‹¹ ì§€ì›ì‚¬ì—… ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {max_pages}í˜ì´ì§€)")
        
        all_programs = []
        
        # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ìˆ˜ì§‘
        for category_code, category_name in self.category_mapping.items():
            try:
                programs = await self._collect_category_programs(
                    category_code, category_name, max_pages
                )
                all_programs.extend(programs)
                logger.info(f"âœ… {category_name} ì¹´í…Œê³ ë¦¬: {len(programs)}ê°œ í”„ë¡œê·¸ë¨ ìˆ˜ì§‘")
                
                # ìš”ì²­ ê°„ ê°„ê²©
                await asyncio.sleep(2.0)
                
            except Exception as e:
                logger.error(f"âŒ {category_name} ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
        
        logger.info(f"ğŸ¯ ê¸°ì—…ë§ˆë‹¹ ì´ {len(all_programs)}ê°œ í”„ë¡œê·¸ë¨ ìˆ˜ì§‘ ì™„ë£Œ")
        return all_programs
    
    async def _collect_category_programs(
        self, 
        category_code: str, 
        category_name: str,
        max_pages: int
    ) -> List[Dict]:
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ í”„ë¡œê·¸ë¨ ìˆ˜ì§‘"""
        programs = []
        
        for page in range(1, max_pages + 1):
            try:
                page_programs = await self._scrape_category_page(
                    category_code, category_name, page
                )
                
                if not page_programs:
                    logger.info(f"ğŸ“„ {category_name} {page}í˜ì´ì§€: ë°ì´í„° ì—†ìŒ (ìˆ˜ì§‘ ì¢…ë£Œ)")
                    break
                
                programs.extend(page_programs)
                logger.debug(f"ğŸ“„ {category_name} {page}í˜ì´ì§€: {len(page_programs)}ê°œ")
                
                # í˜ì´ì§€ ìš”ì²­ ê°„ê²©
                await asyncio.sleep(1.5)
                
            except Exception as e:
                logger.error(f"âŒ {category_name} {page}í˜ì´ì§€ ì‹¤íŒ¨: {str(e)}")
                break
        
        return programs
    
    async def _scrape_category_page(
        self,
        category_code: str,
        category_name: str, 
        page: int
    ) -> List[Dict]:
        """ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘"""
        
        # ê¸°ì—…ë§ˆë‹¹ ëª©ë¡ URL êµ¬ì„±
        list_url = f"{self.base_url}/web/lay1/bbs/S1T122{category_code}/AS/list.do"
        params = {
            'currentPage': str(page),
            'pageSize': '20',  # í•œ í˜ì´ì§€ë‹¹ 20ê°œ í•­ëª©
        }
        
        async with self.engine.session.get(list_url, params=params) as response:
            if response.status != 200:
                logger.warning(f"âš ï¸ HTTP {response.status}: {list_url}")
                return []
            
            html = await response.text()
            soup = BeautifulSoup(html, 'html.parser')
            
            return await self._parse_program_list(soup, category_name)
    
    async def _parse_program_list(self, soup: BeautifulSoup, category_name: str) -> List[Dict]:
        """í”„ë¡œê·¸ë¨ ëª©ë¡ íŒŒì‹±"""
        programs = []
        
        # ê¸°ì—…ë§ˆë‹¹ì˜ ê²Œì‹œíŒ êµ¬ì¡° íŒŒì‹±
        # ì‹¤ì œ êµ¬ì¡°ì— ë§ê²Œ ì…€ë ‰í„° ì¡°ì • í•„ìš”
        program_rows = soup.select('table.board-list tbody tr')
        
        if not program_rows:
            # ëŒ€ì²´ ì…€ë ‰í„° ì‹œë„
            program_rows = soup.select('.list-item, .board-item, .program-item')
        
        for row in program_rows:
            try:
                program = await self._parse_single_program(row, category_name)
                if program and program.get('title'):
                    programs.append(program)
                    
            except Exception as e:
                logger.debug(f"í”„ë¡œê·¸ë¨ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                continue
        
        # ë§Œì•½ í…Œì´ë¸” êµ¬ì¡°ê°€ ì•„ë‹ˆë¼ë©´ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ íŒŒì‹±
        if not programs:
            programs = await self._parse_alternative_structure(soup, category_name)
        
        return programs
    
    async def _parse_single_program(self, row, category_name: str) -> Optional[Dict]:
        """ê°œë³„ í”„ë¡œê·¸ë¨ ì •ë³´ íŒŒì‹±"""
        program = {
            'portal_id': 'bizinfo',
            'category': category_name,
            'scraped_at': datetime.now().isoformat(),
        }
        
        try:
            # ì œëª© ì¶”ì¶œ (ì—¬ëŸ¬ ê°€ëŠ¥í•œ ì…€ë ‰í„°)
            title_element = (
                row.select_one('.title a') or
                row.select_one('.subject a') or  
                row.select_one('td.title a') or
                row.select_one('a[href*="view.do"]')
            )
            
            if title_element:
                program['title'] = title_element.get_text(strip=True)
                program['url'] = urljoin(self.base_url, title_element.get('href', ''))
            
            # ê¸°ê´€ëª… ì¶”ì¶œ
            agency_element = (
                row.select_one('.agency') or
                row.select_one('.organ') or
                row.select_one('td:nth-child(3)') or
                row.select_one('.institution')
            )
            
            if agency_element:
                program['agency'] = agency_element.get_text(strip=True)
            
            # ê¸°ê°„ ì •ë³´ ì¶”ì¶œ
            period_element = (
                row.select_one('.period') or
                row.select_one('.date') or
                row.select_one('td:nth-child(4)')
            )
            
            if period_element:
                program['application_period'] = period_element.get_text(strip=True)
            
            # ìƒì„¸ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ê°€ ìˆ˜ì§‘
            if program.get('url'):
                detailed_info = await self._collect_program_details(program['url'])
                program.update(detailed_info)
            
            return program
            
        except Exception as e:
            logger.debug(f"ê°œë³„ í”„ë¡œê·¸ë¨ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            return None
    
    async def _parse_alternative_structure(self, soup: BeautifulSoup, category_name: str) -> List[Dict]:
        """ëŒ€ì²´ êµ¬ì¡° íŒŒì‹± (ì¹´ë“œ í˜•íƒœ ë“±)"""
        programs = []
        
        # ë‹¤ì–‘í•œ ê°€ëŠ¥í•œ êµ¬ì¡° ì‹œë„
        selectors_to_try = [
            '.program-card',
            '.support-item',
            '.list-box .item',
            '.program-list .item',
            '[class*="program"]',
            '[class*="support"]'
        ]
        
        for selector in selectors_to_try:
            items = soup.select(selector)
            if items:
                logger.info(f"ğŸ” ëŒ€ì²´ êµ¬ì¡° ë°œê²¬: {selector} ({len(items)}ê°œ)")
                
                for item in items:
                    try:
                        program = {
                            'portal_id': 'bizinfo',
                            'category': category_name,
                            'scraped_at': datetime.now().isoformat(),
                        }
                        
                        # ì œëª©
                        title_link = item.select_one('a')
                        if title_link:
                            program['title'] = title_link.get_text(strip=True)
                            program['url'] = urljoin(self.base_url, title_link.get('href', ''))
                        
                        # ê¸°ê´€ëª… (ë‹¤ì–‘í•œ ìœ„ì¹˜ ì‹œë„)
                        agency_text = (
                            item.select_one('.agency, .organ, .institution') or
                            item.select_one('[class*="agency"], [class*="organ"]')
                        )
                        if agency_text:
                            program['agency'] = agency_text.get_text(strip=True)
                        
                        if program.get('title'):
                            programs.append(program)
                            
                    except Exception as e:
                        continue
                
                if programs:
                    break  # ì„±ê³µí•œ ì…€ë ‰í„° ì°¾ìœ¼ë©´ ì¤‘ë‹¨
        
        return programs
    
    async def _collect_program_details(self, detail_url: str) -> Dict:
        """í”„ë¡œê·¸ë¨ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘"""
        try:
            async with self.engine.session.get(detail_url) as response:
                if response.status != 200:
                    return {}
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                details = {}
                
                # ì§€ì›ëŒ€ìƒ ì¶”ì¶œ
                target_element = soup.select_one('.target, .object, [class*="target"]')
                if target_element:
                    details['target_audience'] = target_element.get_text(strip=True)
                
                # ì§€ì›ë‚´ìš© ì¶”ì¶œ  
                content_element = soup.select_one('.content, .detail, .support-detail')
                if content_element:
                    details['support_content'] = content_element.get_text(strip=True)[:1000]
                
                # ì‹ ì²­ê¸°ê°„ ì¶”ì¶œ
                period_element = soup.select_one('.apply-period, .period, [class*="period"]')
                if period_element:
                    details['application_period'] = period_element.get_text(strip=True)
                
                # ë‹´ë‹¹ê¸°ê´€ ì—°ë½ì²˜ ì¶”ì¶œ
                contact_element = soup.select_one('.contact, .phone, [class*="contact"]')
                if contact_element:
                    details['contact_info'] = contact_element.get_text(strip=True)
                
                # ì§€ì›ê·œëª¨/ì˜ˆì‚° ì¶”ì¶œ
                budget_element = soup.select_one('.budget, .scale, [class*="budget"]')
                if budget_element:
                    details['support_scale'] = budget_element.get_text(strip=True)
                
                return details
                
        except Exception as e:
            logger.debug(f"ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨ {detail_url}: {str(e)}")
            return {}


class KStartupCollector:
    """ğŸš€ K-Startup (k-startup.go.kr) ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, collection_engine: CoreCollectionEngine):
        self.engine = collection_engine
        self.base_url = "https://www.k-startup.go.kr"
        
        # K-Startup íŠ¹í™” ì„¤ì •
        self.api_endpoints = {
            'startup_support': '/api/startup/support',
            'biz_support': '/api/biz/support',
            'contest': '/api/contest/list'
        }
    
    async def collect_startup_programs(self, max_pages: int = 10) -> List[Dict]:
        """K-Startup ì°½ì—…ì§€ì› í”„ë¡œê·¸ë¨ ìˆ˜ì§‘"""
        logger.info(f"ğŸš€ K-Startup í”„ë¡œê·¸ë¨ ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {max_pages}í˜ì´ì§€)")
        
        all_programs = []
        
        # K-Startupì€ ì£¼ë¡œ Ajax APIë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ API í˜¸ì¶œ
        for api_name, endpoint in self.api_endpoints.items():
            try:
                programs = await self._collect_from_api(api_name, endpoint, max_pages)
                all_programs.extend(programs)
                logger.info(f"âœ… {api_name}: {len(programs)}ê°œ í”„ë¡œê·¸ë¨ ìˆ˜ì§‘")
                
                await asyncio.sleep(2.0)
                
            except Exception as e:
                logger.error(f"âŒ {api_name} API ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
        
        # APIê°€ ì‹¤íŒ¨í•˜ë©´ ì›¹ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ëŒ€ì²´
        if not all_programs:
            all_programs = await self._collect_from_web_pages(max_pages)
        
        logger.info(f"ğŸ¯ K-Startup ì´ {len(all_programs)}ê°œ í”„ë¡œê·¸ë¨ ìˆ˜ì§‘ ì™„ë£Œ")
        return all_programs
    
    async def _collect_from_api(self, api_name: str, endpoint: str, max_pages: int) -> List[Dict]:
        """K-Startup APIì—ì„œ ë°ì´í„° ìˆ˜ì§‘"""
        programs = []
        
        for page in range(1, max_pages + 1):
            try:
                api_url = f"{self.base_url}{endpoint}"
                params = {
                    'page': page,
                    'size': 20,
                    'sort': 'regDate,desc'
                }
                
                async with self.engine.session.get(api_url, params=params) as response:
                    if response.status != 200:
                        break
                    
                    try:
                        data = await response.json()
                    except:
                        # JSONì´ ì•„ë‹ˆë©´ HTML íŒŒì‹±ìœ¼ë¡œ ì „í™˜
                        break
                    
                    page_programs = await self._parse_api_response(data, api_name)
                    
                    if not page_programs:
                        break
                    
                    programs.extend(page_programs)
                    await asyncio.sleep(1.0)
                    
            except Exception as e:
                logger.debug(f"{api_name} API {page}í˜ì´ì§€ ì‹¤íŒ¨: {str(e)}")
                break
        
        return programs
    
    async def _parse_api_response(self, data: Dict, api_name: str) -> List[Dict]:
        """API ì‘ë‹µ ë°ì´í„° íŒŒì‹±"""
        programs = []
        
        # API ì‘ë‹µ êµ¬ì¡°ì— ë”°ë¼ ë°ì´í„° ì¶”ì¶œ
        items = (
            data.get('content', []) or
            data.get('data', []) or
            data.get('list', []) or
            data.get('items', [])
        )
        
        for item in items:
            try:
                program = {
                    'portal_id': 'kstartup',
                    'category': self._map_api_category(api_name),
                    'title': item.get('title', item.get('name', '')),
                    'agency': item.get('agency', item.get('organization', 'ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€')),
                    'application_period': item.get('applyPeriod', item.get('period', '')),
                    'target_audience': item.get('target', ''),
                    'support_content': item.get('content', item.get('description', ''))[:1000],
                    'scraped_at': datetime.now().isoformat(),
                }
                
                # URL êµ¬ì„±
                if item.get('id'):
                    program['url'] = f"{self.base_url}/web/contents/view.do?schId={item['id']}"
                
                if program.get('title'):
                    programs.append(program)
                    
            except Exception as e:
                logger.debug(f"API ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                continue
        
        return programs
    
    async def _collect_from_web_pages(self, max_pages: int) -> List[Dict]:
        """ì›¹í˜ì´ì§€ì—ì„œ ì§ì ‘ ìˆ˜ì§‘ (API ì‹¤íŒ¨ ì‹œ ëŒ€ì²´)"""
        programs = []
        
        # K-Startup ì£¼ìš” í˜ì´ì§€ë“¤
        page_urls = [
            '/web/contents/bizListPage.do',  # ì‚¬ì—…ê³µê³ 
            '/web/contents/supportListPage.do',  # ì§€ì›ì‚¬ì—…
        ]
        
        for page_url in page_urls:
            try:
                url = f"{self.base_url}{page_url}"
                
                async with self.engine.session.get(url) as response:
                    if response.status != 200:
                        continue
                    
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    page_programs = await self._parse_kstartup_page(soup)
                    programs.extend(page_programs)
                    
                    logger.info(f"ğŸ“„ {page_url}: {len(page_programs)}ê°œ ìˆ˜ì§‘")
                    await asyncio.sleep(2.0)
                    
            except Exception as e:
                logger.error(f"K-Startup í˜ì´ì§€ ìˆ˜ì§‘ ì‹¤íŒ¨ {page_url}: {str(e)}")
        
        return programs
    
    async def _parse_kstartup_page(self, soup: BeautifulSoup) -> List[Dict]:
        """K-Startup í˜ì´ì§€ íŒŒì‹±"""
        programs = []
        
        # K-Startupì˜ ê²Œì‹œíŒ êµ¬ì¡° íŒŒì‹±
        program_items = (
            soup.select('.biz-list .item') or
            soup.select('.support-list .item') or
            soup.select('table tbody tr') or
            soup.select('.program-card')
        )
        
        for item in program_items:
            try:
                program = {
                    'portal_id': 'kstartup',
                    'category': 'ì°½ì—…ì§€ì›',
                    'agency': 'ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€',
                    'scraped_at': datetime.now().isoformat(),
                }
                
                # ì œëª© ë° URL
                title_link = item.select_one('a')
                if title_link:
                    program['title'] = title_link.get_text(strip=True)
                    program['url'] = urljoin(self.base_url, title_link.get('href', ''))
                
                # ê¸°ê°„ ì •ë³´
                period = item.select_one('.period, .date')
                if period:
                    program['application_period'] = period.get_text(strip=True)
                
                if program.get('title'):
                    programs.append(program)
                    
            except Exception as e:
                continue
        
        return programs
    
    def _map_api_category(self, api_name: str) -> str:
        """API ì´ë¦„ì„ ì¹´í…Œê³ ë¦¬ë¡œ ë§¤í•‘"""
        mapping = {
            'startup_support': 'ì°½ì—…ì§€ì›',
            'biz_support': 'ê¸°ì—…ì§€ì›',
            'contest': 'ê³µëª¨ì „'
        }
        return mapping.get(api_name, 'ê¸°íƒ€')


# í†µí•© ìˆ˜ì§‘ ì‹¤í–‰ í•¨ìˆ˜

async def run_comprehensive_collection(
    db_connection_string: str,
    portals: List[str] = ['bizinfo', 'kstartup'],
    max_pages: int = 10
):
    """ì¢…í•©ì ì¸ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰"""
    logger.info("ğŸš€ ì¢…í•© ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
    
    # ìˆ˜ì§‘ ì—”ì§„ ì´ˆê¸°í™”
    engine = await create_collection_engine(db_connection_string)
    
    try:
        results = []
        
        if 'bizinfo' in portals:
            # ê¸°ì—…ë§ˆë‹¹ ìˆ˜ì§‘
            bizinfo_collector = BizinfoCollector(engine)
            
            result = await engine.collect_from_portal(
                portal_id='bizinfo',
                extractor_func=bizinfo_collector.collect_support_programs,
                pages_to_scan=max_pages
            )
            results.append(result)
        
        if 'kstartup' in portals:
            # K-Startup ìˆ˜ì§‘
            kstartup_collector = KStartupCollector(engine)
            
            result = await engine.collect_from_portal(
                portal_id='kstartup', 
                extractor_func=kstartup_collector.collect_startup_programs,
                pages_to_scan=max_pages
            )
            results.append(result)
        
        # ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½
        total_new = sum(r.new_items for r in results)
        total_duplicates = sum(r.duplicates for r in results)
        total_time = sum(r.processing_time for r in results)
        
        logger.info(f"ğŸ¯ ì¢…í•© ìˆ˜ì§‘ ì™„ë£Œ: ì‹ ê·œ {total_new}ê°œ | ì¤‘ë³µ {total_duplicates}ê°œ | {total_time:.1f}ì´ˆ")
        
        return results
        
    finally:
        await engine.close()


# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜
async def test_collection():
    """ìˆ˜ì§‘ê¸° í…ŒìŠ¤íŠ¸"""
    # PostgreSQL ì—°ê²° ë¬¸ìì—´ (ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)
    db_conn = "postgresql://postgres:@localhost:5432/paperworkdb"
    
    try:
        results = await run_comprehensive_collection(
            db_connection_string=db_conn,
            portals=['bizinfo'],  # í…ŒìŠ¤íŠ¸ëŠ” ê¸°ì—…ë§ˆë‹¹ë§Œ
            max_pages=3
        )
        
        for result in results:
            print(f"Portal: {result.portal_id}")
            print(f"Success: {result.success}")
            print(f"New items: {result.new_items}")
            print(f"Duplicates: {result.duplicates}")
            print(f"Time: {result.processing_time:.1f}s")
            print("---")
            
    except Exception as e:
        logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")


if __name__ == "__main__":
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_collection())