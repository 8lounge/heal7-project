"""
ê¸°ì—…ë§ˆë‹¹ (bizinfo.go.kr) ì „ìš© ìŠ¤í¬ë˜í¼
ì •ë¶€ ì§€ì›ì‚¬ì—… í†µí•© í¬í„¸ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

Author: Paperwork AI Team
Version: 2.0.0
Date: 2025-08-23
"""

import asyncio
import hashlib
import logging
import re
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from urllib.parse import urljoin, urlparse, parse_qs

import aiohttp
from bs4 import BeautifulSoup, NavigableString
from dataclasses import dataclass

from utils.rate_limiter import RateLimiter
from utils.content_cleaner import ContentCleaner
from database.db_manager import DatabaseManager

logger = logging.getLogger(__name__)

@dataclass
class ScrapingConfig:
    """ìŠ¤í¬ë˜í•‘ ì„¤ì •"""
    base_url: str = "https://www.bizinfo.go.kr"
    max_pages: int = 50
    max_concurrent: int = 5
    request_delay: float = 1.0
    timeout: int = 30
    retries: int = 3

class BizinfoScraper:
    """ê¸°ì—…ë§ˆë‹¹ ì „ìš© ì§€ëŠ¥í˜• ìŠ¤í¬ë˜í¼"""
    
    def __init__(self, db_manager: DatabaseManager, rate_limiter: RateLimiter):
        self.config = ScrapingConfig()
        self.db_manager = db_manager
        self.rate_limiter = rate_limiter
        self.content_cleaner = ContentCleaner()
        
        # ì„¸ì…˜ ì„¤ì •
        self.session = None
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (compatible; PaperworkAI/2.0; +http://paperwork.heal7.com/bot)',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        self.categories = {
            'all': '',
            'startup': 'STARTUP',       # ì°½ì—…
            'funding': 'FUNDING',       # ìê¸ˆ
            'tech': 'TECH',            # ê¸°ìˆ ê°œë°œ
            'export': 'EXPORT',        # í•´ì™¸ì§„ì¶œ
            'market': 'MARKET',        # íŒë¡œ
            'education': 'EDUCATION'    # êµìœ¡/ì»¨ì„¤íŒ…
        }
        
        # ìˆ˜ì§‘ í†µê³„
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'programs_found': 0,
            'start_time': None
        }
    
    async def initialize(self):
        """ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”"""
        if not self.session:
            connector = aiohttp.TCPConnector(
                limit=self.config.max_concurrent,
                ttl_dns_cache=300,
                use_dns_cache=True,
                limit_per_host=3
            )
            
            timeout = aiohttp.ClientTimeout(total=self.config.timeout)
            
            self.session = aiohttp.ClientSession(
                headers=self.headers,
                connector=connector,
                timeout=timeout,
                cookie_jar=aiohttp.CookieJar()
            )
            
            logger.info("âœ… ê¸°ì—…ë§ˆë‹¹ ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def close(self):
        """ìŠ¤í¬ë˜í¼ ì •ë¦¬"""
        if self.session:
            await self.session.close()
            self.session = None
            logger.info("ğŸ”’ ê¸°ì—…ë§ˆë‹¹ ìŠ¤í¬ë˜í¼ ì„¸ì…˜ ì •ë¦¬ ì™„ë£Œ")
    
    async def get_status(self) -> Dict:
        """ìŠ¤í¬ë˜í¼ ìƒíƒœ ì¡°íšŒ"""
        return {
            "scraper": "bizinfo",
            "status": "active" if self.session else "inactive",
            "stats": self.stats.copy(),
            "last_run": await self.db_manager.get_last_scraping_time('bizinfo')
        }
    
    async def scrape_all_programs(self, force_update: bool = False) -> List[Dict]:
        """ëª¨ë“  ì§€ì›ì‚¬ì—… í”„ë¡œê·¸ë¨ ìŠ¤í¬ë˜í•‘"""
        self.stats['start_time'] = datetime.now()
        
        if not self.session:
            await self.initialize()
        
        logger.info("ğŸš€ ê¸°ì—…ë§ˆë‹¹ ì „ì²´ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
        
        all_programs = []
        
        # ì¹´í…Œê³ ë¦¬ë³„ ìŠ¤í¬ë˜í•‘
        for category_name, category_code in self.categories.items():
            if category_name == 'all':
                continue
                
            logger.info(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {category_name}")
            
            try:
                programs = await self.scrape_category(category_code, force_update)
                all_programs.extend(programs)
                self.stats['programs_found'] += len(programs)
                
                logger.info(f"âœ… {category_name} ì™„ë£Œ: {len(programs)}ê°œ í”„ë¡œê·¸ë¨")
                
                # ì¹´í…Œê³ ë¦¬ê°„ ë”œë ˆì´
                await asyncio.sleep(self.config.request_delay)
                
            except Exception as e:
                logger.error(f"âŒ {category_name} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {str(e)}")
                continue
        
        # ì¤‘ë³µ ì œê±°
        unique_programs = self.remove_duplicates(all_programs)
        
        total_time = (datetime.now() - self.stats['start_time']).total_seconds()
        logger.info(f"ğŸ‰ ê¸°ì—…ë§ˆë‹¹ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(unique_programs)}ê°œ í”„ë¡œê·¸ë¨ ({total_time:.1f}ì´ˆ)")
        
        return unique_programs
    
    async def scrape_category(self, category_code: str, force_update: bool = False) -> List[Dict]:
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ ìŠ¤í¬ë˜í•‘"""
        programs = []
        page = 1
        
        while page <= self.config.max_pages:
            try:
                # Rate limiting ì ìš©
                await self.rate_limiter.acquire()
                
                # í˜ì´ì§€ ìŠ¤í¬ë˜í•‘
                page_programs = await self.scrape_page(category_code, page, force_update)
                
                if not page_programs:
                    logger.info(f"ğŸ“„ {category_code} í˜ì´ì§€ {page}: ë” ì´ìƒ í”„ë¡œê·¸ë¨ì´ ì—†ìŒ")
                    break
                
                programs.extend(page_programs)
                logger.info(f"ğŸ“„ {category_code} í˜ì´ì§€ {page}: {len(page_programs)}ê°œ í”„ë¡œê·¸ë¨")
                
                page += 1
                
            except Exception as e:
                logger.error(f"âŒ {category_code} í˜ì´ì§€ {page} ì‹¤íŒ¨: {str(e)}")
                break
        
        return programs
    
    async def scrape_page(self, category_code: str, page: int, force_update: bool) -> List[Dict]:
        """ê°œë³„ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘"""
        url = f"{self.config.base_url}/web/lay1/biz/PBIZ_0000000000000.do"
        
        params = {
            'searchCondition': 'TITLE',
            'searchKeyword': '',
            'bizTycd': category_code,
            'pageIndex': page,
            'recordCountPerPage': '20'  # í˜ì´ì§€ë‹¹ 20ê°œ
        }
        
        try:
            self.stats['total_requests'] += 1
            
            async with self.session.get(url, params=params) as response:
                if response.status != 200:
                    logger.warning(f"âš ï¸ HTTP {response.status}: {url}")
                    self.stats['failed_requests'] += 1
                    return []
                
                html = await response.text()
                self.stats['successful_requests'] += 1
                
                # HTML íŒŒì‹±
                soup = BeautifulSoup(html, 'html.parser')
                
                # í”„ë¡œê·¸ë¨ ëª©ë¡ ì¶”ì¶œ
                programs = await self.extract_programs_from_page(soup, force_update)
                
                return programs
                
        except asyncio.TimeoutError:
            logger.error(f"â° íƒ€ì„ì•„ì›ƒ: {url}")
            self.stats['failed_requests'] += 1
            return []
        except Exception as e:
            logger.error(f"âŒ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜ {url}: {str(e)}")
            self.stats['failed_requests'] += 1
            return []
    
    async def extract_programs_from_page(self, soup: BeautifulSoup, force_update: bool) -> List[Dict]:
        """í˜ì´ì§€ì—ì„œ í”„ë¡œê·¸ë¨ ëª©ë¡ ì¶”ì¶œ"""
        programs = []
        
        # ë‹¤ì–‘í•œ ì…€ë ‰í„° ì‹œë„ (ì‚¬ì´íŠ¸ êµ¬ì¡° ë³€ê²½ ëŒ€ì‘)
        selectors = [
            '.business-item',
            '.biz-item', 
            '.support-item',
            '.list-item',
            'tr[onclick]',  # í…Œì´ë¸” í˜•íƒœ
            '.row.border'   # Bootstrap ìŠ¤íƒ€ì¼
        ]
        
        program_elements = []
        for selector in selectors:
            elements = soup.select(selector)
            if elements:
                program_elements = elements
                logger.debug(f"ğŸ¯ ì…€ë ‰í„° ì‚¬ìš©: {selector} ({len(elements)}ê°œ)")
                break
        
        if not program_elements:
            logger.warning("âš ï¸ í”„ë¡œê·¸ë¨ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return []
        
        # ê° í”„ë¡œê·¸ë¨ ì •ë³´ ì¶”ì¶œ
        tasks = []
        semaphore = asyncio.Semaphore(self.config.max_concurrent)
        
        for element in program_elements:
            task = self.extract_single_program(semaphore, element, force_update)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"âŒ í”„ë¡œê·¸ë¨ ì¶”ì¶œ ì˜¤ë¥˜: {str(result)}")
                continue
            if result:
                programs.append(result)
        
        return programs
    
    async def extract_single_program(self, semaphore: asyncio.Semaphore, element, force_update: bool) -> Optional[Dict]:
        """ê°œë³„ í”„ë¡œê·¸ë¨ ì •ë³´ ì¶”ì¶œ"""
        async with semaphore:
            try:
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                basic_info = self.extract_basic_info(element)
                
                if not basic_info or not basic_info.get('detail_url'):
                    return None
                
                # ìƒì„¸ ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì¶”ê°€ ìš”ì²­
                if force_update or await self.needs_detail_update(basic_info):
                    detail_info = await self.scrape_program_detail(basic_info['detail_url'])
                    basic_info.update(detail_info)
                
                # í”„ë¡œê·¸ë¨ ID ìƒì„±
                basic_info['program_id'] = self.generate_program_id(basic_info)
                basic_info['hash_value'] = self.generate_content_hash(basic_info)
                basic_info['portal_id'] = 'bizinfo'
                basic_info['scraped_at'] = datetime.now().isoformat()
                
                return basic_info
                
            except Exception as e:
                logger.error(f"âŒ ê°œë³„ í”„ë¡œê·¸ë¨ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                return None
    
    def extract_basic_info(self, element) -> Dict:
        """ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ"""
        info = {}
        
        try:
            # ì œëª© ì¶”ì¶œ (ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„)
            title_selectors = ['.title', 'h3', 'h4', '.subject', '.biz-title', 'strong']
            title = None
            
            for selector in title_selectors:
                title_elem = element.select_one(selector)
                if title_elem:
                    title = self.content_cleaner.clean_text(title_elem.get_text())
                    break
            
            if not title:
                # onclick ì´ë²¤íŠ¸ì—ì„œ ì œëª© ì¶”ì¶œ ì‹œë„
                onclick = element.get('onclick', '')
                if 'bizId' in onclick:
                    title = self.content_cleaner.clean_text(element.get_text())
            
            info['title'] = title or 'N/A'
            
            # ì£¼ê´€ê¸°ê´€ ì¶”ì¶œ
            agency_selectors = ['.agency', '.org', '.institution', '.dept']
            agency = 'N/A'
            
            for selector in agency_selectors:
                agency_elem = element.select_one(selector)
                if agency_elem:
                    agency = self.content_cleaner.clean_text(agency_elem.get_text())
                    break
            
            info['agency'] = agency
            
            # ì‹ ì²­ê¸°ê°„ ì¶”ì¶œ
            period_selectors = ['.period', '.date', '.term', '.deadline']
            period = 'N/A'
            
            for selector in period_selectors:
                period_elem = element.select_one(selector)
                if period_elem:
                    period = self.content_cleaner.clean_text(period_elem.get_text())
                    break
            
            info['application_period'] = period
            
            # ìƒíƒœ ì •ë³´ ì¶”ì¶œ
            status_selectors = ['.status', '.state', '.badge']
            status = 'N/A'
            
            for selector in status_selectors:
                status_elem = element.select_one(selector)
                if status_elem:
                    status = self.content_cleaner.clean_text(status_elem.get_text())
                    break
            
            info['status'] = status
            
            # ìƒì„¸ ë§í¬ ì¶”ì¶œ
            link_elem = element.select_one('a[href]')
            if not link_elem:
                # onclick ì´ë²¤íŠ¸ì—ì„œ ë§í¬ ì •ë³´ ì¶”ì¶œ
                onclick = element.get('onclick', '')
                biz_id_match = re.search(r'bizId[\'\"]\s*:\s*[\'\"](.*?)[\'\"', onclick)
                if biz_id_match:
                    biz_id = biz_id_match.group(1)
                    info['detail_url'] = f"{self.config.base_url}/web/lay1/biz/PBIZ_0000000000001.do?bizId={biz_id}"
                else:
                    info['detail_url'] = None
            else:
                href = link_elem.get('href')
                if href:
                    info['detail_url'] = urljoin(self.config.base_url, href)
                else:
                    info['detail_url'] = None
            
            return info
            
        except Exception as e:
            logger.error(f"âŒ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return {}
    
    async def scrape_program_detail(self, detail_url: str) -> Dict:
        """í”„ë¡œê·¸ë¨ ìƒì„¸ ì •ë³´ ìŠ¤í¬ë˜í•‘"""
        if not detail_url:
            return {}
        
        try:
            await self.rate_limiter.acquire()
            
            async with self.session.get(detail_url) as response:
                if response.status != 200:
                    logger.warning(f"âš ï¸ ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨ {response.status}: {detail_url}")
                    return {}
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                # ìƒì„¸ ì •ë³´ ì¶”ì¶œ
                detail_info = {
                    'support_details': self.extract_support_details(soup),
                    'target_audience': self.extract_target_audience(soup),
                    'required_documents': self.extract_required_documents(soup),
                    'evaluation_criteria': self.extract_evaluation_criteria(soup),
                    'contact_info': self.extract_contact_info(soup),
                    'attachments': self.extract_attachments(soup),
                    'detailed_description': self.extract_detailed_description(soup)
                }
                
                return detail_info
                
        except Exception as e:
            logger.error(f"âŒ ìƒì„¸ ì •ë³´ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ {detail_url}: {str(e)}")
            return {}
    
    def extract_support_details(self, soup: BeautifulSoup) -> Dict:
        """ì§€ì› ë‚´ìš© ì¶”ì¶œ"""
        support_info = {}
        
        try:
            # ì§€ì› ë‚´ìš© ì„¹ì…˜ ì°¾ê¸°
            support_section = soup.select_one('.support-content, .biz-content, .detail-content')
            
            if support_section:
                content_text = self.content_cleaner.clean_text(support_section.get_text())
                
                # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ
                support_info['raw_content'] = content_text
                support_info['support_amount'] = self.extract_support_amount(content_text)
                support_info['support_period'] = self.extract_support_period(content_text)
                support_info['support_type'] = self.extract_support_type(content_text)
            
        except Exception as e:
            logger.error(f"âŒ ì§€ì› ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        
        return support_info
    
    def extract_support_amount(self, text: str) -> str:
        """ì§€ì› ê¸ˆì•¡ ì¶”ì¶œ"""
        patterns = [
            r'(\d{1,3}(?:,\d{3})*(?:\.\d+)?)\s*(?:ì–µ|ë§Œì›|ì›|ë°±ë§Œì›)',
            r'ìµœëŒ€\s*(\d{1,3}(?:,\d{3})*(?:\.\d+)?)\s*(?:ì–µ|ë§Œì›|ì›)',
            r'(\d{1,3}(?:,\d{3})*(?:\.\d+)?)\s*(?:ì²œë§Œì›|ë°±ë§Œì›)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(0)
        
        return 'N/A'
    
    def extract_support_period(self, text: str) -> str:
        """ì§€ì› ê¸°ê°„ ì¶”ì¶œ"""
        patterns = [
            r'(\d+)\s*ê°œì›”',
            r'(\d+)\s*ë…„',
            r'(\d{4})\s*ë…„\s*(\d{1,2})\s*ì›”',
            r'(\d{1,2})\s*ì›”\s*~\s*(\d{1,2})\s*ì›”'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(0)
        
        return 'N/A'
    
    def extract_support_type(self, text: str) -> str:
        """ì§€ì› ë°©ì‹ ì¶”ì¶œ"""
        types = ['ìœµì', 'ë³´ì¡°ê¸ˆ', 'ì§€ì›ê¸ˆ', 'ë°”ìš°ì²˜', 'ì„¸ì•¡ê³µì œ', 'ì»¨ì„¤íŒ…', 'ë©˜í† ë§', 'êµìœ¡']
        
        found_types = []
        for support_type in types:
            if support_type in text:
                found_types.append(support_type)
        
        return ', '.join(found_types) if found_types else 'N/A'
    
    def extract_target_audience(self, soup: BeautifulSoup) -> str:
        """ì§€ì› ëŒ€ìƒ ì¶”ì¶œ"""
        try:
            target_section = soup.select_one('.target, .audience, .subject-area')
            if target_section:
                return self.content_cleaner.clean_text(target_section.get_text())
        except:
            pass
        
        return 'N/A'
    
    def extract_required_documents(self, soup: BeautifulSoup) -> List[str]:
        """í•„ìˆ˜ ì„œë¥˜ ì¶”ì¶œ"""
        documents = []
        
        try:
            # ì„œë¥˜ ëª©ë¡ ì„¹ì…˜ ì°¾ê¸°
            docs_section = soup.select_one('.documents, .attachments, .requirements')
            if docs_section:
                # ë¦¬ìŠ¤íŠ¸ í•­ëª© ì¶”ì¶œ
                items = docs_section.select('li, .item, .doc-item')
                for item in items:
                    doc_text = self.content_cleaner.clean_text(item.get_text())
                    if doc_text and len(doc_text) > 3:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ
                        documents.append(doc_text)
        except:
            pass
        
        return documents
    
    def extract_evaluation_criteria(self, soup: BeautifulSoup) -> List[str]:
        """í‰ê°€ ê¸°ì¤€ ì¶”ì¶œ"""
        criteria = []
        
        try:
            eval_section = soup.select_one('.evaluation, .criteria, .assessment')
            if eval_section:
                items = eval_section.select('li, .item, .criteria-item')
                for item in items:
                    criteria_text = self.content_cleaner.clean_text(item.get_text())
                    if criteria_text and len(criteria_text) > 5:
                        criteria.append(criteria_text)
        except:
            pass
        
        return criteria
    
    def extract_contact_info(self, soup: BeautifulSoup) -> Dict:
        """ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ"""
        contact = {}
        
        try:
            contact_section = soup.select_one('.contact, .info, .inquiry')
            if contact_section:
                text = contact_section.get_text()
                
                # ì „í™”ë²ˆí˜¸ ì¶”ì¶œ
                phone_match = re.search(r'(\d{2,3}-\d{3,4}-\d{4})', text)
                if phone_match:
                    contact['phone'] = phone_match.group(1)
                
                # ì´ë©”ì¼ ì¶”ì¶œ
                email_match = re.search(r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})', text)
                if email_match:
                    contact['email'] = email_match.group(1)
        except:
            pass
        
        return contact
    
    def extract_attachments(self, soup: BeautifulSoup) -> List[Dict]:
        """ì²¨ë¶€ íŒŒì¼ ì •ë³´ ì¶”ì¶œ"""
        attachments = []
        
        try:
            # ì²¨ë¶€íŒŒì¼ ë§í¬ ì°¾ê¸°
            file_links = soup.select('a[href*="download"], a[href*=".pdf"], a[href*=".doc"], a[href*=".hwp"]')
            
            for link in file_links:
                href = link.get('href')
                if href:
                    attachments.append({
                        'filename': self.content_cleaner.clean_text(link.get_text()),
                        'url': urljoin(self.config.base_url, href)
                    })
        except:
            pass
        
        return attachments
    
    def extract_detailed_description(self, soup: BeautifulSoup) -> str:
        """ìƒì„¸ ì„¤ëª… ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            # ì£¼ìš” ì½˜í…ì¸  ì˜ì—­ ì¶”ì¶œ
            content_section = soup.select_one('.content, .detail, .description, .main-content')
            if content_section:
                return self.content_cleaner.clean_text(content_section.get_text())
        except:
            pass
        
        return 'N/A'
    
    async def needs_detail_update(self, basic_info: Dict) -> bool:
        """ìƒì„¸ ì •ë³´ ì—…ë°ì´íŠ¸ í•„ìš” ì—¬ë¶€ í™•ì¸"""
        try:
            existing_program = await self.db_manager.get_program_by_id(basic_info.get('program_id'))
            if not existing_program:
                return True  # ìƒˆë¡œìš´ í”„ë¡œê·¸ë¨
            
            # í•´ì‹œê°’ ë¹„êµë¡œ ë³€ê²½ í™•ì¸
            new_hash = self.generate_content_hash(basic_info)
            return existing_program.get('hash_value') != new_hash
            
        except:
            return True  # ì˜¤ë¥˜ì‹œ ì—…ë°ì´íŠ¸
    
    def generate_program_id(self, program_info: Dict) -> str:
        """í”„ë¡œê·¸ë¨ ê³ ìœ  ID ìƒì„±"""
        # ì œëª© + ê¸°ê´€ + URL ê¸°ë°˜ í•´ì‹œ
        content = f"{program_info.get('title', '')}{program_info.get('agency', '')}{program_info.get('detail_url', '')}"
        return f"bizinfo_{hashlib.md5(content.encode()).hexdigest()[:12]}"
    
    def generate_content_hash(self, program_info: Dict) -> str:
        """ì½˜í…ì¸  ë³€ê²½ ê°ì§€ìš© í•´ì‹œ"""
        # ì£¼ìš” í•„ë“œë“¤ë¡œ í•´ì‹œ ìƒì„±
        key_fields = ['title', 'agency', 'application_period', 'status']
        content = ''.join(str(program_info.get(field, '')) for field in key_fields)
        return hashlib.sha256(content.encode()).hexdigest()
    
    def remove_duplicates(self, programs: List[Dict]) -> List[Dict]:
        """ì¤‘ë³µ í”„ë¡œê·¸ë¨ ì œê±°"""
        seen_ids = set()
        unique_programs = []
        
        for program in programs:
            program_id = program.get('program_id')
            if program_id and program_id not in seen_ids:
                seen_ids.add(program_id)
                unique_programs.append(program)
        
        logger.info(f"ğŸ”„ ì¤‘ë³µ ì œê±°: {len(programs)} â†’ {len(unique_programs)}")
        return unique_programs