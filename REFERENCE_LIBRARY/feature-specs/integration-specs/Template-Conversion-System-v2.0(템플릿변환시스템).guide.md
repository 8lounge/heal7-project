# ğŸ”„ Paperwork AI 2.0 í…œí”Œë¦¿ ë³€í™˜ ì‹œìŠ¤í…œ êµ¬í˜„ ê°€ì´ë“œ v2.0

> **í”„ë¡œì íŠ¸**: "ì œì¶œí•  ë¬¸ì„œì–‘ì‹ì„ AIê°€ ëŒ€ì‹  ì‘ì„±í•´ë“œë¦½ë‹ˆë‹¤" - ê¸°ê´€ê°„ ë¬¸ì„œ ë³€í™˜ ì‹œìŠ¤í…œ  
> **ë²„ì „**: v2.0.0  
> **ì‘ì„±ì¼**: 2025-08-23  
> **ëª©ì **: ë™ì¼í•œ ë‚´ìš©ì„ ë‹¤ë¥¸ ê¸°ê´€ì˜ ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ìë™ ë³€í™˜í•˜ëŠ” í•µì‹¬ ì‹œìŠ¤í…œ  

---

## ğŸ¯ **í•µì‹¬ ë¯¸ì…˜**

### ğŸ’¡ **ì‹¤ì œ ì‚¬ìš©ì ì‹œë‚˜ë¦¬ì˜¤**
```
ğŸ˜° ê¸°ì—… ëŒ€í‘œì˜ í˜„ì‹¤:
"SBAì— ì§€ì›ì‚¬ì—… ì‹ ì²­ì„œë¥¼ ëƒˆëŠ”ë° ë–¨ì–´ì¡Œì–´ìš”. 
ì´ì œ KOSMESì— ë‚´ë ¤ê³  í•˜ëŠ”ë°... ë‚´ìš©ì€ ê°™ì€ë° 
ì–‘ì‹ê³¼ ê°•ì¡°ì ì´ ë‹¬ë¼ì„œ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì¨ì•¼ í•´ìš”."

ğŸ¤– AI ì†”ë£¨ì…˜:
"ê¸°ì¡´ SBA ì‹ ì²­ì„œë¥¼ ì—…ë¡œë“œí•˜ì‹œë©´, 
KOSMES ì–‘ì‹ì— ë§ê²Œ ìë™ìœ¼ë¡œ ë³€í™˜í•´ë“œë¦½ë‹ˆë‹¤!"
```

### ğŸš€ **ë³€í™˜ ì‹œìŠ¤í…œ ê°œìš”**
ì›ë³¸ ë¬¸ì„œì˜ **í•µì‹¬ ë‚´ìš©ê³¼ ë°ì´í„°**ëŠ” ìœ ì§€í•˜ë©´ì„œ, **ëŒ€ìƒ ê¸°ê´€ì˜ í˜•ì‹ê³¼ ê°•ì¡°ì **ì— ë§ê²Œ ìë™ ì¬êµ¬ì„±í•˜ëŠ” ì§€ëŠ¥í˜• ë³€í™˜ ì—”ì§„

---

## ğŸ—ï¸ **ë³€í™˜ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜**

### ğŸ“Š **5ë‹¨ê³„ ë³€í™˜ íŒŒì´í”„ë¼ì¸**
```mermaid
graph TB
    A[ì›ë³¸ ë¬¸ì„œ ë¶„ì„] --> B[ê¸°ê´€ë³„ ì°¨ì´ì  ë§¤í•‘]
    B --> C[ì„¹ì…˜ë³„ ë³€í™˜ ê·œì¹™ ì ìš©]
    C --> D[AI ê¸°ë°˜ ì½˜í…ì¸  ì¬êµ¬ì„±]
    D --> E[í˜•ì‹ ë° ê²€ì¦]
    E --> F[ìµœì¢… ë³€í™˜ ë¬¸ì„œ]
    
    G[ì›ë³¸ ê¸°ê´€ ë°ì´í„°ë² ì´ìŠ¤] --> B
    H[ëŒ€ìƒ ê¸°ê´€ ë°ì´í„°ë² ì´ìŠ¤] --> B
    I[ë³€í™˜ ê·œì¹™ ì—”ì§„] --> C
    J[AI ëª¨ë¸ í—ˆë¸Œ] --> D
    K[í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ] --> E
```

### ğŸ§© **í•µì‹¬ ë³€í™˜ ì—”ì§„**

#### **1. ë¬¸ì„œ ë¶„ì„ ë° ë§¤í•‘ ì‹œìŠ¤í…œ**
```python
class DocumentAnalyzer:
    """ì›ë³¸ ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ë° ì½˜í…ì¸  ë§¤í•‘"""
    
    def __init__(self):
        self.section_classifiers = {
            'business_overview': SectionClassifier('ì‚¬ì—…ê°œìš”'),
            'market_analysis': SectionClassifier('ì‹œì¥ë¶„ì„'),
            'technical_approach': SectionClassifier('ê¸°ìˆ ì ‘ê·¼'),
            'financial_plan': SectionClassifier('ì¬ë¬´ê³„íš'),
            'team_composition': SectionClassifier('íŒ€êµ¬ì„±'),
            'risk_management': SectionClassifier('ìœ„í—˜ê´€ë¦¬'),
            'expected_outcomes': SectionClassifier('ê¸°ëŒ€íš¨ê³¼')
        }
    
    async def analyze_source_document(self, document_path: str, source_institution: str) -> Dict:
        """ì›ë³¸ ë¬¸ì„œ ë¶„ì„ ë° êµ¬ì¡°í™”"""
        
        # 1. ë¬¸ì„œ êµ¬ì¡° íŒŒì•…
        document_structure = await self.extract_document_structure(document_path)
        
        # 2. ì„¹ì…˜ë³„ ë‚´ìš© ë¶„ë¥˜
        classified_sections = {}
        for section_name, content in document_structure.items():
            classification_result = await self.classify_section_content(
                content, section_name, source_institution
            )
            classified_sections[section_name] = classification_result
        
        # 3. í•µì‹¬ ë°ì´í„° ì¶”ì¶œ
        key_data = await self.extract_key_business_data(document_structure)
        
        return {
            'source_institution': source_institution,
            'document_type': await self.identify_document_type(document_structure),
            'sections': classified_sections,
            'key_data': key_data,
            'metadata': {
                'page_count': document_structure.get('page_count'),
                'word_count': document_structure.get('word_count'),
                'creation_date': document_structure.get('creation_date')
            }
        }
    
    async def extract_key_business_data(self, document_structure: Dict) -> Dict:
        """ë¹„ì¦ˆë‹ˆìŠ¤ í•µì‹¬ ë°ì´í„° ì¶”ì¶œ"""
        
        key_data = {
            'company_info': {},
            'financial_data': {},
            'technical_specs': {},
            'market_data': {},
            'team_info': {}
        }
        
        # AI ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ
        extraction_prompt = f"""
        ë‹¤ìŒ ë¬¸ì„œì—ì„œ í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë°ì´í„°ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:
        
        ë¬¸ì„œ ë‚´ìš©:
        {self.format_document_for_analysis(document_structure)}
        
        ì¶”ì¶œí•  ë°ì´í„°:
        1. íšŒì‚¬ ì •ë³´ (íšŒì‚¬ëª…, ëŒ€í‘œì, ì„¤ë¦½ì—°ë„, ì§ì›ìˆ˜, ì£¼ì†Œ)
        2. ì¬ë¬´ ë°ì´í„° (ë§¤ì¶œ, ìë³¸ê¸ˆ, ì†ìµ, ìê¸ˆì¡°ë‹¬ ê³„íš)
        3. ê¸°ìˆ  ì‚¬ì–‘ (í•µì‹¬ê¸°ìˆ , íŠ¹í—ˆ, R&D íˆ¬ì)
        4. ì‹œì¥ ë°ì´í„° (ì‹œì¥ê·œëª¨, ê²½ìŸì‚¬, íƒ€ê²Ÿ ê³ ê°)
        5. íŒ€ ì •ë³´ (í•µì‹¬ì¸ë ¥, ê²½ë ¥, ì—­í• )
        
        JSON í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”í•˜ì—¬ ë°˜í™˜í•´ì£¼ì„¸ìš”.
        """
        
        extracted_data = await self.call_ai_model('data_extractor', extraction_prompt)
        return self.parse_extracted_data(extracted_data)
```

#### **2. ê¸°ê´€ë³„ ì°¨ì´ì  ë¶„ì„ ì—”ì§„**
```python
class InstitutionDifferenceAnalyzer:
    """ê¸°ê´€ê°„ ìš”êµ¬ì‚¬í•­ ì°¨ì´ì  ë¶„ì„ ë° ë§¤í•‘"""
    
    def __init__(self):
        self.db = DatabaseManager()
        self.difference_analyzer = AIModelManager('claude-3.5-sonnet')
    
    async def analyze_conversion_requirements(
        self, 
        source_institution: str, 
        target_institution: str, 
        document_type: str
    ) -> Dict:
        """ë³€í™˜ì„ ìœ„í•œ ê¸°ê´€ë³„ ì°¨ì´ì  ë¶„ì„"""
        
        # 1. ê¸°ê´€ë³„ ìš”êµ¬ì‚¬í•­ ì¡°íšŒ
        source_reqs = await self.get_institution_requirements(source_institution, document_type)
        target_reqs = await self.get_institution_requirements(target_institution, document_type)
        
        # 2. ì„¹ì…˜ë³„ ì°¨ì´ì  ë§¤í•‘
        section_differences = {}
        for target_section in target_reqs['sections']:
            section_differences[target_section] = await self.map_section_differences(
                target_section, source_reqs, target_reqs
            )
        
        # 3. í˜•ì‹ ë³€í™˜ ìš”êµ¬ì‚¬í•­
        format_changes = await self.analyze_format_differences(source_reqs, target_reqs)
        
        # 4. ê°•ì¡°ì  ë³€í™” ë¶„ì„
        emphasis_changes = await self.analyze_emphasis_differences(source_reqs, target_reqs)
        
        return {
            'conversion_type': f"{source_institution}_to_{target_institution}",
            'difficulty_level': self.calculate_conversion_difficulty(source_reqs, target_reqs),
            'section_mapping': section_differences,
            'format_changes': format_changes,
            'emphasis_changes': emphasis_changes,
            'special_considerations': await self.identify_special_considerations(
                source_institution, target_institution
            )
        }
    
    async def map_section_differences(self, target_section: str, source_reqs: Dict, target_reqs: Dict) -> Dict:
        """ê°œë³„ ì„¹ì…˜ì˜ ë³€í™˜ ìš”êµ¬ì‚¬í•­ ë¶„ì„"""
        
        mapping_prompt = f"""
        ë‹¤ìŒ ì„¹ì…˜ì„ ê¸°ê´€ Aì—ì„œ ê¸°ê´€ Bë¡œ ë³€í™˜í•  ë•Œì˜ ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:
        
        ëŒ€ìƒ ì„¹ì…˜: {target_section}
        
        ì›ë³¸ ê¸°ê´€ ({source_reqs['institution']}) ìš”êµ¬ì‚¬í•­:
        {json.dumps(source_reqs.get('sections', {}).get(target_section, {}), indent=2)}
        
        ëŒ€ìƒ ê¸°ê´€ ({target_reqs['institution']}) ìš”êµ¬ì‚¬í•­:
        {json.dumps(target_reqs.get('sections', {}).get(target_section, {}), indent=2)}
        
        ë¶„ì„ ê²°ê³¼ë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì œê³µí•´ì£¼ì„¸ìš”:
        1. ë³€í™˜ ë‚œì´ë„ (1-5ì )
        2. ì£¼ìš” ë³€ê²½ì‚¬í•­
        3. ê°•ì¡°ì  ë³€í™”
        4. ê¸¸ì´ ì¡°ì • í•„ìš”ì„±
        5. ì¶”ê°€/ì œê±°í•  ë‚´ìš©
        6. í†¤ì•¤ë§¤ë„ˆ ë³€ê²½ì 
        """
        
        analysis_result = await self.difference_analyzer.generate(mapping_prompt)
        return self.parse_section_mapping(analysis_result)
```

#### **3. ì½˜í…ì¸  ë³€í™˜ ì‹¤í–‰ ì—”ì§„**
```python
class ContentConverter:
    """ì‹¤ì œ ì½˜í…ì¸  ë³€í™˜ì„ ìˆ˜í–‰í•˜ëŠ” í•µì‹¬ ì—”ì§„"""
    
    def __init__(self):
        self.ai_models = {
            'content_rewriter': 'gpt-4o',           # ì½˜í…ì¸  ì¬ì‘ì„±
            'technical_adapter': 'claude-3.5',      # ê¸°ìˆ ì  ë‚´ìš© ì ì‘
            'financial_converter': 'gemini-pro',    # ì¬ë¬´ ë°ì´í„° ë³€í™˜
            'quality_validator': 'gpt-4.1'          # í’ˆì§ˆ ê²€ì¦
        }
    
    async def convert_document_sections(
        self, 
        analyzed_content: Dict, 
        conversion_requirements: Dict,
        target_institution: str
    ) -> Dict:
        """ë¬¸ì„œ ì„¹ì…˜ë³„ ë³€í™˜ ì‹¤í–‰"""
        
        converted_sections = {}
        
        for section_id, section_data in analyzed_content['sections'].items():
            # 1. í•´ë‹¹ ì„¹ì…˜ì˜ ë³€í™˜ ê·œì¹™ ì ìš©
            conversion_rule = conversion_requirements['section_mapping'].get(section_id)
            if not conversion_rule:
                continue
            
            # 2. AI ëª¨ë¸ì„ ì‚¬ìš©í•œ ì½˜í…ì¸  ë³€í™˜
            converted_content = await self.convert_section_content(
                section_data, conversion_rule, target_institution
            )
            
            # 3. í’ˆì§ˆ ê²€ì¦ ë° ê°œì„ 
            validated_content = await self.validate_and_improve_section(
                converted_content, conversion_rule
            )
            
            converted_sections[section_id] = validated_content
        
        return converted_sections
    
    async def convert_section_content(self, section_data: Dict, conversion_rule: Dict, target_institution: str) -> Dict:
        """ê°œë³„ ì„¹ì…˜ ì½˜í…ì¸  ë³€í™˜"""
        
        # ê¸°ê´€ë³„ ë§ì¶¤í˜• í”„ë¡¬í”„íŠ¸ ìƒì„±
        conversion_prompt = self.build_conversion_prompt(
            section_data, conversion_rule, target_institution
        )
        
        # AI ëª¨ë¸ ì„ íƒ (ì½˜í…ì¸  íƒ€ì…ì— ë”°ë¼)
        model_type = self.select_optimal_model(section_data['content_type'])
        
        # ë³€í™˜ ì‹¤í–‰
        converted_result = await self.ai_models[model_type].generate(conversion_prompt)
        
        return {
            'original_content': section_data['content'],
            'converted_content': converted_result,
            'conversion_method': model_type,
            'applied_rules': conversion_rule,
            'confidence_score': await self.calculate_conversion_confidence(
                section_data['content'], converted_result, conversion_rule
            )
        }
    
    def build_conversion_prompt(self, section_data: Dict, conversion_rule: Dict, target_institution: str) -> str:
        """ê¸°ê´€ë³„ ë§ì¶¤í˜• ë³€í™˜ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        institution_profiles = {
            'sba': {
                'tone': 'í˜ì‹ ì ì´ê³  ì§„ì·¨ì ì¸',
                'focus': 'ê¸°ìˆ í˜ì‹ ê³¼ ê¸€ë¡œë²Œ ê²½ìŸë ¥',
                'keywords': ['í˜ì‹ ', 'ì°¨ë³„í™”', 'ë…ì°½ì„±', 'ê¸€ë¡œë²Œ'],
                'structure': 'ê°œì¡°ì‹ ì¤‘ì‹¬, êµ¬ì²´ì  ìˆ˜ì¹˜ ê°•ì¡°'
            },
            'kosmes': {
                'tone': 'ì‹¤ìš©ì ì´ê³  ì•ˆì •ì ì¸',
                'focus': 'ì‹¤ìš©í™” ê°€ëŠ¥ì„±ê³¼ ì‚¬ì—…í™” ì—­ëŸ‰',
                'keywords': ['ì‹¤ìš©í™”', 'ìƒìš©í™”', 'ì•ˆì •ì„±', 'ì§€ì†ê°€ëŠ¥ì„±'],
                'structure': 'ì„œìˆ í˜•ê³¼ ê°œì¡°ì‹ í˜¼ìš©, êµ¬ì²´ì  ì‹¤í–‰ê³„íš'
            },
            'nipa': {
                'tone': 'ê¸°ìˆ ì ì´ê³  ì „ë¬¸ì ì¸',
                'focus': 'IT ê¸°ìˆ ë ¥ê³¼ ë””ì§€í„¸ í˜ì‹ ',
                'keywords': ['ë””ì§€í„¸', 'í”Œë«í¼', 'ë°ì´í„°', 'ì•Œê³ ë¦¬ì¦˜'],
                'structure': 'ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ ì¤‘ì‹¬, ë…¼ë¦¬ì  êµ¬ì¡°'
            },
            'techno': {
                'tone': 'ì‹ ë¢°ì„± ìˆê³  ë³´ìˆ˜ì ì¸',
                'focus': 'ê¸°ìˆ ì  ì™„ì„±ë„ì™€ ì‹œì¥ ì•ˆì •ì„±',
                'keywords': ['ê²€ì¦', 'ì•ˆì •ì„±', 'ì‹ ë¢°ì„±', 'ì§€ì†ì„±'],
                'structure': 'ë³´ìˆ˜ì  ì ‘ê·¼, ë¦¬ìŠ¤í¬ ê´€ë¦¬ ê°•ì¡°'
            }
        }
        
        target_profile = institution_profiles.get(target_institution, institution_profiles['sba'])
        
        return f"""
        ë‹¤ìŒ ë‚´ìš©ì„ {target_institution} ê¸°ê´€ì˜ ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ë³€í™˜í•´ì£¼ì„¸ìš”:
        
        ì›ë³¸ ë‚´ìš©:
        {section_data['content']}
        
        ë³€í™˜ ê·œì¹™:
        - í†¤ì•¤ë§¤ë„ˆ: {target_profile['tone']}
        - í•µì‹¬ í¬ì»¤ìŠ¤: {target_profile['focus']}
        - ê°•ì¡° í‚¤ì›Œë“œ: {', '.join(target_profile['keywords'])}
        - ë¬¸ì„œ êµ¬ì¡°: {target_profile['structure']}
        
        ì„¸ë¶€ ë³€í™˜ ìš”êµ¬ì‚¬í•­:
        - ê¸¸ì´ ì¡°ì •: {conversion_rule.get('length_adjustment', 'ì›ë³¸ ìœ ì§€')}
        - ê°•ì¡°ì  ë³€í™”: {conversion_rule.get('emphasis_changes', [])}
        - ì¶”ê°€í•  ë‚´ìš©: {conversion_rule.get('content_additions', [])}
        - ì œê±°í•  ë‚´ìš©: {conversion_rule.get('content_removals', [])}
        
        ì§€ì‹œì‚¬í•­:
        1. í•µì‹¬ ì •ë³´ì™€ ë°ì´í„°ëŠ” ë³´ì¡´í•˜ë˜, í‘œí˜„ ë°©ì‹ì„ ì¡°ì •í•˜ì„¸ìš”
        2. {target_institution}ì—ì„œ ì¤‘ìš”í•˜ê²Œ í‰ê°€í•˜ëŠ” ìš”ì†Œë¥¼ ê°•ì¡°í•˜ì„¸ìš”
        3. í•´ë‹¹ ê¸°ê´€ì˜ í‰ê°€ ê¸°ì¤€ì— ë§ëŠ” ë…¼ë¦¬ì  êµ¬ì¡°ë¡œ ì¬êµ¬ì„±í•˜ì„¸ìš”
        4. ì „ë¬¸ì ì´ê³  ì„¤ë“ë ¥ ìˆëŠ” ë¬¸ì²´ë¥¼ ìœ ì§€í•˜ì„¸ìš”
        
        ë³€í™˜ ê²°ê³¼:
        """
```

---

## ğŸ—„ï¸ **ë³€í™˜ ê·œì¹™ ë°ì´í„°ë² ì´ìŠ¤**

### ğŸ“Š **ê¸°ê´€ë³„ ë³€í™˜ ë§¤íŠ¸ë¦­ìŠ¤**
```sql
-- ê¸°ê´€ë³„ ë³€í™˜ ê·œì¹™ í…Œì´ë¸”
CREATE TABLE conversion_rules (
    id SERIAL PRIMARY KEY,
    source_institution VARCHAR(20) NOT NULL,
    target_institution VARCHAR(20) NOT NULL,
    document_type VARCHAR(50) NOT NULL,
    section_id VARCHAR(50) NOT NULL,
    conversion_difficulty INTEGER DEFAULT 1,    -- 1-5ì 
    transformation_rules JSONB NOT NULL,        -- ë³€í™˜ ê·œì¹™
    success_rate FLOAT DEFAULT 0.0,             -- ë³€í™˜ ì„±ê³µë¥ 
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- ë³€í™˜ ì„±ê³µë¥  ì¶”ì ì„ ìœ„í•œ ì´ë ¥ í…Œì´ë¸”
CREATE TABLE conversion_history (
    id SERIAL PRIMARY KEY,
    user_id INTEGER,
    source_institution VARCHAR(20) NOT NULL,
    target_institution VARCHAR(20) NOT NULL,
    document_type VARCHAR(50) NOT NULL,
    conversion_quality_score FLOAT,             -- 1-5ì  ì‚¬ìš©ì í‰ê°€
    processing_time_seconds INTEGER,
    success BOOLEAN DEFAULT true,
    feedback TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);

-- ê¸°ê´€ë³„ ì„ í˜¸ ìŠ¤íƒ€ì¼ ë§¤íŠ¸ë¦­ìŠ¤
INSERT INTO conversion_rules (source_institution, target_institution, document_type, section_id, conversion_difficulty, transformation_rules) VALUES
-- SBA â†’ KOSMES ë³€í™˜ ì˜ˆì‹œ
('sba', 'kosmes', 'support_business', 'business_overview', 2, '{
    "tone_adjustment": "í˜ì‹ ì  â†’ ì‹¤ìš©ì ",
    "length_change": "+20%",
    "emphasis_shift": ["ê¸°ìˆ í˜ì‹  â†’ ìƒìš©í™” ê°€ëŠ¥ì„±", "ê¸€ë¡œë²Œ ì§„ì¶œ â†’ ì•ˆì •ì  ì„±ì¥"],
    "structural_change": "ê°œì¡°ì‹ ìœ ì§€, êµ¬ì²´ì  ì‹¤í–‰ê³„íš ì¶”ê°€",
    "keyword_replacement": {
        "í˜ì‹ ": "ì‹¤ìš©í™”",
        "ì°¨ë³„í™”": "ìƒìš©í™”",
        "ê¸€ë¡œë²Œ": "ì•ˆì •ì "
    }
}'),

-- NIPA â†’ SBA ë³€í™˜ ì˜ˆì‹œ
('nipa', 'sba', 'support_business', 'technical_approach', 3, '{
    "tone_adjustment": "ê¸°ìˆ ì¤‘ì‹¬ â†’ ì‚¬ì—…ì¤‘ì‹¬",
    "length_change": "-15%",
    "emphasis_shift": ["ê¸°ìˆ ìŠ¤í™ â†’ ì‹œì¥ì„±", "í”Œë«í¼ êµ¬ì¡° â†’ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸"],
    "structural_change": "ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ ì¶•ì†Œ, ì‚¬ì—…ì  ê°€ì¹˜ ê°•ì¡°",
    "additional_content": ["ì‹œì¥ ì§„ì¶œ ì „ëµ", "ê¸€ë¡œë²Œ í™•ì¥ì„±"]
}');
```

---

## ğŸ”§ **ë³€í™˜ í’ˆì§ˆ ê´€ë¦¬ ì‹œìŠ¤í…œ**

### ğŸ¯ **ë‹¤ë‹¨ê³„ í’ˆì§ˆ ê²€ì¦**
```python
class ConversionQualityManager:
    """ë³€í™˜ í’ˆì§ˆ ê´€ë¦¬ ë° ìµœì í™” ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.quality_metrics = {
            'content_preservation': 0.3,    # ë‚´ìš© ë³´ì¡´ë„
            'format_compliance': 0.25,      # í˜•ì‹ ì¤€ìˆ˜ë„  
            'institution_alignment': 0.25,  # ê¸°ê´€ ì í•©ë„
            'readability': 0.2              # ê°€ë…ì„±
        }
    
    async def evaluate_conversion_quality(self, conversion_result: Dict) -> Dict:
        """ë³€í™˜ ê²°ê³¼ í’ˆì§ˆ ì¢…í•© í‰ê°€"""
        
        quality_scores = {}
        
        # 1. ë‚´ìš© ë³´ì¡´ë„ í‰ê°€
        quality_scores['content_preservation'] = await self.evaluate_content_preservation(
            conversion_result['original_content'],
            conversion_result['converted_content']
        )
        
        # 2. í˜•ì‹ ì¤€ìˆ˜ë„ í‰ê°€
        quality_scores['format_compliance'] = await self.evaluate_format_compliance(
            conversion_result['converted_content'],
            conversion_result['target_requirements']
        )
        
        # 3. ê¸°ê´€ ì í•©ë„ í‰ê°€
        quality_scores['institution_alignment'] = await self.evaluate_institution_alignment(
            conversion_result['converted_content'],
            conversion_result['target_institution']
        )
        
        # 4. ê°€ë…ì„± í‰ê°€
        quality_scores['readability'] = await self.evaluate_readability(
            conversion_result['converted_content']
        )
        
        # 5. ì¢…í•© ì ìˆ˜ ê³„ì‚°
        overall_score = sum(
            score * weight 
            for score, weight in zip(quality_scores.values(), self.quality_metrics.values())
        )
        
        return {
            'overall_score': overall_score,
            'detailed_scores': quality_scores,
            'improvement_suggestions': await self.generate_improvement_suggestions(quality_scores),
            'pass_threshold': overall_score >= 0.7  # 70% ì´ìƒ í†µê³¼
        }
    
    async def generate_improvement_suggestions(self, quality_scores: Dict) -> List[str]:
        """í’ˆì§ˆ ê°œì„  ì œì•ˆ ìƒì„±"""
        
        suggestions = []
        
        if quality_scores['content_preservation'] < 0.7:
            suggestions.append("í•µì‹¬ ì •ë³´ê°€ ëˆ„ë½ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤. ì›ë³¸ ë‚´ìš©ì„ ë” ìì„¸íˆ ë³´ì¡´í•˜ì„¸ìš”.")
        
        if quality_scores['format_compliance'] < 0.7:
            suggestions.append("ëŒ€ìƒ ê¸°ê´€ì˜ í˜•ì‹ ìš”êµ¬ì‚¬í•­ì„ ë” ì •í™•íˆ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        if quality_scores['institution_alignment'] < 0.7:
            suggestions.append("ëŒ€ìƒ ê¸°ê´€ì˜ í‰ê°€ ê¸°ì¤€ê³¼ ì„ í˜¸ ìŠ¤íƒ€ì¼ì„ ë” ë°˜ì˜í•˜ì„¸ìš”.")
        
        if quality_scores['readability'] < 0.7:
            suggestions.append("ë¬¸ì„œì˜ ê°€ë…ì„±ê³¼ ë…¼ë¦¬ì  íë¦„ì„ ê°œì„ í•˜ì„¸ìš”.")
        
        return suggestions
```

---

## ğŸš€ **êµ¬í˜„ ë¡œë“œë§µ**

### ğŸ“… **Phase 1: í•µì‹¬ ë³€í™˜ ì—”ì§„ (2ì£¼)**
- [ ] ë¬¸ì„œ ë¶„ì„ ë° êµ¬ì¡°í™” ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] ê¸°ê´€ë³„ ì°¨ì´ì  ë§¤í•‘ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ
- [ ] ê¸°ë³¸ ë³€í™˜ ê·œì¹™ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• (SBA â†” KOSMES)

### ğŸ“… **Phase 2: AI ë³€í™˜ ê³ ë„í™” (2ì£¼)**
- [ ] ë©€í‹° AI ëª¨ë¸ ë³€í™˜ ì‹œìŠ¤í…œ êµ¬ì¶•
- [ ] í’ˆì§ˆ í‰ê°€ ë° ê°œì„  ì‹œìŠ¤í…œ ê°œë°œ
- [ ] ì‹¤ì‹œê°„ ë³€í™˜ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### ğŸ“… **Phase 3: ì „ì²´ ê¸°ê´€ í™•ì¥ (2ì£¼)**  
- [ ] ì „ì²´ ê¸°ê´€ ë³€í™˜ ë§¤íŠ¸ë¦­ìŠ¤ ì™„ì„± (NIPA, TECHNO í¬í•¨)
- [ ] ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ìë™ í•™ìŠµ ì‹œìŠ¤í…œ
- [ ] ë³€í™˜ ê²°ê³¼ ìµœì í™” ì—”ì§„

---

**ğŸ’¡ í•µì‹¬ ê°€ì¹˜**: "í•œ ë²ˆ ì‘ì„±ìœ¼ë¡œ ëª¨ë“  ê¸°ê´€ ëŒ€ì‘" - ë™ì¼í•œ ì‚¬ì—… ë‚´ìš©ì„ ê° ê¸°ê´€ì˜ íŠ¹ì„±ê³¼ ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ AIê°€ ìë™ìœ¼ë¡œ ìµœì í™”í•˜ì—¬ ë³€í™˜í•˜ëŠ” í˜ì‹ ì ì¸ ì‹œìŠ¤í…œ

*ğŸ“ ì´ ì‹œìŠ¤í…œìœ¼ë¡œ ê¸°ì—…ë“¤ì´ ì—¬ëŸ¬ ê¸°ê´€ì— ì§€ì›í•  ë•Œ ê²ªëŠ” "ê°™ì€ ë‚´ìš©, ë‹¤ë¥¸ ì–‘ì‹" ë¬¸ì œë¥¼ ì™„ì „íˆ í•´ê²°í•˜ê³ , ì§€ì› ì„±ê³µë¥ ì„ íšê¸°ì ìœ¼ë¡œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.*