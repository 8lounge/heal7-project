#!/usr/bin/env python3
"""
ğŸ”¥ ì‹¤ì‹œê°„ í¬ë¡¤ë§ ëŒ€ì‹œë³´ë“œ API
ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ ëŒ€ì‹œë³´ë“œ ì—”ë“œí¬ì¸íŠ¸

Author: HEAL7 Development Team
Version: 2.0.0 (ì‹¤ì œ DB ì—°ë™)
Date: 2025-08-29
"""

import asyncio
import json
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any
import logging

import asyncpg
from fastapi import APIRouter, HTTPException, Query, Depends, BackgroundTasks
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field

logger = logging.getLogger(__name__)

# FastAPI ë¼ìš°í„° ìƒì„±
router = APIRouter(
    prefix="/api/scraping-dashboard",
    tags=["ì‹¤ì‹œê°„ í¬ë¡¤ë§ ëŒ€ì‹œë³´ë“œ"],
    responses={404: {"description": "Not found"}},
)

# Pydantic ëª¨ë¸ë“¤
class CollectionListResponse(BaseModel):
    success: bool
    data: Dict[str, Any]
    timestamp: str
    message: Optional[str] = None

class StatsResponse(BaseModel):
    success: bool
    data: Dict[str, Any] 
    timestamp: str
    message: Optional[str] = None

class CollectionTriggerRequest(BaseModel):
    portal_ids: List[str] = Field(..., description="ìˆ˜ì§‘í•  í¬í„¸ ëª©ë¡ (bizinfo, kstartup)")
    max_pages: int = Field(5, ge=1, le=20, description="ìµœëŒ€ í˜ì´ì§€ ìˆ˜")
    force_update: bool = Field(False, description="ê°•ì œ ì—…ë°ì´íŠ¸ ì—¬ë¶€")

class CollectionTriggerResponse(BaseModel):
    success: bool
    task_id: str
    message: str
    estimated_time: int

# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
class DatabaseManager:
    """ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œìš© ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ì"""
    
    def __init__(self):
        # PostgreSQL ì—°ê²° ë¬¸ìì—´ (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©)
        self.connection_string = "postgresql://postgres:@localhost:5432/paperworkdb"
        self.connection_pool = None
    
    async def initialize(self):
        """ì—°ê²° í’€ ì´ˆê¸°í™”"""
        try:
            self.connection_pool = await asyncpg.create_pool(
                self.connection_string,
                min_size=1,
                max_size=5,
                command_timeout=60
            )
            logger.info("âœ… ëŒ€ì‹œë³´ë“œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ ìƒì„± ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
    
    async def get_connection(self):
        """ì—°ê²° í’€ì—ì„œ ì—°ê²° ê°€ì ¸ì˜¤ê¸°"""
        if not self.connection_pool:
            await self.initialize()
        return self.connection_pool
    
    async def close(self):
        """ì—°ê²° í’€ ë‹«ê¸°"""
        if self.connection_pool:
            await self.connection_pool.close()

# ì „ì—­ ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € 
db_manager = DatabaseManager()

async def get_collection_data(
    portal_id: Optional[str] = None,
    limit: int = 20,
    offset: int = 0,
    date_from: Optional[str] = None,
    date_to: Optional[str] = None
) -> Dict[str, Any]:
    """ğŸ”¥ ì‹¤ì œ ìˆ˜ì§‘ ë°ì´í„° ì¡°íšŒ (ì‹¤ì‹œê°„ DB ì—°ë™)"""
    
    try:
        # ê¸°ë³¸ ë‚ ì§œ ì„¤ì •
        if not date_from:
            date_from = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
        if not date_to:
            date_to = datetime.now().strftime('%Y-%m-%d')
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        pool = await db_manager.get_connection()
        
        # WHERE ì¡°ê±´ êµ¬ì„±
        where_conditions = ["scraped_at BETWEEN $1 AND $2"]
        params = [f"{date_from} 00:00:00", f"{date_to} 23:59:59"]
        param_count = 2
        
        if portal_id:
            param_count += 1
            where_conditions.append(f"portal_id = ${param_count}")
            params.append(portal_id)
        
        where_clause = " AND ".join(where_conditions)
        
        # ë©”ì¸ ë°ì´í„° ì¡°íšŒ
        query = f"""
            SELECT 
                id,
                portal_id,
                title,
                agency,
                category,
                scraped_at,
                quality_score,
                processing_status as status,
                url,
                raw_data
            FROM raw_scraped_data 
            WHERE {where_clause}
            AND processing_status != 'failed'
            ORDER BY scraped_at DESC
            LIMIT ${param_count + 1} OFFSET ${param_count + 2}
        """
        
        params.extend([limit, offset])
        
        # ì´ ì¹´ìš´íŠ¸ ì¡°íšŒ
        count_query = f"""
            SELECT COUNT(*) as total_count
            FROM raw_scraped_data 
            WHERE {where_clause}
            AND processing_status != 'failed'
        """
        
        async with pool.acquire() as conn:
            # ë©”ì¸ ë°ì´í„° ì¡°íšŒ
            rows = await conn.fetch(query, *params)
            
            # ì´ ì¹´ìš´íŠ¸ ì¡°íšŒ
            count_row = await conn.fetchrow(count_query, *params[:-2])
            total_count = count_row['total_count'] if count_row else 0
            
            # ë°ì´í„° ë³€í™˜
            items = []
            for row in rows:
                item = {
                    'id': row['id'],
                    'portal_id': row['portal_id'],
                    'title': row['title'],
                    'agency': row['agency'] or 'ë¯¸ë¶„ë¥˜',
                    'category': row['category'] or 'ì¼ë°˜',
                    'scraped_at': row['scraped_at'].strftime('%Y-%m-%d %H:%M:%S'),
                    'quality_score': float(row['quality_score'] or 0),
                    'status': row['status'],
                    'url': row['url'] or ''
                }
                
                # ì¶”ê°€ ì •ë³´ê°€ raw_dataì— ìˆìœ¼ë©´ í¬í•¨
                if row['raw_data']:
                    try:
                        raw_data = json.loads(row['raw_data']) if isinstance(row['raw_data'], str) else row['raw_data']
                        item['support_content'] = raw_data.get('support_content', '')[:200] + '...' if raw_data.get('support_content') else ''
                        item['target_audience'] = raw_data.get('target_audience', '')
                    except:
                        pass
                
                items.append(item)
            
            return {
                'items': items,
                'total_count': total_count,
                'filtered_count': len(items),
                'has_more': (offset + limit) < total_count,
                'page_info': {
                    'current_offset': offset,
                    'current_limit': limit,
                    'next_offset': offset + limit if (offset + limit) < total_count else None
                }
            }
            
    except Exception as e:
        logger.error(f"âŒ ì‹¤ì œ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
        # DB ì˜¤ë¥˜ ì‹œ ë¹ˆ ë°ì´í„° ë°˜í™˜
        return {
            'items': [],
            'total_count': 0,
            'filtered_count': 0,
            'has_more': False,
            'error': str(e)
        }

def get_simulation_data(portal_id: Optional[str], limit: int, offset: int) -> Dict[str, Any]:
    """ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±"""
    
    programs = [
        'ì²­ë…„ì°½ì—…ì‚¬ê´€í•™êµ', 'K-Global 300 í”„ë¡œê·¸ë¨', 'ì¤‘ì†Œê¸°ì—… ê¸°ìˆ í˜ì‹  ì§€ì›ì‚¬ì—…',
        'ìŠ¤ë§ˆíŠ¸ ì œì¡°í˜ì‹  ì¶”ì§„ë‹¨', 'AI ê¸°ë°˜ ìŠ¤íƒ€íŠ¸ì—… ìœ¡ì„±', 'ê¸€ë¡œë²Œ ì§„ì¶œ ì§€ì› í”„ë¡œê·¸ë¨',
        'ë””ì§€í„¸ ë‰´ë”œ ì°½ì—…ì§€ì›', 'ê·¸ë¦°ë‰´ë”œ ê¸°ìˆ ê°œë°œ', 'ë°”ì´ì˜¤í—¬ìŠ¤ R&D ì§€ì›',
        'ì§€ì—­í˜ì‹  ì°½ì—…ìƒíƒœê³„', 'ì†Œì…œë²¤ì²˜ ì„±ì¥ì§€ì›', 'ì—¬ì„±ê¸°ì—… ì°½ì—…ì§€ì›',
        'ì‹œë‹ˆì–´ ì°½ì—… ì•„ì¹´ë°ë¯¸', 'ë†ì‹í’ˆ ìŠ¤íƒ€íŠ¸ì—… ìœ¡ì„±', 'ë¬¸í™”ì½˜í…ì¸  ì°½ì—…ì§€ì›'
    ]

    agencies = [
        'ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€', 'ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€', 'ì‚°ì—…í†µìƒìì›ë¶€',
        'ê¸°ì—…ì§„í¥ì›', 'ì°½ì—…ì§„í¥ì›', 'ì—°êµ¬ê°œë°œíŠ¹êµ¬ì§„í¥ì¬ë‹¨',
        'ì •ë³´í†µì‹ ì‚°ì—…ì§„í¥ì›', 'ë†ë¦¼ì¶•ì‚°ì‹í’ˆë¶€', 'ë¬¸í™”ì²´ìœ¡ê´€ê´‘ë¶€'
    ]

    import random
    
    # í•„í„°ë§ëœ í¬í„¸ë³„ ë°ì´í„° ìƒì„±
    filtered_portals = [portal_id] if portal_id else ['bizinfo', 'kstartup']
    
    items = []
    for i in range(limit):
        selected_portal = random.choice(filtered_portals)
        random_program = random.choice(programs)
        random_agency = random.choice(agencies)
        
        # ìµœê·¼ ì‹œê°„ ìƒì„±
        hours_ago = random.randint(1, 168)  # 1ì‹œê°„~7ì¼ ì „
        scraped_time = (datetime.now() - timedelta(hours=hours_ago)).strftime('%Y-%m-%d %H:%M:%S')
        
        items.append({
            'id': offset + i + 1,
            'portal_id': selected_portal,
            'title': f"{random_program} {random.choice([2024, 2025])}ë…„ {random.randint(1, 4)}ì°¨",
            'agency': random_agency,
            'category': random.choice(['ì°½ì—…ì§€ì›', 'ê¸°ìˆ ê°œë°œ', 'ë§ˆì¼€íŒ…ì§€ì›', 'R&D', 'AI/ë””ì§€í„¸', 'ê¸€ë¡œë²Œì§„ì¶œ']),
            'scraped_at': scraped_time,
            'quality_score': round(random.uniform(6.5, 9.8), 1),
            'status': random.choice(['completed', 'processing', 'completed', 'completed'])  # completed í™•ë¥  ë†’ì„
        })

    total_count = random.randint(1200, 4800)
    return {
        'items': items,
        'total_count': total_count,
        'filtered_count': len(items),
        'has_more': (offset + limit) < total_count
    }

async def get_stats_data() -> Dict[str, Any]:
    """ğŸ”¥ ì‹¤ì‹œê°„ í†µê³„ ë°ì´í„° ì¡°íšŒ (ì‹¤ì œ DB ì—°ë™)"""
    
    try:
        pool = await db_manager.get_connection()
        today = datetime.now().date()
        
        async with pool.acquire() as conn:
            # í¬í„¸ë³„ í†µê³„ ì¡°íšŒ
            portal_stats_query = """
                SELECT 
                    portal_id,
                    COUNT(*) as total_count,
                    COUNT(CASE WHEN DATE(scraped_at) = $1 THEN 1 END) as today_count,
                    AVG(quality_score) as avg_quality,
                    COUNT(CASE WHEN processing_status = 'duplicate' THEN 1 END) as duplicate_count,
                    MAX(scraped_at) as last_scraped
                FROM raw_scraped_data 
                WHERE processing_status IN ('completed', 'duplicate', 'processing')
                GROUP BY portal_id
            """
            
            portal_rows = await conn.fetch(portal_stats_query, today)
            
            # ì „ì²´ í†µê³„ ì¡°íšŒ  
            overall_stats_query = """
                SELECT 
                    COUNT(*) as total_items,
                    COUNT(CASE WHEN DATE(scraped_at) = $1 THEN 1 END) as today_items,
                    AVG(quality_score) as overall_quality,
                    COUNT(DISTINCT portal_id) as active_portals,
                    pg_size_pretty(pg_total_relation_size('raw_scraped_data')) as table_size
                FROM raw_scraped_data 
                WHERE processing_status IN ('completed', 'duplicate', 'processing')
            """
            
            overall_row = await conn.fetchrow(overall_stats_query, today)
            
            # ìµœê·¼ 7ì¼ê°„ ìˆ˜ì§‘ ì¶”ì„¸
            trend_query = """
                SELECT 
                    DATE(scraped_at) as collection_date,
                    COUNT(*) as daily_count,
                    COUNT(DISTINCT portal_id) as portals_active
                FROM raw_scraped_data 
                WHERE scraped_at >= $1 
                AND processing_status IN ('completed', 'duplicate')
                GROUP BY DATE(scraped_at)
                ORDER BY collection_date DESC
                LIMIT 7
            """
            
            trend_rows = await conn.fetch(trend_query, today - timedelta(days=6))
            
            # í’ˆì§ˆë³„ ë¶„í¬
            quality_query = """
                SELECT 
                    CASE 
                        WHEN quality_score >= 8.0 THEN 'excellent'
                        WHEN quality_score >= 6.0 THEN 'good'
                        WHEN quality_score >= 4.0 THEN 'fair'
                        ELSE 'poor'
                    END as quality_tier,
                    COUNT(*) as count
                FROM raw_scraped_data 
                WHERE processing_status = 'completed'
                GROUP BY quality_tier
            """
            
            quality_rows = await conn.fetch(quality_query)
        
        # í¬í„¸ë³„ ë°ì´í„° ì •ë¦¬
        portal_data = {}
        total_items = 0
        total_today = 0
        total_duplicates = 0
        
        for row in portal_rows:
            portal_id = row['portal_id']
            portal_data[portal_id] = {
                'total_count': int(row['total_count']),
                'today_count': int(row['today_count']),
                'avg_quality': round(float(row['avg_quality'] or 0), 1),
                'duplicate_count': int(row['duplicate_count']),
                'last_scraped': row['last_scraped'].strftime('%Y-%m-%d %H:%M:%S') if row['last_scraped'] else None
            }
            
            total_items += int(row['total_count'])
            total_today += int(row['today_count'])
            total_duplicates += int(row['duplicate_count'])
        
        # ì„±ê³µë¥  ê³„ì‚°
        success_rate = round(((total_items - total_duplicates) / total_items * 100), 1) if total_items > 0 else 0
        
        # 7ì¼ ì¶”ì„¸ ë°ì´í„°
        trend_data = []
        for row in trend_rows:
            trend_data.append({
                'date': row['collection_date'].strftime('%Y-%m-%d'),
                'count': int(row['daily_count']),
                'active_portals': int(row['portals_active'])
            })
        
        # í’ˆì§ˆ ë¶„í¬
        quality_distribution = {}
        for row in quality_rows:
            quality_distribution[row['quality_tier']] = int(row['count'])
        
        return {
            'summary': {
                'total_items': total_items,
                'today_collected': total_today,
                'success_rate': success_rate,
                'active_portals': len(portal_data),
                'duplicate_filtered_today': total_duplicates,
                'overall_quality': round(float(overall_row['overall_quality'] or 0), 1) if overall_row else 0,
                'storage_size': overall_row['table_size'] if overall_row else 'N/A',
                'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            },
            'portals': portal_data,
            'trends': {
                'daily_collection': trend_data,
                'quality_distribution': quality_distribution
            },
            'performance': {
                'avg_quality_threshold': 6.0,
                'collection_efficiency': round((total_today / max(total_items / 30, 1)), 2),  # ì¼ì¼ í‰ê·  ëŒ€ë¹„
                'quality_pass_rate': round((quality_distribution.get('excellent', 0) + quality_distribution.get('good', 0)) / max(sum(quality_distribution.values()), 1) * 100, 1)
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ ì‹¤ì‹œê°„ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        # DB ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜
        return {
            'summary': {
                'total_items': 0,
                'today_collected': 0,
                'success_rate': 0,
                'active_portals': 0,
                'duplicate_filtered_today': 0,
                'overall_quality': 0,
                'storage_size': 'Error',
                'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            },
            'portals': {},
            'trends': {'daily_collection': [], 'quality_distribution': {}},
            'performance': {'avg_quality_threshold': 6.0, 'collection_efficiency': 0, 'quality_pass_rate': 0},
            'error': str(e)
        }

# API ì—”ë“œí¬ì¸íŠ¸ë“¤

@router.get("/collection-list", response_model=CollectionListResponse)
async def get_collection_list(
    portal_id: Optional[str] = Query(None, description="í¬í„¸ ID (bizinfo, kstartup)"),
    limit: int = Query(20, ge=1, le=100, description="í˜ì´ì§€ í¬ê¸°"),
    offset: int = Query(0, ge=0, description="ì˜¤í”„ì…‹"),
    date_from: Optional[str] = Query(None, description="ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)"),
    date_to: Optional[str] = Query(None, description="ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)")
):
    """ì‹¤ì‹œê°„ ìˆ˜ì§‘ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ"""
    try:
        data = await get_collection_data(
            portal_id=portal_id,
            limit=limit,
            offset=offset,
            date_from=date_from,
            date_to=date_to
        )
        
        return CollectionListResponse(
            success=True,
            data=data,
            timestamp=datetime.now().isoformat()
        )
        
    except Exception as e:
        logger.error(f"Collection list error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/stats", response_model=StatsResponse)
async def get_collection_stats():
    """ìˆ˜ì§‘ í†µê³„ ì¡°íšŒ"""
    try:
        data = await get_stats_data()
        
        return StatsResponse(
            success=True,
            data=data,
            timestamp=datetime.now().isoformat()
        )
        
    except Exception as e:
        logger.error(f"Stats error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "service": "Scraping Dashboard API",
        "timestamp": datetime.now().isoformat()
    }

# ğŸ”¥ ìƒˆë¡œìš´ ì‹¤ì‹œê°„ ê¸°ëŠ¥ë“¤

@router.post("/trigger-collection", response_model=CollectionTriggerResponse)
async def trigger_manual_collection(
    request: CollectionTriggerRequest,
    background_tasks: BackgroundTasks
):
    """ğŸš€ ìˆ˜ë™ ìˆ˜ì§‘ íŠ¸ë¦¬ê±° (ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰)"""
    try:
        # ì‘ì—… ID ìƒì„±
        import uuid
        task_id = str(uuid.uuid4())[:8]
        
        # ì˜ˆìƒ ì‹œê°„ ê³„ì‚° (í˜ì´ì§€ë‹¹ ì•½ 3ì´ˆ)
        estimated_time = len(request.portal_ids) * request.max_pages * 3
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìˆ˜ì§‘ ì‹¤í–‰
        background_tasks.add_task(
            run_background_collection,
            task_id,
            request.portal_ids,
            request.max_pages,
            request.force_update
        )
        
        logger.info(f"ğŸš€ ìˆ˜ì§‘ ì‘ì—… ì‹œì‘: {task_id} | í¬í„¸: {request.portal_ids} | í˜ì´ì§€: {request.max_pages}")
        
        return CollectionTriggerResponse(
            success=True,
            task_id=task_id,
            message=f"{len(request.portal_ids)}ê°œ í¬í„¸ì—ì„œ ìˆ˜ì§‘ ì‹œì‘",
            estimated_time=estimated_time
        )
        
    except Exception as e:
        logger.error(f"âŒ ìˆ˜ì§‘ íŠ¸ë¦¬ê±° ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ìˆ˜ì§‘ íŠ¸ë¦¬ê±° ì‹¤íŒ¨: {str(e)}")

@router.get("/collection-status/{task_id}")
async def get_collection_status(task_id: str):
    """ìˆ˜ì§‘ ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
    # ì‹¤ì œë¡œëŠ” Redisë‚˜ ë³„ë„ ì €ì¥ì†Œì—ì„œ ìƒíƒœë¥¼ í™•ì¸í•´ì•¼ í•¨
    # ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ êµ¬í˜„ë§Œ
    return {
        "task_id": task_id,
        "status": "running",
        "progress": 45,
        "message": "ê¸°ì—…ë§ˆë‹¹ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...",
        "timestamp": datetime.now().isoformat()
    }

@router.get("/real-time-stats") 
async def get_real_time_stats():
    """âš¡ ì‹¤ì‹œê°„ í†µê³„ (1ë¶„ë§ˆë‹¤ ê°±ì‹ ë˜ëŠ” í•µì‹¬ ì§€í‘œ)"""
    try:
        pool = await db_manager.get_connection()
        
        async with pool.acquire() as conn:
            # ìµœê·¼ 1ì‹œê°„ ìˆ˜ì§‘ í˜„í™©
            recent_query = """
                SELECT 
                    COUNT(*) as recent_count,
                    COUNT(DISTINCT portal_id) as active_portals,
                    AVG(quality_score) as avg_quality,
                    MAX(scraped_at) as last_activity
                FROM raw_scraped_data
                WHERE scraped_at >= NOW() - INTERVAL '1 hour'
                AND processing_status = 'completed'
            """
            
            recent_row = await conn.fetchrow(recent_query)
            
            # ì‹¤ì‹œê°„ ì²˜ë¦¬ ìƒíƒœ
            processing_query = """
                SELECT 
                    processing_status,
                    COUNT(*) as count
                FROM raw_scraped_data 
                WHERE scraped_at >= NOW() - INTERVAL '10 minutes'
                GROUP BY processing_status
            """
            
            processing_rows = await conn.fetch(processing_query)
            
        processing_status = {}
        for row in processing_rows:
            processing_status[row['processing_status']] = int(row['count'])
        
        return {
            "real_time": {
                "recent_hour_collected": int(recent_row['recent_count']) if recent_row else 0,
                "active_portals": int(recent_row['active_portals']) if recent_row else 0,
                "current_avg_quality": round(float(recent_row['avg_quality'] or 0), 1) if recent_row else 0,
                "last_activity": recent_row['last_activity'].strftime('%Y-%m-%d %H:%M:%S') if recent_row and recent_row['last_activity'] else None,
                "processing_status": processing_status,
                "system_health": "healthy" if processing_status.get('completed', 0) > 0 else "idle"
            },
            "timestamp": datetime.now().isoformat(),
            "refresh_interval": 60  # 60ì´ˆë§ˆë‹¤ ê°±ì‹  ê¶Œì¥
        }
        
    except Exception as e:
        logger.error(f"âŒ ì‹¤ì‹œê°„ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return {
            "real_time": {
                "recent_hour_collected": 0,
                "active_portals": 0,
                "current_avg_quality": 0,
                "last_activity": None,
                "processing_status": {},
                "system_health": "error"
            },
            "timestamp": datetime.now().isoformat(),
            "error": str(e)
        }

@router.get("/portal-comparison")
async def get_portal_comparison(days: int = Query(7, ge=1, le=30)):
    """ğŸ“Š í¬í„¸ê°„ ìˆ˜ì§‘ ì„±ê³¼ ë¹„êµ ë¶„ì„"""
    try:
        pool = await db_manager.get_connection()
        since_date = datetime.now() - timedelta(days=days)
        
        async with pool.acquire() as conn:
            comparison_query = """
                SELECT 
                    portal_id,
                    COUNT(*) as total_collected,
                    COUNT(CASE WHEN processing_status = 'completed' THEN 1 END) as successful,
                    COUNT(CASE WHEN processing_status = 'duplicate' THEN 1 END) as duplicates,
                    AVG(quality_score) as avg_quality,
                    MIN(scraped_at) as first_collection,
                    MAX(scraped_at) as latest_collection,
                    COUNT(DISTINCT DATE(scraped_at)) as active_days
                FROM raw_scraped_data 
                WHERE scraped_at >= $1
                GROUP BY portal_id
                ORDER BY total_collected DESC
            """
            
            rows = await conn.fetch(comparison_query, since_date)
        
        comparisons = []
        for row in rows:
            success_rate = round((int(row['successful']) / max(int(row['total_collected']), 1)) * 100, 1)
            daily_avg = round(int(row['total_collected']) / max(int(row['active_days']), 1), 1)
            
            comparisons.append({
                'portal_id': row['portal_id'],
                'total_collected': int(row['total_collected']),
                'successful': int(row['successful']),
                'duplicates': int(row['duplicates']),
                'success_rate': success_rate,
                'avg_quality': round(float(row['avg_quality'] or 0), 1),
                'daily_average': daily_avg,
                'active_days': int(row['active_days']),
                'consistency': round((int(row['active_days']) / days) * 100, 1),
                'latest_collection': row['latest_collection'].strftime('%Y-%m-%d %H:%M:%S') if row['latest_collection'] else None
            })
        
        return {
            'comparison_period': f"{days} days",
            'portals': comparisons,
            'summary': {
                'best_performer': comparisons[0]['portal_id'] if comparisons else None,
                'total_portals': len(comparisons),
                'average_success_rate': round(sum(p['success_rate'] for p in comparisons) / max(len(comparisons), 1), 1),
                'average_quality': round(sum(p['avg_quality'] for p in comparisons) / max(len(comparisons), 1), 1)
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ í¬í„¸ ë¹„êµ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"í¬í„¸ ë¹„êµ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

async def run_background_collection(
    task_id: str,
    portal_ids: List[str], 
    max_pages: int,
    force_update: bool
):
    """ë°±ê·¸ë¼ìš´ë“œ ìˆ˜ì§‘ ì‹¤í–‰"""
    try:
        logger.info(f"ğŸ”„ ë°±ê·¸ë¼ìš´ë“œ ìˆ˜ì§‘ ì‹œì‘: {task_id}")
        
        # ì‹¤ì œ ìˆ˜ì§‘ê¸° import ë° ì‹¤í–‰
        from bizinfo_collector import run_comprehensive_collection
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë¬¸ìì—´ 
        db_conn = "postgresql://postgres:@localhost:5432/paperworkdb"
        
        # ìˆ˜ì§‘ ì‹¤í–‰
        results = await run_comprehensive_collection(
            db_connection_string=db_conn,
            portals=portal_ids,
            max_pages=max_pages
        )
        
        # ê²°ê³¼ ë¡œê¹…
        total_new = sum(r.new_items for r in results)
        total_time = sum(r.processing_time for r in results)
        
        logger.info(f"âœ… ë°±ê·¸ë¼ìš´ë“œ ìˆ˜ì§‘ ì™„ë£Œ: {task_id} | ì‹ ê·œ {total_new}ê°œ | {total_time:.1f}ì´ˆ")
        
        # ì‹¤ì œë¡œëŠ” ê²°ê³¼ë¥¼ Redisë‚˜ ë³„ë„ ì €ì¥ì†Œì— ì €ì¥í•´ì•¼ í•¨
        
    except Exception as e:
        logger.error(f"âŒ ë°±ê·¸ë¼ìš´ë“œ ìˆ˜ì§‘ ì‹¤íŒ¨: {task_id} - {str(e)}")

# ì´ì „ PHP í˜¸í™˜ì„±ì„ ìœ„í•œ ì—”ë“œí¬ì¸íŠ¸ (ê¸°ì¡´ í”„ë¡ íŠ¸ì—”ë“œì™€ í˜¸í™˜)
@router.get("")
async def legacy_endpoint(action: str = Query(..., description="ì•¡ì…˜ íƒ€ì…")):
    """ê¸°ì¡´ PHP APIì™€ í˜¸í™˜ë˜ëŠ” ì—”ë“œí¬ì¸íŠ¸"""
    if action == "collection_list":
        return await get_collection_list()
    elif action == "stats":
        return await get_collection_stats()
    else:
        raise HTTPException(status_code=400, detail=f"Invalid action: {action}")