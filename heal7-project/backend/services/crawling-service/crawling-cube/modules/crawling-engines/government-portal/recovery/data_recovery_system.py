"""
ë°ì´í„° ë³µêµ¬ ë° ë™ê¸°í™” ì‹œìŠ¤í…œ
ìë™ ê°ì§€, ë³µêµ¬, ë™ê¸°í™”ë¥¼ í†µí•œ ì™„ì „í•œ ë°ì´í„° ë³´í˜¸

Author: Paperwork AI Team
Version: 2.0.0
Date: 2025-08-23
"""

import asyncio
import asyncpg
import logging
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from enum import Enum
import json
from uuid import UUID, uuid4

from fallback.multi_tier_backup_system import MultiTierBackupSystem, BackupRecord, BackupTier
from database.migration_engine import MigrationEngine, MigrationStats
from utils.error_handler import ErrorHandler, ErrorContext, ErrorType

logger = logging.getLogger(__name__)

class RecoveryTrigger(Enum):
    """ë³µêµ¬ íŠ¸ë¦¬ê±° ìœ í˜•"""
    AUTOMATIC = "automatic"         # ìë™ ê°ì§€
    MANUAL = "manual"              # ìˆ˜ë™ ìš”ì²­
    SCHEDULED = "scheduled"        # ìŠ¤ì¼€ì¤„ëœ ê²€ì¦
    INTEGRITY_CHECK = "integrity"  # ë¬´ê²°ì„± ê²€ì‚¬

class RecoveryScope(Enum):
    """ë³µêµ¬ ë²”ìœ„"""
    SINGLE_RECORD = "single_record"
    SESSION = "session"
    DATE_RANGE = "date_range"
    FULL_PORTAL = "full_portal"
    FULL_SYSTEM = "full_system"

class RecoveryStatus(Enum):
    """ë³µêµ¬ ìƒíƒœ"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    PARTIAL = "partial"
    CANCELLED = "cancelled"

@dataclass
class LossDetectionConfig:
    """ì†ì‹¤ ê°ì§€ ì„¤ì •"""
    # ìŠ¤í¬ë˜í•‘ ì„¸ì…˜ ê²€ì¦
    max_session_duration_hours: int = 6
    min_items_per_session: int = 5
    expected_success_rate: float = 0.8
    
    # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
    min_data_completeness: float = 0.7
    max_processing_delay_hours: int = 24
    
    # ì£¼ê¸°ì  ê²€ì¦
    daily_verification_enabled: bool = True
    weekly_full_check_enabled: bool = True
    
    # ì•Œë¦¼ ì„¤ì •
    immediate_notification_enabled: bool = True
    escalation_delay_minutes: int = 30

@dataclass
class RecoveryOperation:
    """ë³µêµ¬ ì‘ì—…"""
    operation_id: UUID = field(default_factory=uuid4)
    trigger_type: RecoveryTrigger = RecoveryTrigger.AUTOMATIC
    scope: RecoveryScope = RecoveryScope.SINGLE_RECORD
    
    # ëŒ€ìƒ ì •ë³´
    target_portal_id: Optional[str] = None
    target_session_id: Optional[UUID] = None
    target_date_range: Optional[Tuple[datetime, datetime]] = None
    affected_records: List[str] = field(default_factory=list)
    
    # ë³µêµ¬ ì§„í–‰ ìƒí™©
    started_at: datetime = field(default_factory=datetime.now)
    completed_at: Optional[datetime] = None
    status: RecoveryStatus = RecoveryStatus.PENDING
    
    # ê²°ê³¼
    records_recovered: int = 0
    records_failed: int = 0
    recovery_details: Dict = field(default_factory=dict)
    
    # ê²€ì¦
    verification_passed: bool = False
    verification_details: Dict = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'operation_id': str(self.operation_id),
            'trigger_type': self.trigger_type.value,
            'scope': self.scope.value,
            'target_portal_id': self.target_portal_id,
            'target_session_id': str(self.target_session_id) if self.target_session_id else None,
            'target_date_range': [dt.isoformat() for dt in self.target_date_range] if self.target_date_range else None,
            'affected_records': self.affected_records,
            'started_at': self.started_at.isoformat(),
            'completed_at': self.completed_at.isoformat() if self.completed_at else None,
            'status': self.status.value,
            'records_recovered': self.records_recovered,
            'records_failed': self.records_failed,
            'recovery_details': self.recovery_details,
            'verification_passed': self.verification_passed,
            'verification_details': self.verification_details
        }

class DataLossDetector:
    """ë°ì´í„° ì†ì‹¤ ê°ì§€ê¸°"""
    
    def __init__(self, db_pool: asyncpg.Pool, config: LossDetectionConfig = None):
        self.db_pool = db_pool
        self.config = config or LossDetectionConfig()
    
    async def detect_session_failures(self, hours_back: int = 24) -> List[Dict[str, Any]]:
        """ì„¸ì…˜ ì‹¤íŒ¨ ê°ì§€"""
        logger.info(f"ğŸ” ì„¸ì…˜ ì‹¤íŒ¨ ê°ì§€ ì‹œì‘: ìµœê·¼ {hours_back}ì‹œê°„")
        
        failed_sessions = []
        cutoff_time = datetime.now() - timedelta(hours=hours_back)
        
        query = """
            SELECT id, portal_id, started_at, completed_at, status,
                   items_found, items_processed, errors_encountered,
                   total_duration_seconds
            FROM scraping_sessions
            WHERE started_at >= $1
              AND (
                  status = 'failed' 
                  OR (status = 'running' AND started_at < $2)
                  OR (status = 'completed' AND items_found < $3)
                  OR (errors_encountered > items_processed * 0.5)
              )
            ORDER BY started_at DESC
        """
        
        max_duration = self.config.max_session_duration_hours * 3600  # ì´ˆë¡œ ë³€í™˜
        timeout_cutoff = datetime.now() - timedelta(hours=self.config.max_session_duration_hours)
        
        async with self.db_pool.acquire() as conn:
            rows = await conn.fetch(
                query, 
                cutoff_time,
                timeout_cutoff,
                self.config.min_items_per_session
            )
            
            for row in rows:
                failure_reason = self._analyze_session_failure(dict(row))
                
                failed_sessions.append({
                    'session_id': row['id'],
                    'portal_id': row['portal_id'],
                    'started_at': row['started_at'],
                    'status': row['status'],
                    'failure_reason': failure_reason,
                    'severity': self._calculate_failure_severity(dict(row)),
                    'recovery_recommended': True
                })
        
        logger.info(f"ğŸš¨ ê°ì§€ëœ ì‹¤íŒ¨ ì„¸ì…˜: {len(failed_sessions)}ê°œ")
        return failed_sessions
    
    def _analyze_session_failure(self, session: Dict) -> str:
        """ì„¸ì…˜ ì‹¤íŒ¨ ì›ì¸ ë¶„ì„"""
        if session['status'] == 'failed':
            return "Session marked as failed"
        
        if session['status'] == 'running':
            return "Session timeout - still running after maximum duration"
        
        if session['items_found'] < self.config.min_items_per_session:
            return f"Low item count: {session['items_found']} < {self.config.min_items_per_session}"
        
        error_rate = session['errors_encountered'] / max(session['items_processed'], 1)
        if error_rate > 0.5:
            return f"High error rate: {error_rate:.2%}"
        
        return "Unknown failure pattern"
    
    def _calculate_failure_severity(self, session: Dict) -> str:
        """ì‹¤íŒ¨ ì‹¬ê°ë„ ê³„ì‚°"""
        if session['status'] == 'failed':
            return "high"
        
        if session['items_found'] == 0:
            return "high"
        
        error_rate = session['errors_encountered'] / max(session['items_processed'], 1)
        if error_rate > 0.8:
            return "high"
        elif error_rate > 0.3:
            return "medium"
        else:
            return "low"
    
    async def detect_data_gaps(self, portal_id: str = None, days_back: int = 7) -> List[Dict[str, Any]]:
        """ë°ì´í„° ê³µë°± ê°ì§€"""
        logger.info(f"ğŸ” ë°ì´í„° ê³µë°± ê°ì§€: {portal_id or 'ALL'}, ìµœê·¼ {days_back}ì¼")
        
        gaps_detected = []
        start_date = datetime.now() - timedelta(days=days_back)
        
        # ì¼ë³„ ë°ì´í„° ìˆ˜ì§‘ í˜„í™© í™•ì¸
        query = """
            SELECT 
                DATE(scraped_at) as scrape_date,
                portal_id,
                COUNT(*) as records_count,
                COUNT(DISTINCT scraping_session_id) as sessions_count
            FROM raw_scraped_data
            WHERE scraped_at >= $1
        """
        params = [start_date]
        
        if portal_id:
            query += " AND portal_id = $2"
            params.append(portal_id)
        
        query += """
            GROUP BY DATE(scraped_at), portal_id
            ORDER BY scrape_date DESC, portal_id
        """
        
        async with self.db_pool.acquire() as conn:
            rows = await conn.fetch(query, *params)
            
            # ë‚ ì§œë³„ ë°ì´í„° ë¶„ì„
            data_by_date = {}
            for row in rows:
                date_key = row['scrape_date']
                if date_key not in data_by_date:
                    data_by_date[date_key] = {}
                
                data_by_date[date_key][row['portal_id']] = {
                    'records_count': row['records_count'],
                    'sessions_count': row['sessions_count']
                }
            
            # ê³µë°± ê°ì§€
            expected_portals = ['bizinfo', 'kstartup'] if not portal_id else [portal_id]
            
            for i in range(days_back):
                check_date = (datetime.now() - timedelta(days=i)).date()
                
                if check_date not in data_by_date:
                    # í•´ë‹¹ ë‚ ì§œì— ì „ì²´ ë°ì´í„°ê°€ ì—†ìŒ
                    gaps_detected.append({
                        'gap_type': 'missing_date',
                        'date': check_date.isoformat(),
                        'affected_portals': expected_portals,
                        'severity': 'high'
                    })
                else:
                    # í¬í„¸ë³„ ë°ì´í„° í™•ì¸
                    for expected_portal in expected_portals:
                        if expected_portal not in data_by_date[check_date]:
                            gaps_detected.append({
                                'gap_type': 'missing_portal_data',
                                'date': check_date.isoformat(),
                                'affected_portals': [expected_portal],
                                'severity': 'medium'
                            })
                        else:
                            records_count = data_by_date[check_date][expected_portal]['records_count']
                            if records_count < self.config.min_items_per_session:
                                gaps_detected.append({
                                    'gap_type': 'low_data_count',
                                    'date': check_date.isoformat(),
                                    'affected_portals': [expected_portal],
                                    'records_count': records_count,
                                    'severity': 'low'
                                })
        
        logger.info(f"ğŸ“Š ê°ì§€ëœ ë°ì´í„° ê³µë°±: {len(gaps_detected)}ê°œ")
        return gaps_detected
    
    async def detect_processing_delays(self, hours_threshold: int = None) -> List[Dict[str, Any]]:
        """ì²˜ë¦¬ ì§€ì—° ê°ì§€"""
        threshold_hours = hours_threshold or self.config.max_processing_delay_hours
        logger.info(f"â° ì²˜ë¦¬ ì§€ì—° ê°ì§€: {threshold_hours}ì‹œê°„ ì„ê³„ì¹˜")
        
        delayed_records = []
        cutoff_time = datetime.now() - timedelta(hours=threshold_hours)
        
        query = """
            SELECT id, portal_id, scraped_at, processing_status,
                   quality_score, processed_at, migrated_at
            FROM raw_scraped_data
            WHERE scraped_at <= $1
              AND (
                  processing_status = 'pending'
                  OR (processing_status = 'completed' AND migrated_at IS NULL)
              )
            ORDER BY scraped_at ASC
            LIMIT 1000
        """
        
        async with self.db_pool.acquire() as conn:
            rows = await conn.fetch(query, cutoff_time)
            
            for row in rows:
                delay_hours = (datetime.now() - row['scraped_at']).total_seconds() / 3600
                
                delayed_records.append({
                    'record_id': row['id'],
                    'portal_id': row['portal_id'],
                    'scraped_at': row['scraped_at'].isoformat(),
                    'processing_status': row['processing_status'],
                    'delay_hours': round(delay_hours, 1),
                    'severity': 'high' if delay_hours > threshold_hours * 2 else 'medium'
                })
        
        logger.info(f"â° ê°ì§€ëœ ì²˜ë¦¬ ì§€ì—°: {len(delayed_records)}ê°œ")
        return delayed_records

class DataRecoveryEngine:
    """ë°ì´í„° ë³µêµ¬ ì—”ì§„"""
    
    def __init__(self, db_pool: asyncpg.Pool, backup_system: MultiTierBackupSystem, 
                 migration_engine: MigrationEngine, error_handler: ErrorHandler):
        self.db_pool = db_pool
        self.backup_system = backup_system
        self.migration_engine = migration_engine
        self.error_handler = error_handler
        self.detector = DataLossDetector(db_pool)
        
        # ë³µêµ¬ í†µê³„
        self.recovery_stats = {
            'operations_started': 0,
            'operations_completed': 0,
            'operations_failed': 0,
            'records_recovered': 0,
            'automatic_recoveries': 0,
            'manual_recoveries': 0
        }
    
    async def run_automatic_recovery_cycle(self) -> Dict[str, Any]:
        """ìë™ ë³µêµ¬ ì‚¬ì´í´ ì‹¤í–‰"""
        logger.info("ğŸ”„ ìë™ ë³µêµ¬ ì‚¬ì´í´ ì‹œì‘")
        
        recovery_results = {
            'cycle_started_at': datetime.now().isoformat(),
            'operations_performed': [],
            'total_recovered': 0,
            'issues_remaining': 0
        }
        
        try:
            # 1. ì‹¤íŒ¨í•œ ì„¸ì…˜ ë³µêµ¬
            failed_sessions = await self.detector.detect_session_failures()
            for failed_session in failed_sessions:
                if failed_session['severity'] in ['high', 'medium']:
                    operation = await self.recover_session(
                        UUID(failed_session['session_id']),
                        trigger=RecoveryTrigger.AUTOMATIC
                    )
                    recovery_results['operations_performed'].append(operation.to_dict())
                    recovery_results['total_recovered'] += operation.records_recovered
            
            # 2. ë°ì´í„° ê³µë°± ë³µêµ¬
            data_gaps = await self.detector.detect_data_gaps()
            for gap in data_gaps:
                if gap['severity'] == 'high':
                    # ë‚ ì§œë³„ ë³µêµ¬ ì‹œë„
                    gap_date = datetime.fromisoformat(gap['date']).date()
                    for portal_id in gap['affected_portals']:
                        operation = await self.recover_date_range(
                            portal_id,
                            gap_date,
                            gap_date,
                            trigger=RecoveryTrigger.AUTOMATIC
                        )
                        recovery_results['operations_performed'].append(operation.to_dict())
                        recovery_results['total_recovered'] += operation.records_recovered
            
            # 3. ì²˜ë¦¬ ì§€ì—° ë³µêµ¬
            delayed_records = await self.detector.detect_processing_delays()
            if len(delayed_records) > 10:  # ëŒ€ëŸ‰ ì§€ì—°ì‹œì—ë§Œ ìë™ ë³µêµ¬
                high_priority = [r for r in delayed_records if r['severity'] == 'high']
                if high_priority:
                    operation = await self.recover_delayed_processing(
                        [r['record_id'] for r in high_priority[:50]],  # ìµœëŒ€ 50ê°œ
                        trigger=RecoveryTrigger.AUTOMATIC
                    )
                    recovery_results['operations_performed'].append(operation.to_dict())
                    recovery_results['total_recovered'] += operation.records_recovered
            
            recovery_results['cycle_completed_at'] = datetime.now().isoformat()
            self.recovery_stats['automatic_recoveries'] += 1
            
            logger.info(f"âœ… ìë™ ë³µêµ¬ ì‚¬ì´í´ ì™„ë£Œ: {recovery_results['total_recovered']}ê°œ ë ˆì½”ë“œ ë³µêµ¬")
            
        except Exception as e:
            logger.error(f"âŒ ìë™ ë³µêµ¬ ì‚¬ì´í´ ì‹¤íŒ¨: {str(e)}")
            recovery_results['error'] = str(e)
        
        return recovery_results
    
    async def recover_session(self, session_id: UUID, trigger: RecoveryTrigger = RecoveryTrigger.MANUAL) -> RecoveryOperation:
        """íŠ¹ì • ì„¸ì…˜ ë³µêµ¬"""
        logger.info(f"ğŸ”„ ì„¸ì…˜ ë³µêµ¬ ì‹œì‘: {session_id}")
        
        operation = RecoveryOperation(
            trigger_type=trigger,
            scope=RecoveryScope.SESSION,
            target_session_id=session_id,
            status=RecoveryStatus.RUNNING
        )
        
        try:
            await self._log_recovery_start(operation)
            self.recovery_stats['operations_started'] += 1
            
            # 1. ì„¸ì…˜ ì •ë³´ ì¡°íšŒ
            session_info = await self._get_session_info(session_id)
            if not session_info:
                operation.status = RecoveryStatus.FAILED
                operation.recovery_details['error'] = 'Session not found'
                return operation
            
            operation.target_portal_id = session_info['portal_id']
            
            # 2. ì„¸ì…˜ê³¼ ê´€ë ¨ëœ ë°±ì—… ê²€ìƒ‰
            backup_records = await self._find_session_backups(session_id)
            logger.info(f"ğŸ“¦ ì„¸ì…˜ ë°±ì—… ë°œê²¬: {len(backup_records)}ê°œ")
            
            # 3. ë°±ì—…ì—ì„œ ë°ì´í„° ë³µêµ¬
            for backup_record in backup_records:
                try:
                    restored_record = await self.backup_system.restore_from_any_tier(backup_record.backup_id)
                    if restored_record:
                        # ë³µêµ¬ëœ ë°ì´í„°ë¥¼ primary DBì— ì €ì¥
                        await self._restore_to_primary_db(restored_record)
                        operation.records_recovered += 1
                        
                        # ë§ˆì´ê·¸ë ˆì´ì…˜ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
                        await self._trigger_migration_for_recovered_data(restored_record.source_id)
                        
                except Exception as e:
                    logger.error(f"âŒ ë°±ì—… ë³µêµ¬ ì‹¤íŒ¨ {backup_record.backup_id}: {str(e)}")
                    operation.records_failed += 1
            
            # 4. ë³µêµ¬ í›„ ê²€ì¦
            verification_result = await self._verify_session_recovery(session_id)
            operation.verification_passed = verification_result['passed']
            operation.verification_details = verification_result
            
            # 5. ìƒíƒœ ì—…ë°ì´íŠ¸
            if operation.records_recovered > 0:
                operation.status = RecoveryStatus.COMPLETED if operation.verification_passed else RecoveryStatus.PARTIAL
                self.recovery_stats['records_recovered'] += operation.records_recovered
                self.recovery_stats['operations_completed'] += 1
            else:
                operation.status = RecoveryStatus.FAILED
                self.recovery_stats['operations_failed'] += 1
            
            operation.completed_at = datetime.now()
            await self._log_recovery_completion(operation)
            
            logger.info(f"âœ… ì„¸ì…˜ ë³µêµ¬ ì™„ë£Œ: {operation.records_recovered}ê°œ ë³µêµ¬, {operation.records_failed}ê°œ ì‹¤íŒ¨")
            
        except Exception as e:
            operation.status = RecoveryStatus.FAILED
            operation.recovery_details['error'] = str(e)
            operation.completed_at = datetime.now()
            
            await self._log_recovery_error(operation, str(e))
            self.recovery_stats['operations_failed'] += 1
            
            logger.error(f"âŒ ì„¸ì…˜ ë³µêµ¬ ì‹¤íŒ¨ {session_id}: {str(e)}")
        
        return operation
    
    async def recover_date_range(self, portal_id: str, start_date: datetime.date, 
                               end_date: datetime.date, trigger: RecoveryTrigger = RecoveryTrigger.MANUAL) -> RecoveryOperation:
        """ë‚ ì§œ ë²”ìœ„ ë³µêµ¬"""
        logger.info(f"ğŸ“… ë‚ ì§œ ë²”ìœ„ ë³µêµ¬: {portal_id}, {start_date} ~ {end_date}")
        
        operation = RecoveryOperation(
            trigger_type=trigger,
            scope=RecoveryScope.DATE_RANGE,
            target_portal_id=portal_id,
            target_date_range=(datetime.combine(start_date, datetime.min.time()), 
                             datetime.combine(end_date, datetime.max.time())),
            status=RecoveryStatus.RUNNING
        )
        
        try:
            await self._log_recovery_start(operation)
            self.recovery_stats['operations_started'] += 1
            
            # ë‚ ì§œ ë²”ìœ„ì˜ ë°±ì—… ê²€ìƒ‰
            date_backups = await self._find_date_range_backups(portal_id, start_date, end_date)
            logger.info(f"ğŸ“¦ ë‚ ì§œ ë²”ìœ„ ë°±ì—… ë°œê²¬: {len(date_backups)}ê°œ")
            
            for backup_record in date_backups:
                try:
                    restored_record = await self.backup_system.restore_from_any_tier(backup_record.backup_id)
                    if restored_record:
                        await self._restore_to_primary_db(restored_record)
                        operation.records_recovered += 1
                        await self._trigger_migration_for_recovered_data(restored_record.source_id)
                        
                except Exception as e:
                    logger.error(f"âŒ ë‚ ì§œ ë°±ì—… ë³µêµ¬ ì‹¤íŒ¨: {str(e)}")
                    operation.records_failed += 1
            
            # ë³µêµ¬ í›„ ê²€ì¦
            verification_result = await self._verify_date_range_recovery(portal_id, start_date, end_date)
            operation.verification_passed = verification_result['passed']
            operation.verification_details = verification_result
            
            operation.status = RecoveryStatus.COMPLETED if operation.verification_passed else RecoveryStatus.PARTIAL
            operation.completed_at = datetime.now()
            
            await self._log_recovery_completion(operation)
            self.recovery_stats['records_recovered'] += operation.records_recovered
            self.recovery_stats['operations_completed'] += 1
            
            logger.info(f"âœ… ë‚ ì§œ ë²”ìœ„ ë³µêµ¬ ì™„ë£Œ: {operation.records_recovered}ê°œ ë³µêµ¬")
            
        except Exception as e:
            operation.status = RecoveryStatus.FAILED
            operation.recovery_details['error'] = str(e)
            operation.completed_at = datetime.now()
            
            await self._log_recovery_error(operation, str(e))
            self.recovery_stats['operations_failed'] += 1
            
            logger.error(f"âŒ ë‚ ì§œ ë²”ìœ„ ë³µêµ¬ ì‹¤íŒ¨: {str(e)}")
        
        return operation
    
    async def recover_delayed_processing(self, record_ids: List[int], 
                                       trigger: RecoveryTrigger = RecoveryTrigger.MANUAL) -> RecoveryOperation:
        """ì§€ì—°ëœ ì²˜ë¦¬ ë³µêµ¬"""
        logger.info(f"â° ì§€ì—° ì²˜ë¦¬ ë³µêµ¬: {len(record_ids)}ê°œ ë ˆì½”ë“œ")
        
        operation = RecoveryOperation(
            trigger_type=trigger,
            scope=RecoveryScope.SINGLE_RECORD,
            affected_records=[str(r_id) for r_id in record_ids],
            status=RecoveryStatus.RUNNING
        )
        
        try:
            await self._log_recovery_start(operation)
            
            # ì§€ì—°ëœ ë ˆì½”ë“œë“¤ì„ ë§ˆì´ê·¸ë ˆì´ì…˜ íŒŒì´í”„ë¼ì¸ì— ë‹¤ì‹œ íˆ¬ì…
            for record_id in record_ids:
                try:
                    # ë ˆì½”ë“œ ìƒíƒœë¥¼ pendingìœ¼ë¡œ ë¦¬ì…‹
                    await self._reset_record_processing_status(record_id)
                    
                    # ë§ˆì´ê·¸ë ˆì´ì…˜ ì¬ì‹œë„
                    await self._trigger_migration_for_record(record_id)
                    operation.records_recovered += 1
                    
                except Exception as e:
                    logger.error(f"âŒ ë ˆì½”ë“œ {record_id} ë³µêµ¬ ì‹¤íŒ¨: {str(e)}")
                    operation.records_failed += 1
            
            operation.status = RecoveryStatus.COMPLETED
            operation.completed_at = datetime.now()
            
            await self._log_recovery_completion(operation)
            self.recovery_stats['records_recovered'] += operation.records_recovered
            self.recovery_stats['operations_completed'] += 1
            
        except Exception as e:
            operation.status = RecoveryStatus.FAILED
            operation.recovery_details['error'] = str(e)
            operation.completed_at = datetime.now()
            
            await self._log_recovery_error(operation, str(e))
            self.recovery_stats['operations_failed'] += 1
        
        return operation
    
    async def _get_session_info(self, session_id: UUID) -> Optional[Dict]:
        """ì„¸ì…˜ ì •ë³´ ì¡°íšŒ"""
        query = "SELECT * FROM scraping_sessions WHERE id = $1"
        
        async with self.db_pool.acquire() as conn:
            row = await conn.fetchrow(query, session_id)
            return dict(row) if row else None
    
    async def _find_session_backups(self, session_id: UUID) -> List[BackupRecord]:
        """ì„¸ì…˜ê³¼ ê´€ë ¨ëœ ë°±ì—… ê²€ìƒ‰"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°±ì—… ë©”íƒ€ë°ì´í„°ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì„¸ì…˜ IDì™€ ë§¤ì¹­ë˜ëŠ” ë°±ì—…ì„ ì°¾ì•„ì•¼ í•¨
        # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        return []
    
    async def _find_date_range_backups(self, portal_id: str, start_date: datetime.date, 
                                     end_date: datetime.date) -> List[BackupRecord]:
        """ë‚ ì§œ ë²”ìœ„ì˜ ë°±ì—… ê²€ìƒ‰"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°±ì—… ìƒì„± ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ê²€ìƒ‰
        return []
    
    async def _restore_to_primary_db(self, backup_record: BackupRecord):
        """ë°±ì—… ë ˆì½”ë“œë¥¼ Primary DBì— ë³µì›"""
        query = """
            INSERT INTO raw_scraped_data (
                portal_id, url, raw_data, processing_status, scraped_at
            ) VALUES ($1, $2, $3, 'pending', $4)
            ON CONFLICT DO NOTHING
        """
        
        async with self.db_pool.acquire() as conn:
            await conn.execute(
                query,
                backup_record.data.get('portal_id'),
                backup_record.data.get('url', ''),
                json.dumps(backup_record.data),
                backup_record.created_at
            )
    
    async def _trigger_migration_for_recovered_data(self, source_id: str):
        """ë³µêµ¬ëœ ë°ì´í„°ì˜ ë§ˆì´ê·¸ë ˆì´ì…˜ íŠ¸ë¦¬ê±°"""
        # ë§ˆì´ê·¸ë ˆì´ì…˜ ì—”ì§„ì„ í†µí•´ ë³µêµ¬ëœ ë°ì´í„°ë¥¼ ì²˜ë¦¬
        try:
            await self.migration_engine.run_full_migration_pipeline()
        except Exception as e:
            logger.error(f"âŒ ë³µêµ¬ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {str(e)}")
    
    async def _trigger_migration_for_record(self, record_id: int):
        """íŠ¹ì • ë ˆì½”ë“œì˜ ë§ˆì´ê·¸ë ˆì´ì…˜ íŠ¸ë¦¬ê±°"""
        # ê°œë³„ ë ˆì½”ë“œ ì²˜ë¦¬ ë¡œì§
        pass
    
    async def _reset_record_processing_status(self, record_id: int):
        """ë ˆì½”ë“œ ì²˜ë¦¬ ìƒíƒœ ë¦¬ì…‹"""
        query = """
            UPDATE raw_scraped_data 
            SET processing_status = 'pending', processed_at = NULL 
            WHERE id = $1
        """
        
        async with self.db_pool.acquire() as conn:
            await conn.execute(query, record_id)
    
    async def _verify_session_recovery(self, session_id: UUID) -> Dict[str, Any]:
        """ì„¸ì…˜ ë³µêµ¬ ê²€ì¦"""
        # ë³µêµ¬ëœ ë°ì´í„°ì˜ í’ˆì§ˆê³¼ ì™„ì •ì„± ê²€ì¦
        return {'passed': True, 'details': 'Session recovery verified'}
    
    async def _verify_date_range_recovery(self, portal_id: str, start_date: datetime.date, 
                                        end_date: datetime.date) -> Dict[str, Any]:
        """ë‚ ì§œ ë²”ìœ„ ë³µêµ¬ ê²€ì¦"""
        return {'passed': True, 'details': 'Date range recovery verified'}
    
    async def _log_recovery_start(self, operation: RecoveryOperation):
        """ë³µêµ¬ ì‹œì‘ ë¡œê·¸"""
        query = """
            INSERT INTO recovery_operations (
                operation_id, trigger_type, recovery_scope, target_portal_id,
                target_session_id, affected_records, started_at, status
            ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
        """
        
        async with self.db_pool.acquire() as conn:
            await conn.execute(
                query,
                operation.operation_id,
                operation.trigger_type.value,
                operation.scope.value,
                operation.target_portal_id,
                operation.target_session_id,
                json.dumps(operation.affected_records),
                operation.started_at,
                operation.status.value
            )
    
    async def _log_recovery_completion(self, operation: RecoveryOperation):
        """ë³µêµ¬ ì™„ë£Œ ë¡œê·¸"""
        query = """
            UPDATE recovery_operations
            SET completed_at = $1, status = $2, records_recovered = $3,
                records_failed = $4, recovery_details = $5,
                verification_passed = $6, verification_details = $7
            WHERE operation_id = $8
        """
        
        async with self.db_pool.acquire() as conn:
            await conn.execute(
                query,
                operation.completed_at,
                operation.status.value,
                operation.records_recovered,
                operation.records_failed,
                json.dumps(operation.recovery_details),
                operation.verification_passed,
                json.dumps(operation.verification_details),
                operation.operation_id
            )
    
    async def _log_recovery_error(self, operation: RecoveryOperation, error_message: str):
        """ë³µêµ¬ ì˜¤ë¥˜ ë¡œê·¸"""
        await self._log_recovery_completion(operation)
        logger.error(f"âŒ ë³µêµ¬ ì‘ì—… ì‹¤íŒ¨ {operation.operation_id}: {error_message}")
    
    def get_recovery_statistics(self) -> Dict[str, Any]:
        """ë³µêµ¬ í†µê³„ ì¡°íšŒ"""
        return {
            'recovery_statistics': self.recovery_stats.copy(),
            'last_updated': datetime.now().isoformat()
        }

class DataSynchronizationManager:
    """ë°ì´í„° ë™ê¸°í™” ê´€ë¦¬ì"""
    
    def __init__(self, backup_system: MultiTierBackupSystem, recovery_engine: DataRecoveryEngine):
        self.backup_system = backup_system
        self.recovery_engine = recovery_engine
    
    async def synchronize_backup_tiers(self) -> Dict[str, Any]:
        """ë°±ì—… í‹°ì–´ ê°„ ë™ê¸°í™”"""
        logger.info("ğŸ”„ ë°±ì—… í‹°ì–´ ë™ê¸°í™” ì‹œì‘")
        
        sync_result = {
            'started_at': datetime.now().isoformat(),
            'primary_to_secondary': 0,
            'secondary_to_tertiary': 0,
            'inconsistencies_found': 0,
            'inconsistencies_resolved': 0
        }
        
        # ë™ê¸°í™” ë¡œì§ êµ¬í˜„
        # ì‹¤ì œë¡œëŠ” ê° í‹°ì–´ì˜ ë°ì´í„°ë¥¼ ë¹„êµí•˜ê³  ë¶ˆì¼ì¹˜í•˜ëŠ” ë¶€ë¶„ì„ ë™ê¸°í™”
        
        sync_result['completed_at'] = datetime.now().isoformat()
        logger.info("âœ… ë°±ì—… í‹°ì–´ ë™ê¸°í™” ì™„ë£Œ")
        
        return sync_result
    
    async def verify_data_consistency(self) -> Dict[str, Any]:
        """ë°ì´í„° ì¼ê´€ì„± ê²€ì¦"""
        logger.info("ğŸ” ë°ì´í„° ì¼ê´€ì„± ê²€ì¦ ì‹œì‘")
        
        consistency_result = {
            'verified_at': datetime.now().isoformat(),
            'total_records_checked': 0,
            'inconsistent_records': 0,
            'corruption_detected': 0,
            'auto_fixed': 0,
            'manual_intervention_required': 0
        }
        
        # ì¼ê´€ì„± ê²€ì¦ ë¡œì§
        # ê° í‹°ì–´ì˜ ì²´í¬ì„¬ì„ ë¹„êµí•˜ì—¬ ë¬´ê²°ì„± í™•ì¸
        
        logger.info("âœ… ë°ì´í„° ì¼ê´€ì„± ê²€ì¦ ì™„ë£Œ")
        return consistency_result