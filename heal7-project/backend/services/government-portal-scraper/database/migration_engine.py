"""
í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ ì—”ì§„
NoSQL (JSONB) â†’ ê´€ê³„í˜• DB 3ë‹¨ê³„ íŒŒì´í”„ë¼ì¸

Author: Paperwork AI Team
Version: 2.0.0
Date: 2025-08-23
"""

import asyncio
import asyncpg
import json
import logging
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, AsyncGenerator
from dataclasses import dataclass, field
from enum import Enum
import hashlib
import re
from uuid import UUID, uuid4

logger = logging.getLogger(__name__)

class MigrationStage(Enum):
    """ë§ˆì´ê·¸ë ˆì´ì…˜ ë‹¨ê³„"""
    RAW_COLLECTION = "raw_collection"        # ì›ë³¸ ë°ì´í„° ìˆ˜ì§‘
    DATA_PROCESSING = "data_processing"      # ë°ì´í„° ì •ì œ ë° ê²€ì¦
    RELATIONAL_MIGRATION = "relational_migration"  # ê´€ê³„í˜• í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜
    QUALITY_ASSURANCE = "quality_assurance"  # í’ˆì§ˆ ë³´ì¥
    CLEANUP = "cleanup"                      # ì •ë¦¬ ì‘ì—…

class ProcessingStatus(Enum):
    """ì²˜ë¦¬ ìƒíƒœ"""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    DUPLICATE = "duplicate"
    SKIPPED = "skipped"

@dataclass
class MigrationConfig:
    """ë§ˆì´ê·¸ë ˆì´ì…˜ ì„¤ì •"""
    batch_size: int = 100
    max_concurrent: int = 5
    quality_threshold: float = 6.0
    enable_ai_validation: bool = True
    auto_cleanup: bool = True
    retry_failed_records: bool = True
    max_retries: int = 3
    processing_timeout_seconds: int = 300

@dataclass
class MigrationStats:
    """ë§ˆì´ê·¸ë ˆì´ì…˜ í†µê³„"""
    session_id: UUID = field(default_factory=uuid4)
    started_at: datetime = field(default_factory=datetime.now)
    completed_at: Optional[datetime] = None
    
    # ì²˜ë¦¬ í†µê³„
    total_raw_records: int = 0
    processed_records: int = 0
    migrated_records: int = 0
    failed_records: int = 0
    duplicate_records: int = 0
    
    # í’ˆì§ˆ í†µê³„
    high_quality_count: int = 0  # 8.0+ ì ìˆ˜
    medium_quality_count: int = 0  # 6.0-7.9 ì ìˆ˜
    low_quality_count: int = 0   # 6.0 ë¯¸ë§Œ
    
    # ì„±ëŠ¥ í†µê³„
    processing_time_seconds: float = 0.0
    average_processing_time_per_record: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'session_id': str(self.session_id),
            'started_at': self.started_at.isoformat(),
            'completed_at': self.completed_at.isoformat() if self.completed_at else None,
            'total_raw_records': self.total_raw_records,
            'processed_records': self.processed_records,
            'migrated_records': self.migrated_records,
            'failed_records': self.failed_records,
            'duplicate_records': self.duplicate_records,
            'high_quality_count': self.high_quality_count,
            'medium_quality_count': self.medium_quality_count,
            'low_quality_count': self.low_quality_count,
            'processing_time_seconds': self.processing_time_seconds,
            'average_processing_time_per_record': self.average_processing_time_per_record
        }

class DataValidator:
    """ë°ì´í„° ê²€ì¦ê¸°"""
    
    def __init__(self):
        self.required_fields = {
            'bizinfo': ['title', 'implementing_agency', 'application_period'],
            'kstartup': ['title', 'status', 'category']
        }
        
        self.field_patterns = {
            'phone': r'\d{2,3}-\d{3,4}-\d{4}',
            'email': r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}',
            'url': r'https?://[^\s]+',
            'date': r'\d{4}-\d{2}-\d{2}',
            'amount': r'\d{1,3}(?:,\d{3})*(?:\s*[ì–µë§Œì›]+)?'
        }
    
    def validate_raw_data(self, raw_data: Dict, portal_id: str) -> Tuple[bool, List[str], float]:
        """ì›ë³¸ ë°ì´í„° ê²€ì¦"""
        errors = []
        quality_score = 10.0
        
        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        required = self.required_fields.get(portal_id, [])
        for field in required:
            if not raw_data.get(field) or raw_data[field] in ['N/A', '', None]:
                errors.append(f"Missing required field: {field}")
                quality_score -= 2.0
        
        # ì œëª© ê²€ì¦
        title = raw_data.get('title', '')
        if len(title) < 10:
            errors.append("Title too short")
            quality_score -= 1.0
        elif len(title) > 300:
            errors.append("Title too long")
            quality_score -= 0.5
        
        # URL ê²€ì¦
        detail_url = raw_data.get('detail_url')
        if detail_url and not re.match(self.field_patterns['url'], detail_url):
            errors.append("Invalid detail URL format")
            quality_score -= 0.5
        
        # ë‚ ì§œ í˜•ì‹ ê²€ì¦
        app_period = raw_data.get('application_period', '')
        if app_period and app_period != 'N/A':
            # í•œêµ­ì–´ ë‚ ì§œ íŒ¨í„´ë„ í—ˆìš©
            if not (re.search(r'\d{4}', app_period) and re.search(r'\d{1,2}', app_period)):
                quality_score -= 0.5
        
        # ê¸°ê´€ëª… ê²€ì¦
        agency = raw_data.get('implementing_agency', raw_data.get('agency', ''))
        if agency and len(agency) < 3:
            errors.append("Agency name too short")
            quality_score -= 0.5
        
        # ì¤‘ë³µ ê²€ì¦ (í•´ì‹œ ê¸°ë°˜)
        content_hash = self._generate_content_hash(raw_data)
        raw_data['content_hash'] = content_hash
        
        quality_score = max(0.0, min(10.0, quality_score))
        is_valid = len(errors) == 0 or quality_score >= 6.0
        
        return is_valid, errors, quality_score
    
    def _generate_content_hash(self, raw_data: Dict) -> str:
        """ì½˜í…ì¸  í•´ì‹œ ìƒì„± (ì¤‘ë³µ ê²€ì¶œìš©)"""
        key_fields = ['title', 'implementing_agency', 'agency', 'application_period']
        content = ''.join(str(raw_data.get(field, '')) for field in key_fields)
        return hashlib.sha256(content.encode()).hexdigest()[:32]

class DataProcessor:
    """ë°ì´í„° ì²˜ë¦¬ê¸°"""
    
    def __init__(self, validator: DataValidator):
        self.validator = validator
        
    def process_raw_data(self, raw_data: Dict, portal_id: str) -> Dict[str, Any]:
        """ì›ë³¸ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ì—¬ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜"""
        processed = {
            'portal_id': portal_id,
            'raw_data': raw_data,
            'processed_at': datetime.now().isoformat(),
        }
        
        # í¬í„¸ë³„ ì²˜ë¦¬
        if portal_id == 'bizinfo':
            processed.update(self._process_bizinfo_data(raw_data))
        elif portal_id == 'kstartup':
            processed.update(self._process_kstartup_data(raw_data))
        else:
            processed.update(self._process_generic_data(raw_data))
        
        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        is_valid, errors, quality_score = self.validator.validate_raw_data(raw_data, portal_id)
        processed.update({
            'validation_errors': errors,
            'quality_score': quality_score,
            'is_valid': is_valid
        })
        
        return processed
    
    def _process_bizinfo_data(self, raw_data: Dict) -> Dict:
        """ê¸°ì—…ë§ˆë‹¹ ë°ì´í„° ì²˜ë¦¬"""
        return {
            'program_id': f"bizinfo_{raw_data.get('content_hash', '')}",
            'title': self._clean_text(raw_data.get('title', '')),
            'implementing_agency': self._clean_text(raw_data.get('implementing_agency', '')),
            'jurisdiction': self._clean_text(raw_data.get('jurisdiction', '')),
            'support_field': self._clean_text(raw_data.get('support_field', '')),
            'application_period': self._clean_text(raw_data.get('application_period', '')),
            'registration_date': self._parse_date(raw_data.get('registration_date', '')),
            'view_count': self._parse_integer(raw_data.get('view_count', 0)),
            'detail_url': raw_data.get('detail_url'),
            'contact_info': self._extract_contact_info(raw_data),
            'support_details': self._extract_support_details(raw_data)
        }
    
    def _process_kstartup_data(self, raw_data: Dict) -> Dict:
        """K-Startup ë°ì´í„° ì²˜ë¦¬"""
        return {
            'program_id': f"kstartup_{raw_data.get('content_hash', '')}",
            'title': self._clean_text(raw_data.get('title', '')),
            'implementing_agency': 'K-Startup',
            'support_field': self._clean_text(raw_data.get('category', '')),
            'application_period': self._clean_text(raw_data.get('period', raw_data.get('application_period', ''))),
            'application_status': self._normalize_status(raw_data.get('status', 'active')),
            'target_audience': self._clean_text(raw_data.get('target', '')),
            'description': self._clean_text(raw_data.get('description', '')),
            'detail_url': raw_data.get('detail_url'),
            'support_details': {
                'budget': raw_data.get('budget'),
                'category': raw_data.get('category'),
                'organization': raw_data.get('organization')
            }
        }
    
    def _process_generic_data(self, raw_data: Dict) -> Dict:
        """ë²”ìš© ë°ì´í„° ì²˜ë¦¬"""
        return {
            'program_id': f"generic_{raw_data.get('content_hash', '')}",
            'title': self._clean_text(raw_data.get('title', '')),
            'implementing_agency': self._clean_text(raw_data.get('agency', raw_data.get('implementing_agency', ''))),
            'description': self._clean_text(raw_data.get('description', '')),
            'detail_url': raw_data.get('detail_url')
        }
    
    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not text or text == 'N/A':
            return ''
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', str(text))
        
        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r'\s+', ' ', text)
        
        # ì•ë’¤ ê³µë°± ì œê±°
        return text.strip()
    
    def _parse_date(self, date_str: str) -> Optional[str]:
        """ë‚ ì§œ íŒŒì‹±"""
        if not date_str or date_str == 'N/A':
            return None
        
        # YYYY-MM-DD íŒ¨í„´ ì°¾ê¸°
        match = re.search(r'(\d{4})-(\d{2})-(\d{2})', str(date_str))
        if match:
            return match.group(0)
        
        return None
    
    def _parse_integer(self, value) -> int:
        """ì •ìˆ˜ íŒŒì‹±"""
        try:
            if isinstance(value, int):
                return value
            if isinstance(value, str):
                # ì½¤ë§ˆ ì œê±° í›„ ìˆ«ì ì¶”ì¶œ
                numbers = re.findall(r'\d+', value.replace(',', ''))
                if numbers:
                    return int(numbers[0])
            return 0
        except:
            return 0
    
    def _normalize_status(self, status: str) -> str:
        """ìƒíƒœ ì •ê·œí™”"""
        status_map = {
            'ëª¨ì§‘ì¤‘': 'active',
            'ëª¨ì§‘ë§ˆê°': 'closed',
            'ì ‘ìˆ˜ì¤‘': 'active',
            'ë§ˆê°': 'closed',
            'ì¢…ë£Œ': 'expired',
            'active': 'active',
            'closed': 'closed'
        }
        
        return status_map.get(str(status).strip(), 'active')
    
    def _extract_contact_info(self, raw_data: Dict) -> Dict:
        """ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ"""
        contact = {}
        
        # ì „í™”ë²ˆí˜¸ ì¶”ì¶œ
        for field in ['contact_info', 'phone', 'tel', 'contact']:
            value = raw_data.get(field, '')
            if value:
                phone_match = re.search(r'(\d{2,3}-\d{3,4}-\d{4})', str(value))
                if phone_match:
                    contact['phone'] = phone_match.group(1)
                    break
        
        # ì´ë©”ì¼ ì¶”ì¶œ
        for field in ['contact_info', 'email', 'contact']:
            value = raw_data.get(field, '')
            if value:
                email_match = re.search(r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})', str(value))
                if email_match:
                    contact['email'] = email_match.group(1)
                    break
        
        return contact
    
    def _extract_support_details(self, raw_data: Dict) -> Dict:
        """ì§€ì› ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
        details = {}
        
        # ì§€ì› ê¸ˆì•¡
        for field in ['support_amount', 'budget', 'amount']:
            value = raw_data.get(field, '')
            if value and value != 'N/A':
                details['support_amount'] = str(value)
                break
        
        # ì§€ì› ê¸°ê°„
        for field in ['support_period', 'period', 'duration']:
            value = raw_data.get(field, '')
            if value and value != 'N/A':
                details['support_period'] = str(value)
                break
        
        # ì§€ì› ë°©ì‹
        for field in ['support_type', 'type']:
            value = raw_data.get(field, '')
            if value and value != 'N/A':
                details['support_type'] = str(value)
                break
        
        return details

class MigrationEngine:
    """ë§ˆì´ê·¸ë ˆì´ì…˜ ì—”ì§„"""
    
    def __init__(self, db_pool: asyncpg.Pool, config: MigrationConfig = None):
        self.db_pool = db_pool
        self.config = config or MigrationConfig()
        self.validator = DataValidator()
        self.processor = DataProcessor(self.validator)
        self.stats = MigrationStats()
        
    async def run_full_migration_pipeline(self, portal_id: str = None, session_id: UUID = None) -> MigrationStats:
        """ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info(f"ğŸš€ ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜ íŒŒì´í”„ë¼ì¸ ì‹œì‘: {portal_id or 'ALL'}")
        
        self.stats = MigrationStats()
        if session_id:
            self.stats.session_id = session_id
        
        try:
            # 1ë‹¨ê³„: ì›ë³¸ ë°ì´í„° ìˆ˜ì§‘ ìƒíƒœ í™•ì¸
            await self._log_migration_start(portal_id)
            
            # 2ë‹¨ê³„: ë°ì´í„° ì²˜ë¦¬ (Raw â†’ Processed)
            await self._process_raw_data_batch(portal_id)
            
            # 3ë‹¨ê³„: ê´€ê³„í˜• ë§ˆì´ê·¸ë ˆì´ì…˜ (Processed â†’ Relational)
            await self._migrate_to_relational_batch(portal_id)
            
            # 4ë‹¨ê³„: í’ˆì§ˆ ë³´ì¥
            await self._quality_assurance_check(portal_id)
            
            # 5ë‹¨ê³„: ì •ë¦¬ ì‘ì—…
            if self.config.auto_cleanup:
                await self._cleanup_completed_records(portal_id)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.stats.completed_at = datetime.now()
            self.stats.processing_time_seconds = (
                self.stats.completed_at - self.stats.started_at
            ).total_seconds()
            
            if self.stats.processed_records > 0:
                self.stats.average_processing_time_per_record = (
                    self.stats.processing_time_seconds / self.stats.processed_records
                )
            
            await self._log_migration_completion()
            
            logger.info(f"âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ: {self.stats.migrated_records}ê°œ ë ˆì½”ë“œ")
            
        except Exception as e:
            logger.error(f"âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {str(e)}")
            await self._log_migration_error(str(e))
            raise
        
        return self.stats
    
    async def _process_raw_data_batch(self, portal_id: Optional[str]):
        """ì›ë³¸ ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬"""
        logger.info("ğŸ“Š 2ë‹¨ê³„: ì›ë³¸ ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘")
        
        async for batch in self._get_pending_raw_data_batches(portal_id):
            semaphore = asyncio.Semaphore(self.config.max_concurrent)
            
            tasks = [
                self._process_single_raw_record(semaphore, record) 
                for record in batch
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ê²°ê³¼ ë¶„ì„
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨ {batch[i]['id']}: {str(result)}")
                    self.stats.failed_records += 1
                elif result:
                    self.stats.processed_records += 1
    
    async def _get_pending_raw_data_batches(self, portal_id: Optional[str]) -> AsyncGenerator[List[Dict], None]:
        """ì²˜ë¦¬ ëŒ€ê¸° ì¤‘ì¸ ì›ë³¸ ë°ì´í„° ë°°ì¹˜ ìƒì„±"""
        query = """
            SELECT id, portal_id, raw_data, url, scraping_session_id
            FROM raw_scraped_data
            WHERE processing_status = 'pending'
        """
        params = []
        
        if portal_id:
            query += " AND portal_id = $1"
            params = [portal_id]
        
        query += " ORDER BY scraped_at ASC LIMIT $" + str(len(params) + 1)
        params.append(self.config.batch_size)
        
        async with self.db_pool.acquire() as conn:
            offset = 0
            while True:
                batch_query = query + f" OFFSET {offset}"
                rows = await conn.fetch(batch_query, *params)
                
                if not rows:
                    break
                
                batch = [dict(row) for row in rows]
                self.stats.total_raw_records += len(batch)
                yield batch
                
                offset += len(batch)
                
                if len(batch) < self.config.batch_size:
                    break
    
    async def _process_single_raw_record(self, semaphore: asyncio.Semaphore, record: Dict) -> bool:
        """ê°œë³„ ì›ë³¸ ë ˆì½”ë“œ ì²˜ë¦¬"""
        async with semaphore:
            try:
                # ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸
                await self._update_raw_record_status(record['id'], ProcessingStatus.PROCESSING)
                
                # ë°ì´í„° ì²˜ë¦¬
                processed_data = self.processor.process_raw_data(
                    record['raw_data'], 
                    record['portal_id']
                )
                
                # í’ˆì§ˆ ì ìˆ˜ì— ë”°ë¥¸ ì²˜ë¦¬
                quality_score = processed_data.get('quality_score', 0.0)
                
                if quality_score >= 8.0:
                    self.stats.high_quality_count += 1
                elif quality_score >= 6.0:
                    self.stats.medium_quality_count += 1
                else:
                    self.stats.low_quality_count += 1
                
                # ê²€ì¦ ì˜¤ë¥˜ ì €ì¥
                validation_errors = processed_data.get('validation_errors', [])
                
                # ì›ë³¸ ë ˆì½”ë“œ ì—…ë°ì´íŠ¸
                await self._update_raw_record_processed(
                    record['id'], 
                    quality_score,
                    validation_errors,
                    ProcessingStatus.COMPLETED if processed_data['is_valid'] else ProcessingStatus.FAILED
                )
                
                return processed_data['is_valid']
                
            except Exception as e:
                logger.error(f"âŒ ë ˆì½”ë“œ ì²˜ë¦¬ ì‹¤íŒ¨ {record['id']}: {str(e)}")
                await self._update_raw_record_status(record['id'], ProcessingStatus.FAILED)
                return False
    
    async def _migrate_to_relational_batch(self, portal_id: Optional[str]):
        """ê´€ê³„í˜• í…Œì´ë¸”ë¡œ ë°°ì¹˜ ë§ˆì´ê·¸ë ˆì´ì…˜"""
        logger.info("ğŸ”„ 3ë‹¨ê³„: ê´€ê³„í˜• í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
        
        async for batch in self._get_processed_data_batches(portal_id):
            semaphore = asyncio.Semaphore(self.config.max_concurrent)
            
            tasks = [
                self._migrate_single_record(semaphore, record) 
                for record in batch
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ê²°ê³¼ ë¶„ì„
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨ {batch[i]['id']}: {str(result)}")
                elif result:
                    self.stats.migrated_records += 1
    
    async def _get_processed_data_batches(self, portal_id: Optional[str]) -> AsyncGenerator[List[Dict], None]:
        """ì²˜ë¦¬ëœ ë°ì´í„° ë°°ì¹˜ ìƒì„±"""
        query = """
            SELECT id, portal_id, raw_data, quality_score, processing_status
            FROM raw_scraped_data
            WHERE processing_status = 'completed' AND quality_score >= $1 AND migrated_at IS NULL
        """
        params = [self.config.quality_threshold]
        
        if portal_id:
            query += " AND portal_id = $2"
            params.append(portal_id)
            limit_param = "$3"
        else:
            limit_param = "$2"
        
        query += f" ORDER BY quality_score DESC, scraped_at ASC LIMIT {limit_param}"
        params.append(self.config.batch_size)
        
        async with self.db_pool.acquire() as conn:
            offset = 0
            while True:
                batch_query = query + f" OFFSET {offset}"
                rows = await conn.fetch(batch_query, *params)
                
                if not rows:
                    break
                
                batch = [dict(row) for row in rows]
                yield batch
                
                offset += len(batch)
                
                if len(batch) < self.config.batch_size:
                    break
    
    async def _migrate_single_record(self, semaphore: asyncio.Semaphore, record: Dict) -> bool:
        """ê°œë³„ ë ˆì½”ë“œ ë§ˆì´ê·¸ë ˆì´ì…˜"""
        async with semaphore:
            try:
                processed_data = self.processor.process_raw_data(
                    record['raw_data'], 
                    record['portal_id']
                )
                
                # ì¤‘ë³µ í™•ì¸
                existing_program = await self._check_duplicate_program(processed_data.get('program_id'))
                if existing_program:
                    await self._mark_as_duplicate(record['id'])
                    self.stats.duplicate_records += 1
                    return False
                
                # support_programs í…Œì´ë¸”ì— ì‚½ì…
                await self._insert_support_program(processed_data, record['id'])
                
                # ì›ë³¸ ë ˆì½”ë“œì˜ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ í‘œì‹œ
                await self._mark_migration_completed(record['id'])
                
                return True
                
            except Exception as e:
                logger.error(f"âŒ ë ˆì½”ë“œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨ {record['id']}: {str(e)}")
                return False
    
    async def _insert_support_program(self, processed_data: Dict, raw_id: int):
        """support_programs í…Œì´ë¸”ì— ë°ì´í„° ì‚½ì…"""
        query = """
            INSERT INTO support_programs (
                program_id, portal_id, original_raw_id, title, description,
                support_field, implementing_agency, jurisdiction, contact_info,
                support_details, support_amount, support_period, support_type,
                application_period, application_status, target_audience,
                evaluation_criteria, required_documents, detail_url, attachments,
                view_count, registration_date, data_quality_score, verification_status
            ) VALUES (
                $1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17, $18, $19, $20, $21, $22, $23, $24
            ) ON CONFLICT (program_id) DO UPDATE SET
                updated_at = CURRENT_TIMESTAMP,
                data_quality_score = EXCLUDED.data_quality_score
        """
        
        support_details = processed_data.get('support_details', {})
        contact_info = processed_data.get('contact_info', {})
        
        async with self.db_pool.acquire() as conn:
            await conn.execute(
                query,
                processed_data.get('program_id'),
                processed_data.get('portal_id'),
                raw_id,
                processed_data.get('title', ''),
                processed_data.get('description', ''),
                processed_data.get('support_field', ''),
                processed_data.get('implementing_agency', ''),
                processed_data.get('jurisdiction', ''),
                json.dumps(contact_info) if contact_info else None,
                json.dumps(support_details) if support_details else None,
                support_details.get('support_amount'),
                support_details.get('support_period'),
                support_details.get('support_type'),
                processed_data.get('application_period', ''),
                processed_data.get('application_status', 'active'),
                processed_data.get('target_audience', ''),
                None,  # evaluation_criteria (ì¶”í›„ êµ¬í˜„)
                None,  # required_documents (ì¶”í›„ êµ¬í˜„)
                processed_data.get('detail_url'),
                None,  # attachments (ì¶”í›„ êµ¬í˜„)
                processed_data.get('view_count', 0),
                processed_data.get('registration_date'),
                processed_data.get('quality_score', 0.0),
                'unverified'
            )
    
    async def _check_duplicate_program(self, program_id: str) -> bool:
        """ì¤‘ë³µ í”„ë¡œê·¸ë¨ í™•ì¸"""
        query = "SELECT 1 FROM support_programs WHERE program_id = $1"
        
        async with self.db_pool.acquire() as conn:
            result = await conn.fetchval(query, program_id)
            return result is not None
    
    async def _update_raw_record_status(self, record_id: int, status: ProcessingStatus):
        """ì›ë³¸ ë ˆì½”ë“œ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        query = "UPDATE raw_scraped_data SET processing_status = $1 WHERE id = $2"
        
        async with self.db_pool.acquire() as conn:
            await conn.execute(query, status.value, record_id)
    
    async def _update_raw_record_processed(self, record_id: int, quality_score: float, 
                                         validation_errors: List[str], status: ProcessingStatus):
        """ì›ë³¸ ë ˆì½”ë“œ ì²˜ë¦¬ ì™„ë£Œ ì—…ë°ì´íŠ¸"""
        query = """
            UPDATE raw_scraped_data 
            SET processing_status = $1, quality_score = $2, validation_errors = $3, processed_at = CURRENT_TIMESTAMP
            WHERE id = $4
        """
        
        async with self.db_pool.acquire() as conn:
            await conn.execute(
                query, 
                status.value, 
                quality_score, 
                json.dumps(validation_errors),
                record_id
            )
    
    async def _mark_as_duplicate(self, record_id: int):
        """ì¤‘ë³µìœ¼ë¡œ ë§ˆí¬"""
        await self._update_raw_record_status(record_id, ProcessingStatus.DUPLICATE)
    
    async def _mark_migration_completed(self, record_id: int):
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ ë§ˆí¬"""
        query = "UPDATE raw_scraped_data SET migrated_at = CURRENT_TIMESTAMP WHERE id = $1"
        
        async with self.db_pool.acquire() as conn:
            await conn.execute(query, record_id)
    
    async def _quality_assurance_check(self, portal_id: Optional[str]):
        """í’ˆì§ˆ ë³´ì¥ ê²€ì‚¬"""
        logger.info("ğŸ” 4ë‹¨ê³„: í’ˆì§ˆ ë³´ì¥ ê²€ì‚¬ ì‹œì‘")
        
        # ë§ˆì´ê·¸ë ˆì´ì…˜ëœ ë°ì´í„°ì˜ í’ˆì§ˆ í™•ì¸
        query = """
            SELECT COUNT(*) as total_count,
                   COUNT(*) FILTER (WHERE data_quality_score >= 8.0) as high_quality,
                   COUNT(*) FILTER (WHERE data_quality_score >= 6.0) as acceptable_quality
            FROM support_programs sp
            JOIN raw_scraped_data rsd ON sp.original_raw_id = rsd.id
            WHERE rsd.migrated_at >= $1
        """
        
        params = [self.stats.started_at]
        if portal_id:
            query += " AND sp.portal_id = $2"
            params.append(portal_id)
        
        async with self.db_pool.acquire() as conn:
            result = await conn.fetchrow(query, *params)
            
            if result:
                total = result['total_count']
                high_quality = result['high_quality']
                acceptable = result['acceptable_quality']
                
                logger.info(f"ğŸ“Š í’ˆì§ˆ ë³´ì¥ ê²°ê³¼:")
                logger.info(f"  ì „ì²´: {total}ê°œ")
                logger.info(f"  ê³ í’ˆì§ˆ (8.0+): {high_quality}ê°œ ({high_quality/total*100:.1f}%)")
                logger.info(f"  í—ˆìš© í’ˆì§ˆ (6.0+): {acceptable}ê°œ ({acceptable/total*100:.1f}%)")
    
    async def _cleanup_completed_records(self, portal_id: Optional[str]):
        """ì™„ë£Œëœ ë ˆì½”ë“œ ì •ë¦¬"""
        logger.info("ğŸ§¹ 5ë‹¨ê³„: ì •ë¦¬ ì‘ì—… ì‹œì‘")
        
        # 30ì¼ ì´ìƒ ëœ ì™„ë£Œ ë ˆì½”ë“œì˜ HTML ì½˜í…ì¸  ì œê±° (ìš©ëŸ‰ ì ˆì•½)
        cleanup_query = """
            UPDATE raw_scraped_data 
            SET html_content = NULL 
            WHERE processing_status = 'completed' 
              AND migrated_at IS NOT NULL
              AND scraped_at < CURRENT_TIMESTAMP - INTERVAL '30 days'
        """
        
        if portal_id:
            cleanup_query += " AND portal_id = $1"
            params = [portal_id]
        else:
            params = []
        
        async with self.db_pool.acquire() as conn:
            result = await conn.execute(cleanup_query, *params)
            logger.info(f"ğŸ§¹ HTML ì½˜í…ì¸  ì •ë¦¬ ì™„ë£Œ: {result} ê±´")
    
    async def _log_migration_start(self, portal_id: Optional[str]):
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘ ë¡œê·¸"""
        query = """
            INSERT INTO scraping_sessions (id, portal_id, session_type, status)
            VALUES ($1, $2, 'migration', 'running')
        """
        
        async with self.db_pool.acquire() as conn:
            await conn.execute(query, self.stats.session_id, portal_id or 'all')
    
    async def _log_migration_completion(self):
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ ë¡œê·¸"""
        query = """
            UPDATE scraping_sessions 
            SET completed_at = CURRENT_TIMESTAMP,
                status = 'completed',
                items_found = $1,
                items_processed = $2,
                items_migrated = $3,
                total_duration_seconds = $4
            WHERE id = $5
        """
        
        async with self.db_pool.acquire() as conn:
            await conn.execute(
                query,
                self.stats.total_raw_records,
                self.stats.processed_records,
                self.stats.migrated_records,
                int(self.stats.processing_time_seconds),
                self.stats.session_id
            )
    
    async def _log_migration_error(self, error_message: str):
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ì˜¤ë¥˜ ë¡œê·¸"""
        query = """
            UPDATE scraping_sessions 
            SET completed_at = CURRENT_TIMESTAMP,
                status = 'failed',
                error_details = $1
            WHERE id = $2
        """
        
        async with self.db_pool.acquire() as conn:
            await conn.execute(
                query,
                json.dumps({'error': error_message, 'stats': self.stats.to_dict()}),
                self.stats.session_id
            )
    
    async def get_migration_status(self, session_id: UUID) -> Optional[Dict]:
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒíƒœ ì¡°íšŒ"""
        query = """
            SELECT * FROM scraping_sessions WHERE id = $1
        """
        
        async with self.db_pool.acquire() as conn:
            result = await conn.fetchrow(query, session_id)
            return dict(result) if result else None