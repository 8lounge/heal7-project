"""
K-Startup (k-startup.go.kr) ì „ìš© SPA ìŠ¤í¬ë˜í¼
ì •ë¶€ ì°½ì—… ì§€ì› í”Œë«í¼ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

Author: Paperwork AI Team
Version: 2.0.0
Date: 2025-08-23
"""

import asyncio
import hashlib
import json
import logging
import re
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from urllib.parse import urljoin, urlparse, parse_qs

import aiohttp
from bs4 import BeautifulSoup
from dataclasses import dataclass
from playwright.async_api import async_playwright, Browser, Page

from utils.rate_limiter import RateLimiter
from utils.content_cleaner import ContentCleaner
from database.db_manager import DatabaseManager

logger = logging.getLogger(__name__)

@dataclass
class KStartupConfig:
    """K-Startup ìŠ¤í¬ë˜í•‘ ì„¤ì •"""
    base_url: str = "https://www.k-startup.go.kr"
    max_pages: int = 30
    max_concurrent: int = 3  # SPAì´ë¯€ë¡œ ë‚®ê²Œ ì„¤ì •
    request_delay: float = 2.0  # SPAì´ë¯€ë¡œ ë†’ê²Œ ì„¤ì •
    timeout: int = 45
    retries: int = 3
    headless: bool = True
    viewport_width: int = 1920
    viewport_height: int = 1080

class KStartupScraper:
    """K-Startup ì „ìš© SPA ëŒ€ì‘ ìŠ¤í¬ë˜í¼"""
    
    def __init__(self, db_manager: DatabaseManager, rate_limiter: RateLimiter):
        self.config = KStartupConfig()
        self.db_manager = db_manager
        self.rate_limiter = rate_limiter
        self.content_cleaner = ContentCleaner()
        
        # Playwright ë¸Œë¼ìš°ì € ì„¤ì •
        self.playwright = None
        self.browser = None
        self.pages_pool = []
        
        # ìŠ¤í¬ë˜í•‘ ëŒ€ìƒ URL
        self.target_urls = {
            'business_announcements': '/homepage/powerup/business/list.do',
            'startup_programs': '/homepage/bizplan/program/list.do',
            'education_courses': '/homepage/academy/education/list.do',
            'contest_events': '/homepage/contest/list.do',
            'support_programs': '/homepage/support/program/list.do'
        }
        
        # API ì—”ë“œí¬ì¸íŠ¸ (ê°€ëŠ¥í•œ ê²½ìš°)
        self.api_endpoints = {
            'business_list': '/api/business/announcements',
            'program_list': '/api/programs/list',
            'education_list': '/api/education/courses'
        }
        
        # ìˆ˜ì§‘ í†µê³„
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'programs_found': 0,
            'api_calls': 0,
            'browser_actions': 0,
            'start_time': None
        }
        
        logger.info("ğŸš€ K-Startup SPA ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def initialize(self):
        """ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™” (Playwright ë¸Œë¼ìš°ì € ì‹œì‘)"""
        if not self.playwright:
            self.playwright = await async_playwright().start()
            
            # ë¸Œë¼ìš°ì € ì‹œì‘
            self.browser = await self.playwright.chromium.launch(
                headless=self.config.headless,
                args=[
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-extensions',
                    '--disable-gpu',
                    '--disable-web-security',
                    '--disable-features=VizDisplayCompositor'
                ]
            )
            
            # í˜ì´ì§€ í’€ ìƒì„±
            await self.create_page_pool()
            
            logger.info("âœ… K-Startup ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def create_page_pool(self):
        """ë¸Œë¼ìš°ì € í˜ì´ì§€ í’€ ìƒì„±"""
        for i in range(self.config.max_concurrent):
            context = await self.browser.new_context(
                viewport={'width': self.config.viewport_width, 'height': self.config.viewport_height},
                user_agent='Mozilla/5.0 (compatible; PaperworkAI-KStartup/2.0; +http://paperwork.heal7.com/bot)'
            )
            
            page = await context.new_page()
            
            # ê¸°ë³¸ ì„¤ì •
            await page.set_extra_http_headers({
                'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
            })
            
            self.pages_pool.append(page)
            
        logger.info(f"ğŸ“„ ë¸Œë¼ìš°ì € í˜ì´ì§€ í’€ ìƒì„± ì™„ë£Œ: {len(self.pages_pool)}ê°œ")
    
    async def close(self):
        """ìŠ¤í¬ë˜í¼ ì •ë¦¬"""
        if self.pages_pool:
            for page in self.pages_pool:
                await page.context.close()
            self.pages_pool.clear()
        
        if self.browser:
            await self.browser.close()
            self.browser = None
            
        if self.playwright:
            await self.playwright.stop()
            self.playwright = None
            
        logger.info("ğŸ”’ K-Startup ìŠ¤í¬ë˜í¼ ì •ë¦¬ ì™„ë£Œ")
    
    async def get_status(self) -> Dict:
        """ìŠ¤í¬ë˜í¼ ìƒíƒœ ì¡°íšŒ"""
        return {
            "scraper": "kstartup",
            "status": "active" if self.browser else "inactive",
            "browser_pages": len(self.pages_pool),
            "stats": self.stats.copy(),
            "last_run": await self.db_manager.get_last_scraping_time('kstartup')
        }
    
    async def scrape_all_programs(self, force_update: bool = False) -> List[Dict]:
        """ëª¨ë“  K-Startup í”„ë¡œê·¸ë¨ ìŠ¤í¬ë˜í•‘"""
        self.stats['start_time'] = datetime.now()
        
        if not self.browser:
            await self.initialize()
        
        logger.info("ğŸš€ K-Startup ì „ì²´ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
        
        all_programs = []
        
        # API ë¨¼ì € ì‹œë„
        api_programs = await self.try_api_scraping()
        if api_programs:
            all_programs.extend(api_programs)
            logger.info(f"ğŸ“¡ API ìŠ¤í¬ë˜í•‘: {len(api_programs)}ê°œ í”„ë¡œê·¸ë¨")
        
        # ë¸Œë¼ìš°ì € ìŠ¤í¬ë˜í•‘
        for category_name, url_path in self.target_urls.items():
            logger.info(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {category_name}")
            
            try:
                programs = await self.scrape_category_spa(url_path, category_name, force_update)
                all_programs.extend(programs)
                self.stats['programs_found'] += len(programs)
                
                logger.info(f"âœ… {category_name} ì™„ë£Œ: {len(programs)}ê°œ í”„ë¡œê·¸ë¨")
                
                # ì¹´í…Œê³ ë¦¬ê°„ ë”œë ˆì´
                await asyncio.sleep(self.config.request_delay)
                
            except Exception as e:
                logger.error(f"âŒ {category_name} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {str(e)}")
                continue
        
        # ì¤‘ë³µ ì œê±°
        unique_programs = self.remove_duplicates(all_programs)
        
        total_time = (datetime.now() - self.stats['start_time']).total_seconds()
        logger.info(f"ğŸ‰ K-Startup ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(unique_programs)}ê°œ í”„ë¡œê·¸ë¨ ({total_time:.1f}ì´ˆ)")
        
        return unique_programs
    
    async def try_api_scraping(self) -> List[Dict]:
        """API ì—”ë“œí¬ì¸íŠ¸ ìŠ¤í¬ë˜í•‘ ì‹œë„"""
        api_programs = []
        
        async with aiohttp.ClientSession() as session:
            for endpoint_name, endpoint_path in self.api_endpoints.items():
                try:
                    await self.rate_limiter.acquire()
                    
                    url = f"{self.config.base_url}{endpoint_path}"
                    
                    async with session.get(url, timeout=aiohttp.ClientTimeout(total=15)) as response:
                        if response.status == 200:
                            data = await response.json()
                            
                            if isinstance(data, dict) and 'data' in data:
                                programs = data['data']
                            elif isinstance(data, list):
                                programs = data
                            else:
                                continue
                            
                            # API ë°ì´í„° í‘œì¤€í™”
                            standardized_programs = [
                                self.standardize_api_data(program, endpoint_name) 
                                for program in programs
                            ]
                            
                            api_programs.extend(standardized_programs)
                            self.stats['api_calls'] += 1
                            
                            logger.info(f"ğŸ“¡ API ì„±ê³µ {endpoint_name}: {len(programs)}ê°œ")
                            
                        else:
                            logger.debug(f"âš ï¸ API ì‹¤íŒ¨ {endpoint_name}: HTTP {response.status}")
                            
                except Exception as e:
                    logger.debug(f"âš ï¸ API ì˜¤ë¥˜ {endpoint_name}: {str(e)}")
                    continue
        
        return api_programs
    
    def standardize_api_data(self, api_data: Dict, endpoint_name: str) -> Dict:
        """API ë°ì´í„°ë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        standardized = {
            'title': api_data.get('title', api_data.get('name', 'N/A')),
            'agency': 'K-Startup',
            'category': endpoint_name,
            'status': api_data.get('status', api_data.get('state', 'N/A')),
            'application_period': api_data.get('period', api_data.get('deadline', 'N/A')),
            'detail_url': self.construct_detail_url(api_data, endpoint_name),
            'program_id': self.generate_program_id_from_api(api_data, endpoint_name),
            'portal_id': 'kstartup',
            'scraped_at': datetime.now().isoformat(),
            'source': 'api'
        }
        
        # ì¶”ê°€ í•„ë“œ
        if 'description' in api_data:
            standardized['description'] = api_data['description']
        if 'budget' in api_data:
            standardized['budget'] = api_data['budget']
        
        return standardized
    
    async def scrape_category_spa(self, url_path: str, category_name: str, force_update: bool) -> List[Dict]:
        """SPA ì¹´í…Œê³ ë¦¬ ìŠ¤í¬ë˜í•‘"""
        programs = []
        page = await self.get_page_from_pool()
        
        if not page:
            logger.error("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ í˜ì´ì§€ ì—†ìŒ")
            return []
        
        try:
            full_url = f"{self.config.base_url}{url_path}"
            
            # í˜ì´ì§€ ë¡œë“œ
            await self.rate_limiter.acquire()
            await page.goto(full_url, wait_until='networkidle', timeout=30000)
            
            self.stats['browser_actions'] += 1
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            await self.wait_for_content_load(page)
            
            # ë¬´í•œìŠ¤í¬ë¡¤ ë˜ëŠ” í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬
            await self.handle_pagination_or_scroll(page, category_name)
            
            # í”„ë¡œê·¸ë¨ ëª©ë¡ ì¶”ì¶œ
            program_elements = await page.query_selector_all(self.get_program_selectors(category_name))
            
            logger.info(f"ğŸ“‹ {category_name} ìš”ì†Œ ë°œê²¬: {len(program_elements)}ê°œ")
            
            # ê° í”„ë¡œê·¸ë¨ ì •ë³´ ì¶”ì¶œ
            for i, element in enumerate(program_elements):
                try:
                    program_data = await self.extract_program_from_element(element, page, category_name)
                    if program_data:
                        programs.append(program_data)
                        
                        # ì§„í–‰ë¥  ë¡œê¹…
                        if (i + 1) % 10 == 0:
                            logger.info(f"ğŸ“Š {category_name} ì§„í–‰ë¥ : {i + 1}/{len(program_elements)}")
                            
                except Exception as e:
                    logger.error(f"âŒ í”„ë¡œê·¸ë¨ ì¶”ì¶œ ì˜¤ë¥˜ {i}: {str(e)}")
                    continue
            
        except Exception as e:
            logger.error(f"âŒ {category_name} SPA ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {str(e)}")
        
        finally:
            await self.return_page_to_pool(page)
        
        return programs
    
    async def get_page_from_pool(self) -> Optional[Page]:
        """í˜ì´ì§€ í’€ì—ì„œ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°"""
        if self.pages_pool:
            return self.pages_pool.pop()
        return None
    
    async def return_page_to_pool(self, page: Page):
        """í˜ì´ì§€ë¥¼ í’€ì— ë°˜í™˜"""
        try:
            # í˜ì´ì§€ ì •ë¦¬
            await page.evaluate('() => { document.body.innerHTML = ""; }')
            self.pages_pool.append(page)
        except Exception as e:
            logger.error(f"âŒ í˜ì´ì§€ ë°˜í™˜ ì˜¤ë¥˜: {str(e)}")
    
    async def wait_for_content_load(self, page: Page):
        """ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°"""
        try:
            # ì¼ë°˜ì ì¸ ë¡œë”© ì¸ë””ì¼€ì´í„° ëŒ€ê¸°
            await page.wait_for_selector('body', timeout=10000)
            
            # ì¶”ê°€ ë¡œë”© ëŒ€ê¸° (Ajax ë“±)
            await page.wait_for_timeout(3000)
            
            # ë¡œë”© ìŠ¤í”¼ë„ˆê°€ ì‚¬ë¼ì§ˆ ë•Œê¹Œì§€ ëŒ€ê¸°
            try:
                await page.wait_for_selector('.loading, .spinner, .ajax-loading', state='detached', timeout=5000)
            except:
                pass  # ë¡œë”© ìŠ¤í”¼ë„ˆê°€ ì—†ì„ ìˆ˜ë„ ìˆìŒ
                
        except Exception as e:
            logger.warning(f"âš ï¸ ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸° ì˜¤ë¥˜: {str(e)}")
    
    async def handle_pagination_or_scroll(self, page: Page, category_name: str):
        """í˜ì´ì§€ë„¤ì´ì…˜ ë˜ëŠ” ë¬´í•œìŠ¤í¬ë¡¤ ì²˜ë¦¬"""
        try:
            # ë¬´í•œìŠ¤í¬ë¡¤ í™•ì¸
            has_infinite_scroll = await page.evaluate('''
                () => {
                    const scrollElements = document.querySelectorAll('[data-infinite], .infinite-scroll, .load-more');
                    return scrollElements.length > 0;
                }
            ''')
            
            if has_infinite_scroll:
                await self.handle_infinite_scroll(page)
            else:
                await self.handle_pagination(page)
                
        except Exception as e:
            logger.error(f"âŒ í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
    
    async def handle_infinite_scroll(self, page: Page):
        """ë¬´í•œìŠ¤í¬ë¡¤ ì²˜ë¦¬"""
        logger.info("ğŸ“œ ë¬´í•œìŠ¤í¬ë¡¤ ê°ì§€, ìŠ¤í¬ë¡¤ ì‹œì‘")
        
        previous_height = 0
        scroll_attempts = 0
        max_scrolls = 10
        
        while scroll_attempts < max_scrolls:
            # í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤
            await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
            
            # ìƒˆ ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°
            await page.wait_for_timeout(2000)
            
            # í˜ì´ì§€ ë†’ì´ í™•ì¸
            current_height = await page.evaluate('document.body.scrollHeight')
            
            if current_height == previous_height:
                # ë” ì´ìƒ ìƒˆë¡œìš´ ì½˜í…ì¸ ê°€ ì—†ìŒ
                break
            
            previous_height = current_height
            scroll_attempts += 1
            
            logger.info(f"ğŸ“œ ìŠ¤í¬ë¡¤ {scroll_attempts}/{max_scrolls}: ë†’ì´ {current_height}")
        
        logger.info(f"âœ… ë¬´í•œìŠ¤í¬ë¡¤ ì™„ë£Œ: {scroll_attempts}íšŒ ìŠ¤í¬ë¡¤")
    
    async def handle_pagination(self, page: Page):
        """í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬"""
        logger.info("ğŸ“„ í˜ì´ì§€ë„¤ì´ì…˜ ê°ì§€, í˜ì´ì§€ ìˆœíšŒ ì‹œì‘")
        
        current_page = 1
        max_pages = min(self.config.max_pages, 10)  # SPAëŠ” í˜ì´ì§€ ì œí•œ
        
        while current_page <= max_pages:
            try:
                # ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ ì°¾ê¸°
                next_selectors = [
                    '.pagination .next:not(.disabled)',
                    '.paging .next:not(.disabled)', 
                    '.page-nav .next:not(.disabled)',
                    'a[onclick*="page"]:has-text("ë‹¤ìŒ")',
                    'button:has-text("ë‹¤ìŒ"):not(:disabled)'
                ]
                
                next_button = None
                for selector in next_selectors:
                    try:
                        next_button = await page.query_selector(selector)
                        if next_button:
                            break
                    except:
                        continue
                
                if not next_button:
                    logger.info("ğŸ“„ ë” ì´ìƒ í˜ì´ì§€ ì—†ìŒ")
                    break
                
                # ë‹¤ìŒ í˜ì´ì§€ í´ë¦­
                await next_button.click()
                await self.wait_for_content_load(page)
                
                current_page += 1
                logger.info(f"ğŸ“„ í˜ì´ì§€ {current_page}/{max_pages} ë¡œë“œ ì™„ë£Œ")
                
            except Exception as e:
                logger.error(f"âŒ í˜ì´ì§€ë„¤ì´ì…˜ ì˜¤ë¥˜: {str(e)}")
                break
        
        logger.info(f"âœ… í˜ì´ì§€ë„¤ì´ì…˜ ì™„ë£Œ: {current_page}í˜ì´ì§€ ì²˜ë¦¬")
    
    def get_program_selectors(self, category_name: str) -> str:
        """ì¹´í…Œê³ ë¦¬ë³„ í”„ë¡œê·¸ë¨ ì…€ë ‰í„° ë°˜í™˜"""
        selector_map = {
            'business_announcements': '.business-card, .announce-item, .program-item',
            'startup_programs': '.program-card, .startup-item, .biz-item', 
            'education_courses': '.course-card, .edu-item, .education-item',
            'contest_events': '.contest-card, .event-item, .competition-item',
            'support_programs': '.support-card, .program-card, .support-item'
        }
        
        return selector_map.get(category_name, '.card, .item, .program, .list-item, tr[onclick]')
    
    async def extract_program_from_element(self, element, page: Page, category_name: str) -> Optional[Dict]:
        """ìš”ì†Œì—ì„œ í”„ë¡œê·¸ë¨ ì •ë³´ ì¶”ì¶œ"""
        try:
            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            title = await self.extract_text_from_element(element, ['.title', 'h3', 'h4', '.subject', 'strong'])
            status = await self.extract_text_from_element(element, ['.status', '.state', '.badge', '.label'])
            period = await self.extract_text_from_element(element, ['.period', '.date', '.deadline', '.term'])
            description = await self.extract_text_from_element(element, ['.description', '.content', '.summary'])
            
            # ë§í¬ ì¶”ì¶œ
            detail_url = await self.extract_link_from_element(element, page)
            
            # ì¶”ê°€ ì •ë³´
            budget = await self.extract_text_from_element(element, ['.budget', '.amount', '.money'])
            target = await self.extract_text_from_element(element, ['.target', '.audience', '.who'])
            
            if not title or title == 'N/A':
                return None
            
            program_data = {
                'title': self.content_cleaner.clean_title(title),
                'agency': 'K-Startup',
                'category': category_name,
                'status': self.content_cleaner.clean_text(status) if status != 'N/A' else 'N/A',
                'application_period': self.content_cleaner.clean_text(period) if period != 'N/A' else 'N/A',
                'description': self.content_cleaner.clean_text(description) if description != 'N/A' else '',
                'detail_url': detail_url,
                'program_id': self.generate_program_id_from_spa({'title': title, 'category': category_name}),
                'portal_id': 'kstartup',
                'scraped_at': datetime.now().isoformat(),
                'source': 'spa'
            }
            
            # ì„ íƒì  í•„ë“œ
            if budget != 'N/A':
                program_data['budget'] = self.content_cleaner.clean_text(budget)
            if target != 'N/A':
                program_data['target_audience'] = self.content_cleaner.clean_text(target)
            
            # í•´ì‹œê°’ ìƒì„±
            program_data['hash_value'] = self.generate_content_hash(program_data)
            
            return program_data
            
        except Exception as e:
            logger.error(f"âŒ ìš”ì†Œì—ì„œ í”„ë¡œê·¸ë¨ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return None
    
    async def extract_text_from_element(self, element, selectors: List[str]) -> str:
        """ìš”ì†Œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„)"""
        try:
            for selector in selectors:
                try:
                    sub_element = await element.query_selector(selector)
                    if sub_element:
                        text = await sub_element.inner_text()
                        if text and text.strip():
                            return text.strip()
                except:
                    continue
            
            # ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
            text = await element.inner_text()
            return text.strip() if text else 'N/A'
            
        except:
            return 'N/A'
    
    async def extract_link_from_element(self, element, page: Page) -> Optional[str]:
        """ìš”ì†Œì—ì„œ ë§í¬ ì¶”ì¶œ"""
        try:
            # ì§ì ‘ ë§í¬
            link_element = await element.query_selector('a[href]')
            if link_element:
                href = await link_element.get_attribute('href')
                if href:
                    return urljoin(self.config.base_url, href)
            
            # onclick ì´ë²¤íŠ¸ì—ì„œ ë§í¬ ì¶”ì¶œ
            onclick = await element.get_attribute('onclick')
            if onclick:
                # JavaScript í•¨ìˆ˜ì—ì„œ ID ì¶”ì¶œ
                match = re.search(r'(?:viewDetail|showDetail|goDetail)\s*\(\s*[\'"]([^\'"]+)[\'"]', onclick)
                if match:
                    detail_id = match.group(1)
                    return f"{self.config.base_url}/homepage/detail.do?id={detail_id}"
            
            return None
            
        except:
            return None
    
    def construct_detail_url(self, api_data: Dict, endpoint_name: str) -> Optional[str]:
        """API ë°ì´í„°ì—ì„œ ìƒì„¸ URL êµ¬ì„±"""
        detail_id = api_data.get('id', api_data.get('programId', api_data.get('announcementId')))
        
        if not detail_id:
            return None
        
        url_patterns = {
            'business_list': f"/homepage/powerup/business/detail.do?id={detail_id}",
            'program_list': f"/homepage/bizplan/program/detail.do?id={detail_id}",
            'education_list': f"/homepage/academy/education/detail.do?id={detail_id}"
        }
        
        path = url_patterns.get(endpoint_name, f"/homepage/detail.do?id={detail_id}")
        return f"{self.config.base_url}{path}"
    
    def generate_program_id_from_api(self, api_data: Dict, endpoint_name: str) -> str:
        """API ë°ì´í„°ì—ì„œ í”„ë¡œê·¸ë¨ ID ìƒì„±"""
        content = f"kstartup_{endpoint_name}_{api_data.get('id', '')}{api_data.get('title', '')}"
        return hashlib.md5(content.encode()).hexdigest()[:12]
    
    def generate_program_id_from_spa(self, spa_data: Dict) -> str:
        """SPA ë°ì´í„°ì—ì„œ í”„ë¡œê·¸ë¨ ID ìƒì„±"""
        content = f"kstartup_{spa_data.get('category', '')}_{spa_data.get('title', '')}"
        return hashlib.md5(content.encode()).hexdigest()[:12]
    
    def generate_content_hash(self, program_data: Dict) -> str:
        """ì½˜í…ì¸  ë³€ê²½ ê°ì§€ìš© í•´ì‹œ"""
        key_fields = ['title', 'status', 'application_period', 'description']
        content = ''.join(str(program_data.get(field, '')) for field in key_fields)
        return hashlib.sha256(content.encode()).hexdigest()
    
    def remove_duplicates(self, programs: List[Dict]) -> List[Dict]:
        """ì¤‘ë³µ í”„ë¡œê·¸ë¨ ì œê±°"""
        seen_ids = set()
        unique_programs = []
        
        for program in programs:
            program_id = program.get('program_id')
            if program_id and program_id not in seen_ids:
                seen_ids.add(program_id)
                unique_programs.append(program)
        
        logger.info(f"ğŸ”„ ì¤‘ë³µ ì œê±°: {len(programs)} â†’ {len(unique_programs)}")
        return unique_programs