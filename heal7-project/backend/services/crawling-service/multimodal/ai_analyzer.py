#!/usr/bin/env python3
"""
ğŸ¤– ë©€í‹°ëª¨ë‹¬ AI ë¶„ì„ê¸°
Gemini Flash, GPT-4o, Claude Sonnet í†µí•© ì´ë¯¸ì§€/ë¬¸ì„œ ë¶„ì„

Features:
- ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ëª¨ë¸ ì„ íƒ
- ìë™ í´ë°± ì‹œìŠ¤í…œ
- ì´ë¯¸ì§€ OCR ë° ë¶„ì„
- í…Œì´ë¸” ì¶”ì¶œ
- ë¬¸ì„œ ìš”ì•½

Author: HEAL7 Development Team
Version: 1.0.0
Date: 2025-08-30
"""

import os
import asyncio
import base64
import json
import logging
from typing import Dict, Any, Optional, List, Union, BinaryIO
from dataclasses import dataclass
from enum import Enum
from PIL import Image
import io

# AI SDK imports (with fallback)
try:
    import google.generativeai as genai
except ImportError:
    genai = None

try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

try:
    from anthropic import Anthropic
except ImportError:
    Anthropic = None


logger = logging.getLogger(__name__)


class AIModel(Enum):
    """AI ëª¨ë¸ íƒ€ì…"""
    GEMINI_FLASH = "gemini_flash"
    GPT4O = "gpt4o"  
    CLAUDE_SONNET = "claude_sonnet"


@dataclass
class AIModelConfig:
    """AI ëª¨ë¸ ì„¤ì •"""
    name: str
    provider: str
    api_key: Optional[str] = None
    max_tokens: int = 4000
    temperature: float = 0.1
    rate_limit_per_minute: int = 60
    max_image_size_mb: int = 20
    priority: int = 1
    enabled: bool = True


class MultimodalAnalyzer:
    """ğŸ¤– ë©€í‹°ëª¨ë‹¬ AI ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.MultimodalAnalyzer")
        
        # API í‚¤ ë¡œë“œ
        self._load_api_keys()
        
        # ëª¨ë¸ ì„¤ì •
        self.models = self._initialize_models()
        
        # AI í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
        self.clients = {}
        
        # í†µê³„
        self.usage_stats = {model.value: {'requests': 0, 'successes': 0, 'failures': 0} 
                          for model in AIModel}
    
    def _load_api_keys(self):
        """í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ"""
        self.api_keys = {
            'gemini': os.getenv('GEMINI_API_KEY') or os.getenv('GOOGLE_API_KEY'),
            'openai': os.getenv('OPENAI_API_KEY'),
            'anthropic': os.getenv('ANTHROPIC_API_KEY')
        }
        
        # API í‚¤ ìƒíƒœ ë¡œê¹…
        available_keys = [k for k, v in self.api_keys.items() if v and v != 'your-api-key']
        self.logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤: {available_keys}")
    
    def _initialize_models(self) -> Dict[AIModel, AIModelConfig]:
        """ëª¨ë¸ ì„¤ì • ì´ˆê¸°í™”"""
        models = {
            AIModel.GEMINI_FLASH: AIModelConfig(
                name="gemini-1.5-flash",
                provider="google",
                api_key=self.api_keys['gemini'],
                max_tokens=8000,
                rate_limit_per_minute=60,
                max_image_size_mb=20,
                priority=1,  # ìµœìš°ì„  (ë¬´ë£Œ)
                enabled=bool(self.api_keys['gemini'])
            ),
            
            AIModel.GPT4O: AIModelConfig(
                name="gpt-4o",
                provider="openai",
                api_key=self.api_keys['openai'], 
                max_tokens=4000,
                rate_limit_per_minute=20,
                max_image_size_mb=20,
                priority=2,  # 2ìˆœìœ„
                enabled=bool(self.api_keys['openai'])
            ),
            
            AIModel.CLAUDE_SONNET: AIModelConfig(
                name="claude-3-5-sonnet-20241022",
                provider="anthropic",
                api_key=self.api_keys['anthropic'],
                max_tokens=4000,
                rate_limit_per_minute=30,
                max_image_size_mb=5,
                priority=3,  # 3ìˆœìœ„
                enabled=bool(self.api_keys['anthropic'])
            )
        }
        
        # í™œì„±í™”ëœ ëª¨ë¸ë§Œ ë°˜í™˜
        return {k: v for k, v in models.items() if v.enabled}
    
    async def initialize(self):
        """AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        self.logger.info("ğŸ¤– ë©€í‹°ëª¨ë‹¬ AI ë¶„ì„ê¸° ì´ˆê¸°í™”")
        
        for model_type, config in self.models.items():
            try:
                if config.provider == "google" and genai:
                    genai.configure(api_key=config.api_key)
                    self.clients[model_type] = genai.GenerativeModel(config.name)
                    
                elif config.provider == "openai" and OpenAI:
                    self.clients[model_type] = OpenAI(api_key=config.api_key)
                    
                elif config.provider == "anthropic" and Anthropic:
                    self.clients[model_type] = Anthropic(api_key=config.api_key)
                
                self.logger.info(f"âœ… {config.name} í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
                    
            except Exception as e:
                self.logger.error(f"âŒ {config.name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                config.enabled = False
        
        enabled_models = [config.name for config in self.models.values() if config.enabled]
        self.logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {enabled_models}")
    
    async def analyze_image(
        self, 
        image_data: Union[bytes, str], 
        prompt: str,
        preferred_model: Optional[AIModel] = None
    ) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ ë¶„ì„"""
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        if isinstance(image_data, str):
            # íŒŒì¼ ê²½ë¡œì¸ ê²½ìš°
            with open(image_data, 'rb') as f:
                image_bytes = f.read()
        else:
            image_bytes = image_data
        
        # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸ ë° ìµœì í™”
        image_bytes = await self._optimize_image(image_bytes)
        
        # ëª¨ë¸ ì„ íƒ ë° ì‹œë„
        models_to_try = self._get_model_priority_order(preferred_model)
        
        for model_type in models_to_try:
            try:
                self.logger.info(f"ğŸ¯ ì´ë¯¸ì§€ ë¶„ì„ ì‹œë„: {model_type.value}")
                
                result = await self._analyze_with_model(model_type, image_bytes, prompt, "image")
                
                if result['success']:
                    self._update_stats(model_type, True)
                    return result
                    
            except Exception as e:
                self.logger.warning(f"âŒ {model_type.value} ì‹¤íŒ¨: {e}")
                self._update_stats(model_type, False)
                continue
        
        return {
            'success': False,
            'error': 'ëª¨ë“  AI ëª¨ë¸ ì‹¤íŒ¨',
            'content': ''
        }
    
    async def extract_table_from_image(self, image_data: Union[bytes, str]) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ì—ì„œ í…Œì´ë¸” ì¶”ì¶œ"""
        
        prompt = """
        ì´ ì´ë¯¸ì§€ì—ì„œ í…Œì´ë¸”ì„ ì°¾ì•„ êµ¬ì¡°í™”ëœ JSONìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.
        
        ìš”êµ¬ì‚¬í•­:
        1. ëª¨ë“  í…Œì´ë¸”ì„ ì°¾ì•„ ë¶„ì„
        2. í—¤ë”ì™€ ë°ì´í„° í–‰ì„ êµ¬ë¶„
        3. ë¹ˆ ì…€ì€ nullë¡œ í‘œì‹œ
        4. ìˆ«ìëŠ” ìˆ«ìí˜•ìœ¼ë¡œ, í…ìŠ¤íŠ¸ëŠ” ë¬¸ìì—´ë¡œ
        
        ì¶œë ¥ í˜•ì‹:
        {
            "tables": [
                {
                    "table_index": 1,
                    "headers": ["ì»¬ëŸ¼1", "ì»¬ëŸ¼2", "ì»¬ëŸ¼3"],
                    "rows": [
                        ["ê°’1", "ê°’2", "ê°’3"],
                        ["ê°’4", "ê°’5", "ê°’6"]
                    ]
                }
            ],
            "summary": "í…Œì´ë¸” ìš”ì•½ ì„¤ëª…"
        }
        """
        
        result = await self.analyze_image(image_data, prompt, AIModel.CLAUDE_SONNET)
        
        if result['success']:
            try:
                # JSON íŒŒì‹± ì‹œë„
                content = result['content']
                if '```json' in content:
                    content = content.split('```json')[1].split('```')[0]
                
                parsed_data = json.loads(content)
                result['parsed_tables'] = parsed_data
                
            except json.JSONDecodeError:
                result['parsed_tables'] = None
                result['parse_error'] = 'JSON íŒŒì‹± ì‹¤íŒ¨'
        
        return result
    
    async def analyze_webpage_screenshot(self, image_data: Union[bytes, str]) -> Dict[str, Any]:
        """ì›¹í˜ì´ì§€ ìŠ¤í¬ë¦°ìƒ· ë¶„ì„"""
        
        prompt = """
        ì´ ì›¹í˜ì´ì§€ ìŠ¤í¬ë¦°ìƒ·ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:
        
        1. ì£¼ìš” ì½˜í…ì¸ :
           - ì œëª©ê³¼ í—¤ë”© í…ìŠ¤íŠ¸
           - ì¤‘ìš”í•œ ë°ì´í„°ë‚˜ ìˆ˜ì¹˜
           - ëª©ë¡ì´ë‚˜ í…Œì´ë¸”ì˜ í•­ëª©ë“¤
        
        2. ì¸í„°í˜ì´ìŠ¤ ìš”ì†Œ:
           - ë²„íŠ¼ í…ìŠ¤íŠ¸
           - ë§í¬ í…ìŠ¤íŠ¸  
           - í¼ í•„ë“œ ë ˆì´ë¸”
           - ë©”ë‰´ í•­ëª©
        
        3. êµ¬ì¡°ì  ì •ë³´:
           - í˜ì´ì§€ ë ˆì´ì•„ì›ƒ ì„¤ëª…
           - ì£¼ìš” ì„¹ì…˜ êµ¬ë¶„
           - ë„¤ë¹„ê²Œì´ì…˜ êµ¬ì¡°
        
        í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ê³ , ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì´ê³  ì •í™•í•˜ê²Œ ì¶”ì¶œí•˜ì„¸ìš”.
        """
        
        return await self.analyze_image(image_data, prompt, AIModel.GEMINI_FLASH)
    
    async def ocr_extract_text(self, image_data: Union[bytes, str]) -> Dict[str, Any]:
        """OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        
        prompt = """
        ì´ ì´ë¯¸ì§€ì—ì„œ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì •í™•í•˜ê²Œ ì¶”ì¶œí•˜ì„¸ìš”.
        
        ìš”êµ¬ì‚¬í•­:
        1. ì½ì„ ìˆ˜ ìˆëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ
        2. ì›ë³¸ì˜ ë ˆì´ì•„ì›ƒê³¼ ìˆœì„œë¥¼ ìµœëŒ€í•œ ìœ ì§€
        3. í‘œë‚˜ ëª©ë¡ì˜ êµ¬ì¡°ë„ ë³´ì¡´
        4. íë¦¿í•˜ê±°ë‚˜ ì‘ì€ í…ìŠ¤íŠ¸ë„ ìµœëŒ€í•œ ì¶”ì¶œ
        
        ì¶œë ¥ì€ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œë§Œ í•˜ë˜, êµ¬ì¡°ì  ì •ë³´(ì œëª©, ëª©ë¡ ë“±)ëŠ” 
        ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ í‘œí˜„í•˜ì„¸ìš”.
        """
        
        return await self.analyze_image(image_data, prompt, AIModel.GEMINI_FLASH)
    
    async def _analyze_with_model(
        self, 
        model_type: AIModel, 
        image_bytes: bytes, 
        prompt: str,
        task_type: str
    ) -> Dict[str, Any]:
        """íŠ¹ì • ëª¨ë¸ë¡œ ë¶„ì„ ì‹¤í–‰"""
        
        config = self.models[model_type]
        client = self.clients[model_type]
        
        if config.provider == "google":
            return await self._analyze_with_gemini(client, image_bytes, prompt)
            
        elif config.provider == "openai":
            return await self._analyze_with_gpt4o(client, image_bytes, prompt)
            
        elif config.provider == "anthropic":
            return await self._analyze_with_claude(client, image_bytes, prompt)
        
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì œê³µì: {config.provider}")
    
    async def _analyze_with_gemini(self, client, image_bytes: bytes, prompt: str) -> Dict[str, Any]:
        """Gemini Flashë¡œ ë¶„ì„"""
        try:
            # PIL Image ìƒì„±
            image = Image.open(io.BytesIO(image_bytes))
            
            # Geminiì— ì „ì†¡
            response = await asyncio.get_event_loop().run_in_executor(
                None, 
                lambda: client.generate_content([prompt, image])
            )
            
            return {
                'success': True,
                'content': response.text,
                'model_used': 'gemini-1.5-flash'
            }
            
        except Exception as e:
            raise Exception(f"Gemini ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    async def _analyze_with_gpt4o(self, client, image_bytes: bytes, prompt: str) -> Dict[str, Any]:
        """GPT-4oë¡œ ë¶„ì„"""
        try:
            # Base64 ì¸ì½”ë”©
            image_base64 = base64.b64encode(image_bytes).decode('utf-8')
            
            response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {"type": "text", "text": prompt},
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/jpeg;base64,{image_base64}"
                                    }
                                }
                            ]
                        }
                    ],
                    max_tokens=4000
                )
            )
            
            return {
                'success': True,
                'content': response.choices[0].message.content,
                'model_used': 'gpt-4o'
            }
            
        except Exception as e:
            raise Exception(f"GPT-4o ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    async def _analyze_with_claude(self, client, image_bytes: bytes, prompt: str) -> Dict[str, Any]:
        """Claude Sonnetìœ¼ë¡œ ë¶„ì„"""
        try:
            # Base64 ì¸ì½”ë”©
            image_base64 = base64.b64encode(image_bytes).decode('utf-8')
            
            response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: client.messages.create(
                    model="claude-3-5-sonnet-20241022",
                    max_tokens=4000,
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "image",
                                    "source": {
                                        "type": "base64",
                                        "media_type": "image/jpeg",
                                        "data": image_base64
                                    }
                                },
                                {
                                    "type": "text", 
                                    "text": prompt
                                }
                            ]
                        }
                    ]
                )
            )
            
            return {
                'success': True,
                'content': response.content[0].text,
                'model_used': 'claude-3-5-sonnet'
            }
            
        except Exception as e:
            raise Exception(f"Claude ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    async def _optimize_image(self, image_bytes: bytes) -> bytes:
        """ì´ë¯¸ì§€ ìµœì í™”"""
        try:
            # PILë¡œ ì´ë¯¸ì§€ ì—´ê¸°
            image = Image.open(io.BytesIO(image_bytes))
            
            # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸
            if len(image_bytes) > 10 * 1024 * 1024:  # 10MB ì´ìƒ
                # í¬ê¸° ì¡°ì •
                max_dimension = 2048
                image.thumbnail((max_dimension, max_dimension), Image.Resampling.LANCZOS)
                
                # JPEGë¡œ ì••ì¶•
                output = io.BytesIO()
                image.save(output, format='JPEG', quality=85, optimize=True)
                return output.getvalue()
            
            return image_bytes
            
        except Exception as e:
            self.logger.warning(f"ì´ë¯¸ì§€ ìµœì í™” ì‹¤íŒ¨: {e}")
            return image_bytes
    
    def _get_model_priority_order(self, preferred_model: Optional[AIModel] = None) -> List[AIModel]:
        """ëª¨ë¸ ìš°ì„ ìˆœìœ„ ìˆœì„œ ê²°ì •"""
        available_models = [model for model in self.models.keys() if self.models[model].enabled]
        
        if preferred_model and preferred_model in available_models:
            # ì„ í˜¸ ëª¨ë¸ì„ ì²« ë²ˆì§¸ë¡œ
            other_models = [m for m in available_models if m != preferred_model]
            other_models.sort(key=lambda m: self.models[m].priority)
            return [preferred_model] + other_models
        else:
            # ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ ì •ë ¬
            available_models.sort(key=lambda m: self.models[m].priority)
            return available_models
    
    def _update_stats(self, model_type: AIModel, success: bool):
        """ì‚¬ìš© í†µê³„ ì—…ë°ì´íŠ¸"""
        stats = self.usage_stats[model_type.value]
        stats['requests'] += 1
        
        if success:
            stats['successes'] += 1
        else:
            stats['failures'] += 1
    
    def get_usage_stats(self) -> Dict[str, Any]:
        """ì‚¬ìš© í†µê³„ ë°˜í™˜"""
        return {
            'model_stats': self.usage_stats,
            'available_models': [config.name for config in self.models.values() if config.enabled],
            'total_requests': sum(stats['requests'] for stats in self.usage_stats.values())
        }


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤

async def quick_image_analysis(image_path: str, prompt: str) -> str:
    """ë¹ ë¥¸ ì´ë¯¸ì§€ ë¶„ì„"""
    analyzer = MultimodalAnalyzer()
    await analyzer.initialize()
    
    result = await analyzer.analyze_image(image_path, prompt)
    
    if result['success']:
        return result['content']
    else:
        return f"ë¶„ì„ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"


async def extract_table_data(image_path: str) -> Dict:
    """í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ"""
    analyzer = MultimodalAnalyzer()
    await analyzer.initialize()
    
    return await analyzer.extract_table_from_image(image_path)