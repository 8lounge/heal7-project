#!/usr/bin/env python3
"""
ğŸ§° í¬ë¡¤ëŸ¬ íˆ´í‚· - 1+1 í´ë°± ì²´ê³„
ì ì‹œì ì†Œ í¬ë¡¤ëŸ¬ ì„ íƒ ë° ìë™ í´ë°± ì‹œìŠ¤í…œ

Features:
- ë„ë©”ì¸ë³„ ìµœì  í¬ë¡¤ëŸ¬ ì¡°í•© ë§¤í•‘
- ì‹¤íŒ¨ì‹œ ì¦‰ì‹œ í´ë°± ì „í™˜
- ì„±ëŠ¥ ê¸°ë°˜ ë™ì  ì „ëµ ì¡°ì •
- AI ê¸°ë°˜ í˜ì´ì§€ ë¶„ì„ í†µí•©

Author: HEAL7 Development Team
Version: 1.0.0
Date: 2025-09-01
"""

import asyncio
import time
import logging
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, asdict
from enum import Enum
from urllib.parse import urlparse
import json
from pathlib import Path


logger = logging.getLogger(__name__)


class CrawlerType(Enum):
    """í¬ë¡¤ëŸ¬ íƒ€ì… (3ë‹¨ê³„ ê°„ì†Œí™” ì‹œìŠ¤í…œ)"""
    HTTPX = "httpx"           # Tier 1: ì •ì  ì½˜í…ì¸ , API 
    PLAYWRIGHT = "playwright" # Tier 2: JavaScript ë Œë”ë§, ë³µì¡í•œ ìƒí˜¸ì‘ìš©


class TaskType(Enum):
    """ì‘ì—… íƒ€ì…"""
    API_DATA = "api_data"
    TABLE_EXTRACTION = "table_extraction"
    FORM_INTERACTION = "form_interaction"
    PAGE_ANALYSIS = "page_analysis"
    GOVERNMENT_PORTAL = "government_portal"
    LEGACY_SITE = "legacy_site"
    DISCOVERY = "discovery"


@dataclass
class CrawlerStrategy:
    """í¬ë¡¤ëŸ¬ ì „ëµ"""
    primary: CrawlerType      # ì£¼ í¬ë¡¤ëŸ¬
    fallback: CrawlerType     # í´ë°± í¬ë¡¤ëŸ¬
    task_type: TaskType
    success_threshold: float  # ì„±ê³µë¥  ì„ê³„ê°’ (%)
    timeout_seconds: int
    max_retries: int
    description: str


@dataclass
class ExecutionResult:
    """ì‹¤í–‰ ê²°ê³¼"""
    success: bool
    crawler_used: CrawlerType
    execution_time: float
    data_count: int
    error_message: Optional[str] = None
    fallback_triggered: bool = False
    screenshot_path: Optional[str] = None
    performance_score: float = 0.0  # 0-100ì 


class CrawlerToolkit:
    """ğŸ§° í¬ë¡¤ëŸ¬ íˆ´í‚· - 1+1 í´ë°± ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.strategies = self._initialize_strategies()
        self.performance_history: Dict[str, Dict] = {}
        self.fallback_stats = {
            "total_executions": 0,
            "fallback_triggered": 0,
            "success_with_primary": 0,
            "success_with_fallback": 0,
            "complete_failures": 0
        }
        
        logger.info("ğŸ§° í¬ë¡¤ëŸ¬ íˆ´í‚· ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_strategies(self) -> Dict[TaskType, CrawlerStrategy]:
        """ì „ëµ ë§¤í•‘ ì´ˆê¸°í™”"""
        return {
            # ğŸš€ API ë°ì´í„° ìˆ˜ì§‘
            TaskType.API_DATA: CrawlerStrategy(
                primary=CrawlerType.HTTPX,
                fallback=CrawlerType.PLAYWRIGHT,
                task_type=TaskType.API_DATA,
                success_threshold=95.0,
                timeout_seconds=30,
                max_retries=3,
                description="REST APIë‚˜ JSON ì—”ë“œí¬ì¸íŠ¸ëŠ” HTTPXê°€ ìµœì , ì›¹UI í•„ìš”ì‹œ Playwrightë¡œ í´ë°±"
            ),
            
            # ğŸ“Š í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ
            TaskType.TABLE_EXTRACTION: CrawlerStrategy(
                primary=CrawlerType.PLAYWRIGHT,
                fallback=CrawlerType.HTTPX,  # 3ë‹¨ê³„ ê°„ì†Œí™”: Selenium ì œê±°
                task_type=TaskType.TABLE_EXTRACTION,
                success_threshold=85.0,
                timeout_seconds=60,
                max_retries=2,
                description="ë™ì  í…Œì´ë¸”ì€ Playwright, ê°„ë‹¨í•œ í…Œì´ë¸”ì€ HTTPXë¡œ í´ë°± (3ë‹¨ê³„ ê°„ì†Œí™”)"
            ),
            
            # ğŸ“ í¼ ìƒí˜¸ì‘ìš©
            TaskType.FORM_INTERACTION: CrawlerStrategy(
                primary=CrawlerType.PLAYWRIGHT,  # 3ë‹¨ê³„ ê°„ì†Œí™”: Selenium ì œê±°
                fallback=CrawlerType.PLAYWRIGHT,
                task_type=TaskType.FORM_INTERACTION,
                success_threshold=80.0,
                timeout_seconds=90,
                max_retries=2,
                description="ë³µì¤ í¼ ì…ë ¥/ì œì¶œì€ Playwrightê°€ ì•ˆì •ì  (3ë‹¨ê³„ ê°„ì†Œí™”)"
            ),
            
            # ğŸ“„ ì •ë¶€ í¬í„¸
            TaskType.GOVERNMENT_PORTAL: CrawlerStrategy(
                primary=CrawlerType.PLAYWRIGHT,
                fallback=CrawlerType.HTTPX,  # 3ë‹¨ê³„ ê°„ì†Œí™”: Selenium ì œê±°
                task_type=TaskType.GOVERNMENT_PORTAL,
                success_threshold=90.0,
                timeout_seconds=120,
                max_retries=2,
                description="ì •ë¶€ ì‚¬ì´íŠ¸ëŠ” JavaScript í•„ìˆ˜, ë‹¨ìˆœ ë°ì´í„°ëŠ” HTTPXë¡œ í´ë°± (3ë‹¨ê³„ ê°„ì†Œí™”)"
            ),
            
            # ğŸ” í˜ì´ì§€ ë¶„ì„ (AI ì§€ì›)
            TaskType.PAGE_ANALYSIS: CrawlerStrategy(
                primary=CrawlerType.PLAYWRIGHT,
                fallback=CrawlerType.HTTPX,  # 3ë‹¨ê³„ ê°„ì†Œí™”: Selenium ì œê±°
                task_type=TaskType.PAGE_ANALYSIS,
                success_threshold=75.0,
                timeout_seconds=90,
                max_retries=1,
                description="ìŠ¤í¬ë¦°ìƒ· + DOM ë¶„ì„ì€ Playwright, ê¸°ë³¸ ë¶„ì„ì€ HTTPX (3ë‹¨ê³„ ê°„ì†Œí™”)"
            ),
            
            # ğŸ›ï¸ ë ˆê±°ì‹œ ì‚¬ì´íŠ¸
            TaskType.LEGACY_SITE: CrawlerStrategy(
                primary=CrawlerType.PLAYWRIGHT,  # 3ë‹¨ê³„ ê°„ì†Œí™”: Selenium ì œê±°
                fallback=CrawlerType.HTTPX,
                task_type=TaskType.LEGACY_SITE,
                success_threshold=70.0,
                timeout_seconds=180,
                max_retries=1,
                description="ë ˆê±°ì‹œ ì‚¬ì´íŠ¸ë„ Playwrightë¡œ ì²˜ë¦¬, ë‹¨ìˆœ HTMLë§Œ ìˆë‹¤ë©´ HTTPXë¡œ í´ë°± (3ë‹¨ê³„ ê°„ì†Œí™”)"
            ),
            
            # ğŸ” ë””ìŠ¤ì»¤ë²„ë¦¬ í¬ë¡¤ë§
            TaskType.DISCOVERY: CrawlerStrategy(
                primary=CrawlerType.PLAYWRIGHT,
                fallback=CrawlerType.HTTPX,  # 3ë‹¨ê³„ ê°„ì†Œí™”: Selenium ì œê±°
                task_type=TaskType.DISCOVERY,
                success_threshold=60.0,
                timeout_seconds=150,
                max_retries=1,
                description="ìë™ ìš”ì†Œ íƒì§€ëŠ” Playwright, ê¸°ë³¸ ìš”ì†ŒëŠ” HTTPXë¡œ ë¶„ì„ (3ë‹¨ê³„ ê°„ì†Œí™”)"
            )
        }
    
    def analyze_url_and_task(self, url: str, task_hint: str = None) -> TaskType:
        """URLê³¼ ì‘ì—… íŒíŠ¸ ê¸°ë°˜ ì‘ì—… íƒ€ì… ê²°ì •"""
        
        domain = urlparse(url).netloc.lower()
        path = urlparse(url).path.lower()
        
        # ë„ë©”ì¸ ê¸°ë°˜ ë¶„ì„
        if any(gov in domain for gov in ['gov.kr', 'go.kr', 'korea.kr']):
            return TaskType.GOVERNMENT_PORTAL
        elif 'api.' in domain or 'httpbin' in domain or path.startswith('/api/'):
            return TaskType.API_DATA
        elif any(legacy in domain for legacy in ['.co.kr', '.or.kr']) and '2000' in domain:
            return TaskType.LEGACY_SITE
        
        # ì‘ì—… íŒíŠ¸ ê¸°ë°˜ ë¶„ì„
        if task_hint:
            hint_lower = task_hint.lower()
            if 'table' in hint_lower or 'extract' in hint_lower:
                return TaskType.TABLE_EXTRACTION
            elif 'form' in hint_lower or 'submit' in hint_lower or 'login' in hint_lower:
                return TaskType.FORM_INTERACTION
            elif 'discover' in hint_lower or 'explore' in hint_lower:
                return TaskType.DISCOVERY
        
        # ê¸°ë³¸ê°’: í˜ì´ì§€ ë¶„ì„
        return TaskType.PAGE_ANALYSIS
    
    async def execute_with_fallback(
        self, 
        url: str, 
        task_hint: str = None,
        custom_strategy: CrawlerStrategy = None
    ) -> ExecutionResult:
        """ğŸ¯ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - 1+1 í´ë°± ì²´ê³„"""
        
        # ì „ëµ ì„ íƒ
        if custom_strategy:
            strategy = custom_strategy
        else:
            task_type = self.analyze_url_and_task(url, task_hint)
            strategy = self.strategies[task_type]
        
        self.fallback_stats["total_executions"] += 1
        
        logger.info(f"ğŸ¯ ì‘ì—… ì‹¤í–‰: {url}")
        logger.info(f"   ì „ëµ: {strategy.task_type.value}")
        logger.info(f"   ì£¼ í¬ë¡¤ëŸ¬: {strategy.primary.value} â†’ í´ë°±: {strategy.fallback.value}")
        
        # 1ë‹¨ê³„: ì£¼ í¬ë¡¤ëŸ¬ ì‹œë„
        primary_result = await self._execute_single_crawler(
            url, strategy.primary, strategy
        )
        
        if primary_result.success and primary_result.performance_score >= strategy.success_threshold:
            logger.info(f"âœ… ì£¼ í¬ë¡¤ëŸ¬({strategy.primary.value}) ì„±ê³µ!")
            self.fallback_stats["success_with_primary"] += 1
            self._update_performance_history(url, primary_result)
            return primary_result
        
        # 2ë‹¨ê³„: í´ë°± í¬ë¡¤ëŸ¬ ì‹œë„
        logger.warning(f"âš ï¸ ì£¼ í¬ë¡¤ëŸ¬ ì‹¤íŒ¨/ì„±ëŠ¥ë¶€ì¡±, í´ë°±({strategy.fallback.value}) ì‹œë„")
        self.fallback_stats["fallback_triggered"] += 1
        
        fallback_result = await self._execute_single_crawler(
            url, strategy.fallback, strategy
        )
        fallback_result.fallback_triggered = True
        
        if fallback_result.success:
            logger.info(f"âœ… í´ë°± í¬ë¡¤ëŸ¬({strategy.fallback.value}) ì„±ê³µ!")
            self.fallback_stats["success_with_fallback"] += 1
        else:
            logger.error(f"âŒ ëª¨ë“  í¬ë¡¤ëŸ¬ ì‹¤íŒ¨!")
            self.fallback_stats["complete_failures"] += 1
        
        self._update_performance_history(url, fallback_result)
        return fallback_result
    
    async def _execute_single_crawler(
        self, 
        url: str, 
        crawler_type: CrawlerType, 
        strategy: CrawlerStrategy
    ) -> ExecutionResult:
        """ê°œë³„ í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        
        start_time = time.time()
        
        try:
            # TODO: ì‹¤ì œ í¬ë¡¤ëŸ¬ ê°ì²´ë“¤ê³¼ ì—°ë™
            # í˜„ì¬ëŠ” ì‹œë®¬ë ˆì´ì…˜
            
            # í¬ë¡¤ëŸ¬ë³„ ì„±ê³µë¥  ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì œê±°)
            if crawler_type == CrawlerType.HTTPX:
                success_prob = 0.95 if 'api' in url or 'httpbin' in url else 0.60
                data_count = random.randint(1, 50) if random.random() < success_prob else 0
            elif crawler_type == CrawlerType.PLAYWRIGHT:
                success_prob = 0.85 if 'gov' in url or 'dynamic' in url else 0.75
                data_count = random.randint(5, 100) if random.random() < success_prob else 0
            else:  # SELENIUM
                success_prob = 0.90 if 'form' in url or 'legacy' in url else 0.70
                data_count = random.randint(3, 80) if random.random() < success_prob else 0
            
            # ì‹¤í–‰ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
            if crawler_type == CrawlerType.HTTPX:
                await asyncio.sleep(random.uniform(0.5, 2.0))
            elif crawler_type == CrawlerType.PLAYWRIGHT:
                await asyncio.sleep(random.uniform(2.0, 5.0))
            else:  # SELENIUM
                await asyncio.sleep(random.uniform(5.0, 10.0))
            
            execution_time = time.time() - start_time
            success = data_count > 0
            
            # ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
            performance_score = 0
            if success:
                time_score = max(0, 100 - (execution_time * 2))  # ì‹œê°„ ê°€ì 
                data_score = min(data_count * 2, 50)  # ë°ì´í„° ì–‘ ê°€ì   
                performance_score = min(time_score + data_score, 100)
            
            return ExecutionResult(
                success=success,
                crawler_used=crawler_type,
                execution_time=execution_time,
                data_count=data_count,
                performance_score=performance_score,
                error_message=None if success else f"{crawler_type.value} í¬ë¡¤ëŸ¬ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨"
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"âŒ {crawler_type.value} í¬ë¡¤ëŸ¬ ì˜¤ë¥˜: {e}")
            
            return ExecutionResult(
                success=False,
                crawler_used=crawler_type,
                execution_time=execution_time,
                data_count=0,
                error_message=str(e),
                performance_score=0
            )
    
    def _update_performance_history(self, url: str, result: ExecutionResult):
        """ì„±ëŠ¥ ì´ë ¥ ì—…ë°ì´íŠ¸"""
        domain = urlparse(url).netloc
        
        if domain not in self.performance_history:
            self.performance_history[domain] = {
                "total_runs": 0,
                "successful_runs": 0,
                "avg_execution_time": 0,
                "avg_performance_score": 0,
                "crawler_stats": {
                    "httpx": {"runs": 0, "successes": 0, "avg_time": 0},
                    "playwright": {"runs": 0, "successes": 0, "avg_time": 0}
                },
                "last_updated": None
            }
        
        history = self.performance_history[domain]
        crawler_name = result.crawler_used.value
        
        # ì „ì²´ í†µê³„ ì—…ë°ì´íŠ¸
        history["total_runs"] += 1
        if result.success:
            history["successful_runs"] += 1
        
        # í‰ê·  ì‹¤í–‰ì‹œê°„ ì—…ë°ì´íŠ¸
        prev_time = history["avg_execution_time"]
        history["avg_execution_time"] = (prev_time * (history["total_runs"] - 1) + result.execution_time) / history["total_runs"]
        
        # í‰ê·  ì„±ëŠ¥ì ìˆ˜ ì—…ë°ì´íŠ¸
        prev_score = history["avg_performance_score"]
        history["avg_performance_score"] = (prev_score * (history["total_runs"] - 1) + result.performance_score) / history["total_runs"]
        
        # í¬ë¡¤ëŸ¬ë³„ í†µê³„ ì—…ë°ì´íŠ¸
        crawler_stats = history["crawler_stats"][crawler_name]
        crawler_stats["runs"] += 1
        if result.success:
            crawler_stats["successes"] += 1
        
        prev_crawler_time = crawler_stats["avg_time"]
        crawler_stats["avg_time"] = (prev_crawler_time * (crawler_stats["runs"] - 1) + result.execution_time) / crawler_stats["runs"]
        
        history["last_updated"] = time.time()
    
    def get_domain_recommendation(self, url: str) -> Dict[str, Any]:
        """ë„ë©”ì¸ë³„ ì¶”ì²œ ì „ëµ"""
        domain = urlparse(url).netloc
        
        if domain not in self.performance_history:
            # ì²« ë°©ë¬¸: URL ë¶„ì„ ê¸°ë°˜ ì¶”ì²œ
            task_type = self.analyze_url_and_task(url)
            strategy = self.strategies[task_type]
            
            return {
                "domain": domain,
                "first_visit": True,
                "recommended_strategy": {
                    "primary": strategy.primary.value,
                    "fallback": strategy.fallback.value,
                    "reason": strategy.description
                },
                "confidence": "medium"
            }
        
        # ì´ë ¥ ê¸°ë°˜ ì¶”ì²œ
        history = self.performance_history[domain]
        crawler_stats = history["crawler_stats"]
        
        # ìµœê³  ì„±ëŠ¥ í¬ë¡¤ëŸ¬ ì°¾ê¸°
        best_crawler = None
        best_success_rate = 0
        
        for crawler_name, stats in crawler_stats.items():
            if stats["runs"] > 0:
                success_rate = stats["successes"] / stats["runs"]
                if success_rate > best_success_rate:
                    best_success_rate = success_rate
                    best_crawler = crawler_name
        
        return {
            "domain": domain,
            "first_visit": False,
            "total_runs": history["total_runs"],
            "success_rate": history["successful_runs"] / history["total_runs"],
            "avg_execution_time": history["avg_execution_time"],
            "best_crawler": best_crawler,
            "best_success_rate": best_success_rate,
            "recommended_strategy": self._get_optimized_strategy(domain, history),
            "confidence": "high" if history["total_runs"] >= 10 else "medium"
        }
    
    def _get_optimized_strategy(self, domain: str, history: Dict) -> Dict[str, str]:
        """ì„±ëŠ¥ ì´ë ¥ ê¸°ë°˜ ìµœì í™”ëœ ì „ëµ"""
        crawler_stats = history["crawler_stats"]
        
        # ì„±ê³µë¥  ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        sorted_crawlers = sorted(
            crawler_stats.items(),
            key=lambda x: (x[1]["successes"] / max(x[1]["runs"], 1), -x[1]["avg_time"]),
            reverse=True
        )
        
        if len(sorted_crawlers) >= 2:
            primary = sorted_crawlers[0][0]
            fallback = sorted_crawlers[1][0]
        else:
            # ê¸°ë³¸ ì „ëµ ìœ ì§€
            return {"primary": "playwright", "fallback": "httpx", "reason": "ì„±ëŠ¥ ë°ì´í„° ë¶€ì¡± (3ë‹¨ê³„ ê°„ì†Œí™” ì‹œìŠ¤í…œ)"}
        
        return {
            "primary": primary,
            "fallback": fallback, 
            "reason": f"ì„±ëŠ¥ ì´ë ¥ ê¸°ë°˜ ìµœì í™” ({history['total_runs']}íšŒ ì‹¤í–‰ ë¶„ì„)"
        }
    
    def get_toolkit_stats(self) -> Dict[str, Any]:
        """íˆ´í‚· ì „ì²´ í†µê³„"""
        return {
            "strategies_count": len(self.strategies),
            "tracked_domains": len(self.performance_history),
            "execution_stats": self.fallback_stats.copy(),
            "fallback_success_rate": (
                self.fallback_stats["success_with_fallback"] / 
                max(self.fallback_stats["fallback_triggered"], 1)
            ) * 100,
            "overall_success_rate": (
                (self.fallback_stats["success_with_primary"] + self.fallback_stats["success_with_fallback"]) /
                max(self.fallback_stats["total_executions"], 1)
            ) * 100
        }
    
    async def batch_analyze_urls(self, urls: List[str]) -> Dict[str, Dict]:
        """ğŸ” ë°°ì¹˜ URL ë¶„ì„"""
        results = {}
        
        for url in urls:
            try:
                recommendation = self.get_domain_recommendation(url)
                task_type = self.analyze_url_and_task(url)
                
                results[url] = {
                    "task_type": task_type.value,
                    "recommendation": recommendation,
                    "estimated_complexity": self._estimate_complexity(url, task_type)
                }
                
            except Exception as e:
                results[url] = {
                    "error": str(e),
                    "task_type": "unknown"
                }
        
        return results
    
    def _estimate_complexity(self, url: str, task_type: TaskType) -> Dict[str, Any]:
        """ë³µì¡ë„ ì¶”ì •"""
        complexity_scores = {
            TaskType.API_DATA: {"score": 1, "level": "ë‚®ìŒ"},
            TaskType.GOVERNMENT_PORTAL: {"score": 4, "level": "ë†’ìŒ"},
            TaskType.TABLE_EXTRACTION: {"score": 3, "level": "ì¤‘ê°„"},
            TaskType.FORM_INTERACTION: {"score": 4, "level": "ë†’ìŒ"},
            TaskType.PAGE_ANALYSIS: {"score": 2, "level": "ë‚®ìŒ-ì¤‘ê°„"},
            TaskType.LEGACY_SITE: {"score": 5, "level": "ë§¤ìš° ë†’ìŒ"},
            TaskType.DISCOVERY: {"score": 3, "level": "ì¤‘ê°„"}
        }
        
        base_complexity = complexity_scores.get(task_type, {"score": 3, "level": "ì¤‘ê°„"})
        
        # URL íŠ¹ì„± ê¸°ë°˜ ì¡°ì •
        domain = urlparse(url).netloc.lower()
        if 'gov.kr' in domain:
            base_complexity["score"] = min(5, base_complexity["score"] + 1)
        elif 'api.' in domain:
            base_complexity["score"] = max(1, base_complexity["score"] - 1)
        
        return base_complexity


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
crawler_toolkit = CrawlerToolkit()


# ğŸ¯ í¬ë¡¤ëŸ¬ ì¡°í•© ê°€ì´ë“œ
CRAWLER_COMBINATION_GUIDE = {
    "ğŸš€ API/ì •ì ë°ì´í„°": {
        "primary": "HTTPX",
        "fallback": "Playwright", 
        "use_cases": ["REST API", "JSON ì—”ë“œí¬ì¸íŠ¸", "ì •ì  HTML", "CSV ë‹¤ìš´ë¡œë“œ"],
        "success_rate": "95%+",
        "avg_time": "1-3ì´ˆ"
    },
    
    "ğŸ“Š í…Œì´ë¸”/ë™ì ë°ì´í„°": {
        "primary": "Playwright",
        "fallback": "HTTPX",  # 3ë‹¨ê³„ ê°„ì†Œí™”
        "use_cases": ["JavaScript í…Œì´ë¸”", "AJAX ë¡œë”©", "ë¬´í•œìŠ¤í¬ë¡¤", "ì°¨íŠ¸ ë°ì´í„°"],
        "success_rate": "85%+", 
        "avg_time": "3-8ì´ˆ"
    },
    
    "ğŸ“ í¼/ìƒí˜¸ì‘ìš©": {
        "primary": "Playwright",  # 3ë‹¨ê³„ ê°„ì†Œí™” 
        "fallback": "Playwright",
        "use_cases": ["ë¡œê·¸ì¸ í¼", "ë‹¤ë‹¨ê³„ ì…ë ¥", "íŒŒì¼ ì—…ë¡œë“œ", "ë“œë¡­ë‹¤ìš´"],
        "success_rate": "80%+",
        "avg_time": "8-15ì´ˆ"
    },
    
    "ğŸ“„ ì •ë¶€í¬í„¸": {
        "primary": "Playwright",
        "fallback": "HTTPX",  # 3ë‹¨ê³„ ê°„ì†Œí™” 
        "use_cases": ["ì •ë¶€24", "ë‚˜ë¼ì¥í„°", "êµ­ê°€ì •ë³´í¬í„¸", "ê³µê³µAPI"],
        "success_rate": "90%+",
        "avg_time": "5-12ì´ˆ"
    },
    
    "ğŸ›ï¸ ë ˆê±°ì‹œì‚¬ì´íŠ¸": {
        "primary": "Playwright",  # 3ë‹¨ê³„ ê°„ì†Œí™”
        "fallback": "HTTPX",
        "use_cases": ["ActiveX ì‚¬ì´íŠ¸", "Flash ì½˜í…ì¸ ", "êµ¬í˜• JSP", "í”„ë ˆì„ì…‹"],
        "success_rate": "70%+",
        "avg_time": "10-30ì´ˆ" 
    },
    
    "ğŸ” ë””ìŠ¤ì»¤ë²„ë¦¬": {
        "primary": "Playwright",
        "fallback": "HTTPX",  # 3ë‹¨ê³„ ê°„ì†Œí™”
        "use_cases": ["ìš”ì†Œ íƒì§€", "êµ¬ì¡° ë¶„ì„", "ìŠ¤í¬ë¦°ìƒ·", "AI ë¶„ì„"],
        "success_rate": "75%+",
        "avg_time": "8-20ì´ˆ"
    }
}


async def demo_crawler_toolkit():
    """ğŸ§ª í¬ë¡¤ëŸ¬ íˆ´í‚· ë°ëª¨"""
    print("ğŸ§° í¬ë¡¤ëŸ¬ íˆ´í‚· ë°ëª¨ ì‹œì‘")
    
    test_urls = [
        "https://httpbin.org/json",  # API
        "https://www.bizinfo.go.kr",  # ì •ë¶€í¬í„¸  
        "https://example.com/table",  # í…Œì´ë¸”
        "https://old-site.co.kr"  # ë ˆê±°ì‹œ
    ]
    
    for url in test_urls:
        print(f"\nğŸ¯ í…ŒìŠ¤íŠ¸ URL: {url}")
        result = await crawler_toolkit.execute_with_fallback(url)
        
        print(f"   ê²°ê³¼: {'âœ… ì„±ê³µ' if result.success else 'âŒ ì‹¤íŒ¨'}")
        print(f"   í¬ë¡¤ëŸ¬: {result.crawler_used.value}")
        print(f"   ì‹¤í–‰ì‹œê°„: {result.execution_time:.2f}ì´ˆ")
        print(f"   ì„±ëŠ¥ì ìˆ˜: {result.performance_score:.1f}/100")
        if result.fallback_triggered:
            print(f"   ğŸ”„ í´ë°± ì‚¬ìš©ë¨")
    
    # ì „ì²´ í†µê³„
    stats = crawler_toolkit.get_toolkit_stats()
    print(f"\nğŸ“Š ì „ì²´ í†µê³„:")
    print(f"   ì „ì²´ ì„±ê³µë¥ : {stats['overall_success_rate']:.1f}%")
    print(f"   í´ë°± ì„±ê³µë¥ : {stats['fallback_success_rate']:.1f}%") 
    print(f"   ì¶”ì  ë„ë©”ì¸: {stats['tracked_domains']}ê°œ")


if __name__ == "__main__":
    asyncio.run(demo_crawler_toolkit())