#!/usr/bin/env python3
"""
ğŸ­ Playwright í¬ë¡¤ëŸ¬ (Tier 2) 
ë™ì  ì½˜í…ì¸ , JavaScript ë Œë”ë§, ìŠ¤í¬ë¦°ìƒ· ì „ë¬¸

Features:
- JavaScript ë Œë”ë§
- ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜
- í˜ì´ì§€ ìƒí˜¸ì‘ìš©
- ë„¤íŠ¸ì›Œí¬ ëª¨ë‹ˆí„°ë§
- ëª¨ë°”ì¼ ì—ë®¬ë ˆì´ì…˜

Author: HEAL7 Development Team
Version: 1.0.0
Date: 2025-08-30
"""

import asyncio
import time
import json
from typing import Optional, Dict, Any, List, Union
from pathlib import Path
from urllib.parse import urljoin

from playwright.async_api import async_playwright, Browser, BrowserContext, Page, Playwright
from playwright.async_api import TimeoutError as PlaywrightTimeoutError

from .base_crawler import (
    BaseCrawler, CrawlResult, CrawlConfig, CrawlerType,
    CrawlingError, TimeoutError, create_default_headers, is_valid_url
)


class PlaywrightCrawler(BaseCrawler):
    """ğŸ­ Playwright ê¸°ë°˜ ë™ì  í¬ë¡¤ëŸ¬"""
    
    def __init__(self, browser_type: str = "chromium", headless: bool = True):
        super().__init__("playwright")
        self.browser_type = browser_type  # chromium, firefox, webkit
        self.headless = headless
        
        # Playwright ì¸ìŠ¤í„´ìŠ¤
        self.playwright: Optional[Playwright] = None
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        
        # ì„¤ì •
        self.browser_args = [
            '--no-sandbox',
            '--disable-dev-shm-usage',
            '--disable-gpu',
            '--disable-extensions',
            '--disable-blink-features=AutomationControlled'
        ]
        
        self.viewport_size = {"width": 1920, "height": 1080}
        
    async def initialize(self):
        """Playwright ì´ˆê¸°í™”"""
        if self.is_initialized:
            return
            
        try:
            # Playwright ì‹œì‘
            self.playwright = await async_playwright().start()
            
            # ë¸Œë¼ìš°ì € ì„ íƒ
            if self.browser_type == "firefox":
                browser_launcher = self.playwright.firefox
            elif self.browser_type == "webkit":
                browser_launcher = self.playwright.webkit
            else:
                browser_launcher = self.playwright.chromium
            
            # ë¸Œë¼ìš°ì € ì‹œì‘
            self.browser = await browser_launcher.launch(
                headless=self.headless,
                args=self.browser_args
            )
            
            # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            await self._create_context()
            
            self.is_initialized = True
            self.logger.info(f"âœ… Playwright ({self.browser_type}) í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ Playwright ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            await self.cleanup()
            raise CrawlingError(f"Playwright ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    async def _create_context(self, mobile_device: str = None):
        """ë¸Œë¼ìš°ì € ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        context_options = {
            'viewport': self.viewport_size,
            'user_agent': (
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                '(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            ),
            'locale': 'ko-KR',
            'timezone_id': 'Asia/Seoul'
        }
        
        # ëª¨ë°”ì¼ ë””ë°”ì´ìŠ¤ ì—ë®¬ë ˆì´ì…˜
        if mobile_device:
            device = self.playwright.devices.get(mobile_device)
            if device:
                context_options.update(device)
        
        self.context = await self.browser.new_context(**context_options)
        
        # ì¼ë°˜ì ì¸ stealth ì„¤ì •
        await self.context.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined,
            });
        """)
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self.context:
                await self.context.close()
                self.context = None
            
            if self.browser:
                await self.browser.close()
                self.browser = None
            
            if self.playwright:
                await self.playwright.stop()
                self.playwright = None
            
            self.is_initialized = False
            self.logger.info("ğŸ›‘ Playwright í¬ë¡¤ëŸ¬ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ Playwright ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    async def crawl(self, config: CrawlConfig) -> CrawlResult:
        """ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜"""
        if not self.is_initialized:
            await self.initialize()
        
        start_time = time.time()
        page = None
        
        try:
            # URL ìœ íš¨ì„± ê²€ì‚¬
            if not is_valid_url(config.url):
                raise CrawlingError(f"ì˜ëª»ëœ URL: {config.url}")
            
            # ìƒˆ í˜ì´ì§€ ìƒì„±
            page = await self.context.new_page()
            
            # í˜ì´ì§€ ì„¤ì •
            await self._setup_page(page, config)
            
            # í˜ì´ì§€ ë¡œë“œ
            await self._navigate_to_page(page, config)
            
            # ì½˜í…ì¸  ëŒ€ê¸°
            await self._wait_for_content(page, config)
            
            # ë°ì´í„° ì¶”ì¶œ
            html = await page.content()
            screenshot = None
            
            if config.screenshot:
                screenshot = await self._take_screenshot(page, config)
            
            # ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
            metadata = await self._collect_metadata(page)
            
            result = CrawlResult(
                success=True,
                html=html,
                screenshot=screenshot,
                metadata=metadata,
                crawler_used=CrawlerType.PLAYWRIGHT,
                url=config.url,
                response_time=time.time() - start_time
            )
            
            self.update_stats(result)
            return result
            
        except PlaywrightTimeoutError:
            error_msg = f"í˜ì´ì§€ ë¡œë“œ íƒ€ì„ì•„ì›ƒ: {config.url}"
            self.logger.warning(error_msg)
            return CrawlResult(
                success=False,
                error=error_msg,
                error_type="timeout",
                crawler_used=CrawlerType.PLAYWRIGHT,
                url=config.url,
                response_time=time.time() - start_time
            )
            
        except Exception as e:
            error_msg = f"Playwright í¬ë¡¤ë§ ì‹¤íŒ¨: {e}"
            self.logger.error(error_msg)
            return CrawlResult(
                success=False,
                error=error_msg,
                error_type="playwright_error",
                crawler_used=CrawlerType.PLAYWRIGHT,
                url=config.url,
                response_time=time.time() - start_time
            )
            
        finally:
            if page:
                try:
                    await page.close()
                except:
                    pass
    
    async def _setup_page(self, page: Page, config: CrawlConfig):
        """í˜ì´ì§€ ì„¤ì •"""
        # ì‚¬ìš©ì ì—ì´ì „íŠ¸ ì„¤ì •
        if config.user_agent:
            await page.set_extra_http_headers({'User-Agent': config.user_agent})
        
        # ì¶”ê°€ í—¤ë” ì„¤ì •
        if config.headers:
            await page.set_extra_http_headers(config.headers)
        
        # íƒ€ì„ì•„ì›ƒ ì„¤ì •
        page.set_default_timeout(config.timeout * 1000)  # ms ë‹¨ìœ„
        
        # JavaScript ë¹„í™œì„±í™” (ì„ íƒì‚¬í•­)
        # await page.context.add_init_script("window.location.reload = () => {};")
    
    async def _navigate_to_page(self, page: Page, config: CrawlConfig):
        """í˜ì´ì§€ ë„¤ë¹„ê²Œì´ì…˜"""
        # í˜ì´ì§€ ë¡œë“œ
        response = await page.goto(
            config.url,
            wait_until='domcontentloaded',
            timeout=config.timeout * 1000
        )
        
        # ì‘ë‹µ ìƒíƒœ í™•ì¸
        if response:
            status_code = response.status
            if status_code >= 400:
                self.logger.warning(f"HTTP {status_code}: {config.url}")
        
        return response
    
    async def _wait_for_content(self, page: Page, config: CrawlConfig):
        """ì½˜í…ì¸  ë¡œë“œ ëŒ€ê¸°"""
        try:
            if config.wait_for_load:
                # ë„¤íŠ¸ì›Œí¬ ìœ íœ´ ìƒíƒœê¹Œì§€ ëŒ€ê¸°
                await page.wait_for_load_state('networkidle', timeout=15000)
            
            if config.wait_for_selector:
                # íŠ¹ì • ì„ íƒìê¹Œì§€ ëŒ€ê¸°
                await page.wait_for_selector(config.wait_for_selector, timeout=10000)
            
            # ì¶”ê°€ ëŒ€ê¸° (JavaScript ë Œë”ë§)
            await asyncio.sleep(1)
            
        except PlaywrightTimeoutError:
            # íƒ€ì„ì•„ì›ƒë˜ì–´ë„ ê³„ì† ì§„í–‰
            self.logger.debug("ì½˜í…ì¸  ë¡œë“œ ëŒ€ê¸° íƒ€ì„ì•„ì›ƒ - ê³„ì† ì§„í–‰")
    
    async def _take_screenshot(self, page: Page, config: CrawlConfig) -> bytes:
        """ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜"""
        try:
            screenshot_options = {}
            
            if config.screenshot_full_page:
                screenshot_options['full_page'] = True
            
            return await page.screenshot(**screenshot_options)
            
        except Exception as e:
            self.logger.error(f"ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜ ì‹¤íŒ¨: {e}")
            return None
    
    async def _collect_metadata(self, page: Page) -> Dict[str, Any]:
        """í˜ì´ì§€ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘"""
        try:
            # í˜ì´ì§€ ì •ë³´
            title = await page.title()
            url = page.url
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­
            performance = await page.evaluate("""
                () => {
                    const perfData = performance.getEntriesByType('navigation')[0];
                    return {
                        loadTime: perfData ? perfData.loadEventEnd - perfData.loadEventStart : 0,
                        domContentLoaded: perfData ? perfData.domContentLoadedEventEnd - perfData.domContentLoadedEventStart : 0,
                        responseTime: perfData ? perfData.responseEnd - perfData.responseStart : 0
                    };
                }
            """)
            
            # JavaScript ì—ëŸ¬ ìˆ˜ì§‘
            js_errors = []
            
            return {
                'title': title,
                'final_url': url,
                'performance': performance,
                'js_errors': js_errors,
                'viewport': self.viewport_size
            }
            
        except Exception as e:
            self.logger.error(f"ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {}
    
    async def can_handle(self, url: str) -> bool:
        """URL ì²˜ë¦¬ ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨"""
        if not is_valid_url(url):
            return False
        
        # PlaywrightëŠ” ê±°ì˜ ëª¨ë“  ì›¹í˜ì´ì§€ ì²˜ë¦¬ ê°€ëŠ¥
        return True
    
    async def interact_with_page(self, url: str, interactions: List[Dict]) -> CrawlResult:
        """í˜ì´ì§€ ìƒí˜¸ì‘ìš© (í´ë¦­, ì…ë ¥ ë“±)"""
        if not self.is_initialized:
            await self.initialize()
        
        start_time = time.time()
        page = None
        
        try:
            page = await self.context.new_page()
            await page.goto(url, wait_until='networkidle')
            
            # ìƒí˜¸ì‘ìš© ì‹¤í–‰
            for interaction in interactions:
                await self._execute_interaction(page, interaction)
                await asyncio.sleep(1)  # ê° ìƒí˜¸ì‘ìš© í›„ ëŒ€ê¸°
            
            # ìµœì¢… ê²°ê³¼ ìˆ˜ì§‘
            html = await page.content()
            screenshot = await page.screenshot(full_page=True)
            
            return CrawlResult(
                success=True,
                html=html,
                screenshot=screenshot,
                crawler_used=CrawlerType.PLAYWRIGHT,
                url=url,
                response_time=time.time() - start_time,
                metadata={'interactions_count': len(interactions)}
            )
            
        except Exception as e:
            return CrawlResult(
                success=False,
                error=f"í˜ì´ì§€ ìƒí˜¸ì‘ìš© ì‹¤íŒ¨: {e}",
                crawler_used=CrawlerType.PLAYWRIGHT,
                url=url,
                response_time=time.time() - start_time
            )
            
        finally:
            if page:
                await page.close()
    
    async def _execute_interaction(self, page: Page, interaction: Dict):
        """ê°œë³„ ìƒí˜¸ì‘ìš© ì‹¤í–‰"""
        action = interaction.get('action')
        selector = interaction.get('selector')
        value = interaction.get('value')
        
        if action == 'click':
            await page.click(selector)
        elif action == 'type':
            await page.fill(selector, value)
        elif action == 'scroll':
            await page.evaluate(f"window.scrollTo(0, {value or 'document.body.scrollHeight'})")
        elif action == 'wait':
            await page.wait_for_selector(selector)
        elif action == 'select':
            await page.select_option(selector, value)
    
    async def capture_pdf(self, url: str, save_path: str = None) -> CrawlResult:
        """PDF ìº¡ì²˜"""
        if not self.is_initialized:
            await self.initialize()
        
        start_time = time.time()
        page = None
        
        try:
            page = await self.context.new_page()
            await page.goto(url, wait_until='networkidle')
            
            # PDF ìƒì„±
            pdf_bytes = await page.pdf(
                format='A4',
                print_background=True,
                margin={
                    'top': '1cm',
                    'right': '1cm',
                    'bottom': '1cm',
                    'left': '1cm'
                }
            )
            
            # íŒŒì¼ ì €ì¥ (ì„ íƒì‚¬í•­)
            if save_path:
                with open(save_path, 'wb') as f:
                    f.write(pdf_bytes)
            
            return CrawlResult(
                success=True,
                html=f"PDF generated: {len(pdf_bytes)} bytes",
                metadata={
                    'pdf_size': len(pdf_bytes),
                    'pdf_path': save_path
                },
                crawler_used=CrawlerType.PLAYWRIGHT,
                url=url,
                response_time=time.time() - start_time
            )
            
        except Exception as e:
            return CrawlResult(
                success=False,
                error=f"PDF ìƒì„± ì‹¤íŒ¨: {e}",
                crawler_used=CrawlerType.PLAYWRIGHT,
                url=url,
                response_time=time.time() - start_time
            )
            
        finally:
            if page:
                await page.close()
    
    async def mobile_crawl(self, url: str, device: str = "iPhone 13") -> CrawlResult:
        """ëª¨ë°”ì¼ ë””ë°”ì´ìŠ¤ ì—ë®¬ë ˆì´ì…˜ í¬ë¡¤ë§"""
        # ìƒˆ ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ëª¨ë°”ì¼)
        old_context = self.context
        await self._create_context(mobile_device=device)
        
        try:
            config = CrawlConfig(url=url, screenshot=True)
            result = await self.crawl(config)
            result.metadata['mobile_device'] = device
            return result
            
        finally:
            # ì›ë˜ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³µêµ¬
            await self.context.close()
            self.context = old_context
    
    def get_supported_features(self) -> List[str]:
        """ì§€ì›í•˜ëŠ” ê¸°ëŠ¥ ëª©ë¡"""
        return [
            "javascript_rendering",
            "screenshot_capture",
            "page_interaction",
            "pdf_generation", 
            "mobile_emulation",
            "network_monitoring",
            "performance_metrics",
            "stealth_mode"
        ]


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜

async def quick_screenshot(url: str, save_path: str = None) -> bool:
    """ë¹ ë¥¸ ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜"""
    crawler = PlaywrightCrawler()
    try:
        await crawler.initialize()
        config = CrawlConfig(url=url, screenshot=True, screenshot_full_page=True)
        result = await crawler.crawl(config)
        
        if result.success and result.screenshot:
            if save_path:
                with open(save_path, 'wb') as f:
                    f.write(result.screenshot)
            return True
        
        return False
        
    finally:
        await crawler.cleanup()


async def render_js_content(url: str) -> str:
    """JavaScript ë Œë”ë§ í›„ HTML ë°˜í™˜"""
    crawler = PlaywrightCrawler()
    try:
        await crawler.initialize()
        config = CrawlConfig(url=url, wait_for_load=True)
        result = await crawler.crawl(config)
        return result.html if result.success else ""
    finally:
        await crawler.cleanup()