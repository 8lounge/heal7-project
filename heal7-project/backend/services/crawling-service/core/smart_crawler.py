#!/usr/bin/env python3
"""
ğŸ§  ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ëŸ¬ - ì§€ëŠ¥í˜• 3-Tier í†µí•© ì‹œìŠ¤í…œ
ìë™ í´ë°± ë° ìµœì  í¬ë¡¤ëŸ¬ ì„ íƒ

Features:
- ìë™ í¬ë¡¤ëŸ¬ ì„ íƒ
- ìˆœì°¨ í´ë°± ì‹œìŠ¤í…œ
- ì„±ëŠ¥ ê¸°ë°˜ í•™ìŠµ
- ë°°ì¹˜ ì²˜ë¦¬
- ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

Author: HEAL7 Development Team
Version: 1.0.0
Date: 2025-08-30
"""

import asyncio
import logging
import time
import statistics
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict, deque
from urllib.parse import urlparse

from .crawlers import (
    BaseCrawler, CrawlResult, CrawlConfig, CrawlerType,
    HttpxCrawler, PlaywrightCrawler
)
from .ai_crawler_selector import get_ai_crawler_selector, select_crawler_for_url, evaluate_crawl_result
from .user_approval_workflow import (
    get_approval_workflow, request_crawl_approval, wait_for_approval, 
    ApprovalStatus, ApprovalUrgency
)


logger = logging.getLogger(__name__)


class CrawlStrategy(Enum):
    """í¬ë¡¤ë§ ì „ëµ"""
    AUTO = "auto"           # ìë™ ì„ íƒ
    FAST = "fast"          # ì†ë„ ìš°ì„  (httpx â†’ playwright)
    RENDER = "render"      # ë Œë”ë§ ìš°ì„  (playwright â†’ httpx)  
    STEALTH = "stealth"    # ë³µì¡ ì‚¬ì´íŠ¸ ìš°ì„  (playwright â†’ httpx)
    SAFE = "safe"          # ì•ˆì •ì„± ìš°ì„  (ëª¨ë“  í¬ë¡¤ëŸ¬ ì‹œë„)


@dataclass
class CrawlerStats:
    """í¬ë¡¤ëŸ¬ í†µê³„"""
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    average_response_time: float = 0.0
    recent_response_times: deque = field(default_factory=lambda: deque(maxlen=10))
    error_types: Dict[str, int] = field(default_factory=dict)
    last_used: Optional[float] = None
    
    def update(self, result: CrawlResult):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        self.total_requests += 1
        self.last_used = time.time()
        
        if result.success:
            self.successful_requests += 1
        else:
            self.failed_requests += 1
            if result.error_type:
                self.error_types[result.error_type] = self.error_types.get(result.error_type, 0) + 1
        
        if result.response_time:
            self.recent_response_times.append(result.response_time)
            self.average_response_time = statistics.mean(self.recent_response_times)
    
    @property
    def success_rate(self) -> float:
        """ì„±ê³µë¥ """
        if self.total_requests == 0:
            return 0.0
        return self.successful_requests / self.total_requests
    
    @property
    def reliability_score(self) -> float:
        """ì‹ ë¢°ë„ ì ìˆ˜ (ì„±ê³µë¥  + ì†ë„ ê³ ë ¤)"""
        if self.total_requests < 3:  # ìµœì†Œ 3ë²ˆì˜ ì‹œë„ í•„ìš”
            return 0.5
        
        # ì„±ê³µë¥  (70%) + ì†ë„ ì ìˆ˜ (30%)
        speed_score = max(0, 1 - (self.average_response_time / 30))  # 30ì´ˆë¥¼ ê¸°ì¤€
        return (self.success_rate * 0.7) + (speed_score * 0.3)


class SmartCrawler:
    """ğŸ§  ì§€ëŠ¥í˜• 3-Tier í¬ë¡¤ëŸ¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.name = "smart_crawler"
        self.logger = logging.getLogger(f"{__name__}.SmartCrawler")
        
        # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ë“¤ (3ë‹¨ê³„ ê°„ì†Œí™”)
        self.crawlers: Dict[CrawlerType, BaseCrawler] = {}
        self.crawler_stats: Dict[CrawlerType, CrawlerStats] = {
            CrawlerType.HTTPX: CrawlerStats(),
            CrawlerType.PLAYWRIGHT: CrawlerStats()
        }
        
        # ë„ë©”ì¸ë³„ ìµœì  í¬ë¡¤ëŸ¬ ìºì‹œ
        self.domain_preferences: Dict[str, CrawlerType] = {}
        
        # ì „ì—­ ì„¤ì •
        self.max_retries = 2
        self.retry_delay = 1.0
        self.is_initialized = False
        
        # ì„±ëŠ¥ ì„ê³„ê°’
        self.performance_thresholds = {
            'min_success_rate': 0.7,
            'max_response_time': 30.0,
            'min_reliability_score': 0.6
        }
    
    async def initialize(self):
        """ëª¨ë“  í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        if self.is_initialized:
            return
        
        self.logger.info("ğŸ§  Smart Crawler ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘")
        
        try:
            # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (3ë‹¨ê³„ ê°„ì†Œí™”)
            self.crawlers = {
                CrawlerType.HTTPX: HttpxCrawler(),
                CrawlerType.PLAYWRIGHT: PlaywrightCrawler(headless=True)
            }
            
            # ë³‘ë ¬ ì´ˆê¸°í™”
            init_tasks = []
            for crawler_type, crawler in self.crawlers.items():
                init_tasks.append(self._safe_initialize_crawler(crawler_type, crawler))
            
            await asyncio.gather(*init_tasks)
            
            self.is_initialized = True
            self.logger.info("âœ… Smart Crawler ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ Smart Crawler ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def _safe_initialize_crawler(self, crawler_type: CrawlerType, crawler: BaseCrawler):
        """ì•ˆì „í•œ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        try:
            await crawler.initialize()
            self.logger.info(f"âœ… {crawler_type.value} í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"âŒ {crawler_type.value} í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ì´ˆê¸°í™” ì‹¤íŒ¨í•œ í¬ë¡¤ëŸ¬ëŠ” ì œê±°
            if crawler_type in self.crawlers:
                del self.crawlers[crawler_type]
    
    async def cleanup(self):
        """ëª¨ë“  í¬ë¡¤ëŸ¬ ì •ë¦¬"""
        cleanup_tasks = []
        for crawler in self.crawlers.values():
            cleanup_tasks.append(crawler.cleanup())
        
        await asyncio.gather(*cleanup_tasks, return_exceptions=True)
        
        self.crawlers.clear()
        self.is_initialized = False
        self.logger.info("ğŸ›‘ Smart Crawler ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")
    
    async def crawl(
        self, 
        url: str, 
        strategy: CrawlStrategy = CrawlStrategy.AUTO,
        require_approval: bool = False,
        **kwargs
    ) -> CrawlResult:
        """ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì‹¤í–‰ (ì‚¬ìš©ì ìŠ¹ì¸ ì›Œí¬í”Œë¡œìš° í¬í•¨)"""
        if not self.is_initialized:
            await self.initialize()
        
        config = CrawlConfig(url=url, **kwargs)
        
        # ğŸ§  AI ê¸°ë°˜ í¬ë¡¤ëŸ¬ ì¶”ì²œ
        try:
            ai_recommendation = await select_crawler_for_url(url, use_ai=True)
            crawler_config_dict = {
                "url": url,
                "strategy": strategy.value,
                "timeout": config.timeout,
                "retries": config.retries,
                "screenshot": config.screenshot,
                "ai_recommendation": ai_recommendation.to_dict()
            }
            
            # âœ‹ ì‚¬ìš©ì ìŠ¹ì¸ ì›Œí¬í”Œë¡œìš° (í•„ìš”ì‹œ)
            if require_approval:
                approval_request_id = await self._request_crawl_approval(
                    url, crawler_config_dict, ai_recommendation.to_dict()
                )
                
                self.logger.info(f"ğŸ“‹ ìŠ¹ì¸ ìš”ì²­ ëŒ€ê¸° ì¤‘: {approval_request_id}")
                
                # ìŠ¹ì¸ ëŒ€ê¸°
                approval_response = await wait_for_approval(approval_request_id)
                
                if approval_response.status != ApprovalStatus.APPROVED:
                    return CrawlResult(
                        success=False,
                        error=f"í¬ë¡¤ë§ ìŠ¹ì¸ ê±°ë¶€ë¨: {approval_response.comment}",
                        url=url,
                        metadata={"approval_status": approval_response.status.value}
                    )
                
                # ìŠ¹ì¸ëœ ì„¤ì •ìœ¼ë¡œ ì—…ë°ì´íŠ¸
                if approval_response.modified_config:
                    config = CrawlConfig(**approval_response.modified_config)
                    self.logger.info(f"âœ… ìŠ¹ì¸ ì™„ë£Œ, ìˆ˜ì •ëœ ì„¤ì • ì ìš©: {approval_request_id}")
                else:
                    self.logger.info(f"âœ… ìŠ¹ì¸ ì™„ë£Œ, ì›ë˜ ì„¤ì • ì‚¬ìš©: {approval_request_id}")
        
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ì¶”ì²œ ë˜ëŠ” ìŠ¹ì¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            ai_recommendation = None
        
        # í¬ë¡¤ëŸ¬ ìˆœì„œ ê²°ì •
        crawler_order = await self._determine_crawler_order(url, strategy)
        
        if not crawler_order:
            return CrawlResult(
                success=False,
                error="ì‚¬ìš© ê°€ëŠ¥í•œ í¬ë¡¤ëŸ¬ê°€ ì—†ìŠµë‹ˆë‹¤",
                url=url
            )
        
        # ìˆœì°¨ì ìœ¼ë¡œ í¬ë¡¤ëŸ¬ ì‹œë„
        last_result = None
        attempts = 0
        
        for crawler_type in crawler_order:
            if crawler_type not in self.crawlers:
                continue
            
            crawler = self.crawlers[crawler_type]
            attempts += 1
            
            try:
                self.logger.info(f"ğŸ¯ í¬ë¡¤ë§ ì‹œë„ {attempts}: {crawler_type.value} - {url}")
                
                result = await crawler.crawl(config)
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.crawler_stats[crawler_type].update(result)
                
                # ğŸ§  AI ì„±ëŠ¥ í‰ê°€ ë° í•™ìŠµ
                try:
                    ai_score = await evaluate_crawl_result(
                        url, crawler_type, 
                        {"success": result.success, "response_time": result.response_time, "html": result.html}
                    )
                    self.logger.debug(f"ğŸ¤– AI ì„±ëŠ¥ í‰ê°€: {crawler_type.value} â†’ {ai_score:.1f}ì ")
                except Exception as ai_e:
                    self.logger.warning(f"âš ï¸ AI ì„±ëŠ¥ í‰ê°€ ì‹¤íŒ¨: {ai_e}")
                
                if result.success:
                    # ì„±ê³µí•œ í¬ë¡¤ëŸ¬ë¥¼ ë„ë©”ì¸ ì„ í˜¸ë„ì— ì €ì¥
                    domain = self._extract_domain(url)
                    self.domain_preferences[domain] = crawler_type
                    
                    self.logger.info(f"âœ… í¬ë¡¤ë§ ì„±ê³µ: {crawler_type.value}")
                    return result
                else:
                    self.logger.warning(f"âŒ {crawler_type.value} ì‹¤íŒ¨: {result.error}")
                    last_result = result
                    
            except Exception as e:
                error_msg = f"{crawler_type.value} ì˜ˆì™¸: {e}"
                self.logger.error(error_msg)
                
                # í†µê³„ ì—…ë°ì´íŠ¸ (ì‹¤íŒ¨)
                dummy_result = CrawlResult(
                    success=False,
                    error=error_msg,
                    crawler_used=crawler_type,
                    url=url
                )
                self.crawler_stats[crawler_type].update(dummy_result)
                
                last_result = dummy_result
        
        # ëª¨ë“  í¬ë¡¤ëŸ¬ ì‹¤íŒ¨
        return last_result or CrawlResult(
            success=False,
            error="ëª¨ë“  í¬ë¡¤ëŸ¬ ì‹¤íŒ¨",
            url=url
        )
    
    async def _determine_crawler_order(self, url: str, strategy: CrawlStrategy) -> List[CrawlerType]:
        """í¬ë¡¤ëŸ¬ ìš°ì„ ìˆœìœ„ ê²°ì •"""
        domain = self._extract_domain(url)
        
        # ë„ë©”ì¸ ê¸°ë°˜ ì„ í˜¸ë„ê°€ ìˆìœ¼ë©´ ìš°ì„  ì ìš©
        if domain in self.domain_preferences:
            preferred = self.domain_preferences[domain]
            if preferred in self.crawlers:
                other_crawlers = [c for c in self.crawlers.keys() if c != preferred]
                # ì„±ëŠ¥ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
                other_crawlers.sort(key=lambda c: self.crawler_stats[c].reliability_score, reverse=True)
                return [preferred] + other_crawlers
        
        # ì „ëµë³„ ìˆœì„œ ê²°ì •
        if strategy == CrawlStrategy.FAST:
            order = [CrawlerType.HTTPX, CrawlerType.PLAYWRIGHT]
        elif strategy == CrawlStrategy.RENDER:
            order = [CrawlerType.PLAYWRIGHT, CrawlerType.HTTPX]
        elif strategy == CrawlStrategy.STEALTH:
            order = [CrawlerType.PLAYWRIGHT, CrawlerType.HTTPX]  # Selenium ì œê±°, Playwrightë¡œ ëŒ€ì²´
        elif strategy == CrawlStrategy.SAFE:
            # ëª¨ë“  í¬ë¡¤ëŸ¬ë¥¼ ì•ˆì •ì„± ìˆœìœ¼ë¡œ
            order = sorted(self.crawlers.keys(), 
                         key=lambda c: self.crawler_stats[c].reliability_score, 
                         reverse=True)
        else:  # AUTO
            order = await self._auto_determine_order(url)
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ í¬ë¡¤ëŸ¬ë§Œ ë°˜í™˜
        return [c for c in order if c in self.crawlers]
    
    async def _auto_determine_order(self, url: str) -> List[CrawlerType]:
        """ğŸ§  AI ê¸°ë°˜ ìë™ í¬ë¡¤ëŸ¬ ìˆœì„œ ê²°ì • (Gemini 2.0 í†µí•©)"""
        try:
            # AI ê¸°ë°˜ í¬ë¡¤ëŸ¬ ì¶”ì²œ
            recommendation = await select_crawler_for_url(url, use_ai=True)
            
            self.logger.info(
                f"ğŸ¤– AI ì¶”ì²œ: {recommendation.primary_crawler.value} "
                f"(ì‹ ë¢°ë„: {recommendation.confidence_score:.1f}%) - {recommendation.reasoning}"
            )
            
            return [recommendation.primary_crawler, recommendation.fallback_crawler]
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI í¬ë¡¤ëŸ¬ ì„ íƒ ì‹¤íŒ¨, íœ´ë¦¬ìŠ¤í‹± í´ë°±: {e}")
            
            # í´ë°±: ê¸°ì¡´ íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ë¶„ì„
            url_lower = url.lower()
            
            # URL íŒ¨í„´ ê¸°ë°˜ ë¶„ì„
            if any(keyword in url_lower for keyword in ['api', '.json', '/rest/', '/graphql']):
                # API ì—”ë“œí¬ì¸íŠ¸ -> httpx ìš°ì„ 
                return [CrawlerType.HTTPX, CrawlerType.PLAYWRIGHT]
            
            elif any(keyword in url_lower for keyword in ['cloudflare', 'protection']):
                # ë³´í˜¸ëœ ì‚¬ì´íŠ¸ -> playwright ìš°ì„  (Selenium ì œê±°)
                return [CrawlerType.PLAYWRIGHT, CrawlerType.HTTPX]
            
            elif '.go.kr' in url_lower or '.gov' in url_lower:
                # ì •ë¶€ ì‚¬ì´íŠ¸ -> playwright ìš°ì„  (Selenium ì œê±°, ë³´ì•ˆ ê°•í™”)
                return [CrawlerType.PLAYWRIGHT, CrawlerType.HTTPX]
            
            else:
                # ì¼ë°˜ ì‚¬ì´íŠ¸ -> ê¸°ë³¸ ìˆœì„œ
                available_crawlers = list(self.crawlers.keys())
                available_crawlers.sort(
                    key=lambda c: self.crawler_stats[c].reliability_score, 
                    reverse=True
                )
                return available_crawlers
    
    def _extract_domain(self, url: str) -> str:
        """URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ"""
        try:
            parsed = urlparse(url)
            return parsed.netloc.lower()
        except:
            return url.lower()
    
    async def batch_crawl(
        self, 
        urls: List[str], 
        strategy: CrawlStrategy = CrawlStrategy.AUTO,
        max_concurrent: int = 5,
        **kwargs
    ) -> List[CrawlResult]:
        """ë°°ì¹˜ í¬ë¡¤ë§"""
        if not self.is_initialized:
            await self.initialize()
        
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def crawl_single(url: str) -> CrawlResult:
            async with semaphore:
                return await self.crawl(url, strategy, **kwargs)
        
        self.logger.info(f"ğŸš€ ë°°ì¹˜ í¬ë¡¤ë§ ì‹œì‘: {len(urls)}ê°œ URL")
        
        tasks = [crawl_single(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì˜ˆì™¸ ì²˜ë¦¬
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append(CrawlResult(
                    success=False,
                    error=str(result),
                    url=urls[i]
                ))
            else:
                processed_results.append(result)
        
        # ì„±ê³µë¥  í†µê³„
        successful = sum(1 for r in processed_results if r.success)
        self.logger.info(f"âœ… ë°°ì¹˜ í¬ë¡¤ë§ ì™„ë£Œ: {successful}/{len(urls)} ì„±ê³µ")
        
        return processed_results
    
    async def health_check(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
        health_status = {
            'system_healthy': True,
            'initialized': self.is_initialized,
            'available_crawlers': list(self.crawlers.keys()),
            'crawler_health': {},
            'performance_summary': {}
        }
        
        if not self.is_initialized:
            health_status['system_healthy'] = False
            return health_status
        
        # ê° í¬ë¡¤ëŸ¬ ìƒíƒœ í™•ì¸
        for crawler_type, crawler in self.crawlers.items():
            try:
                is_healthy = await crawler.health_check()
                health_status['crawler_health'][crawler_type.value] = {
                    'healthy': is_healthy,
                    'stats': self.get_crawler_stats(crawler_type)
                }
                
                if not is_healthy:
                    health_status['system_healthy'] = False
                    
            except Exception as e:
                health_status['crawler_health'][crawler_type.value] = {
                    'healthy': False,
                    'error': str(e)
                }
                health_status['system_healthy'] = False
        
        # ì „ì²´ ì„±ëŠ¥ ìš”ì•½
        health_status['performance_summary'] = self.get_performance_summary()
        
        return health_status
    
    def get_crawler_stats(self, crawler_type: CrawlerType) -> Dict[str, Any]:
        """í¬ë¡¤ëŸ¬ë³„ í†µê³„"""
        stats = self.crawler_stats[crawler_type]
        return {
            'total_requests': stats.total_requests,
            'success_rate': stats.success_rate,
            'average_response_time': stats.average_response_time,
            'reliability_score': stats.reliability_score,
            'error_types': dict(stats.error_types),
            'last_used': stats.last_used
        }
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """ì „ì²´ ì„±ëŠ¥ ìš”ì•½"""
        total_requests = sum(stats.total_requests for stats in self.crawler_stats.values())
        total_successful = sum(stats.successful_requests for stats in self.crawler_stats.values())
        
        if total_requests == 0:
            return {
                'overall_success_rate': 0.0,
                'total_requests': 0,
                'best_performer': None,
                'domain_preferences_count': len(self.domain_preferences)
            }
        
        # ìµœê³  ì„±ëŠ¥ í¬ë¡¤ëŸ¬
        best_performer = max(
            self.crawler_stats.items(),
            key=lambda x: x[1].reliability_score
        )[0].value if self.crawler_stats else None
        
        return {
            'overall_success_rate': total_successful / total_requests,
            'total_requests': total_requests,
            'best_performer': best_performer,
            'domain_preferences_count': len(self.domain_preferences),
            'avg_response_time': statistics.mean([
                stats.average_response_time 
                for stats in self.crawler_stats.values() 
                if stats.average_response_time > 0
            ]) if any(stats.average_response_time > 0 for stats in self.crawler_stats.values()) else 0
        }
    
    def optimize_performance(self):
        """ì„±ëŠ¥ ìµœì í™”"""
        self.logger.info("ğŸ”§ ì„±ëŠ¥ ìµœì í™” ì‹¤í–‰")
        
        # ì„±ëŠ¥ì´ ë‚®ì€ í¬ë¡¤ëŸ¬ì˜ ë„ë©”ì¸ ì„ í˜¸ë„ ì œê±°
        domains_to_remove = []
        for domain, preferred_crawler in self.domain_preferences.items():
            if preferred_crawler in self.crawler_stats:
                stats = self.crawler_stats[preferred_crawler]
                if stats.reliability_score < self.performance_thresholds['min_reliability_score']:
                    domains_to_remove.append(domain)
        
        for domain in domains_to_remove:
            del self.domain_preferences[domain]
            self.logger.info(f"ë„ë©”ì¸ ì„ í˜¸ë„ ì œê±°: {domain}")
        
        self.logger.info("âœ… ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ")
    
    async def _request_crawl_approval(
        self, 
        url: str, 
        crawler_config: Dict[str, Any], 
        ai_recommendation: Dict[str, Any]
    ) -> str:
        """í¬ë¡¤ë§ ìŠ¹ì¸ ìš”ì²­"""
        title = f"í¬ë¡¤ë§ ì‘ì—…: {self._extract_domain(url)}"
        
        return await request_crawl_approval(
            title=title,
            urls=[url],
            crawler_config=crawler_config,
            ai_recommendation=ai_recommendation,
            requester="smart_crawler",
            estimated_duration="1-5ë¶„"
        )
    
    async def get_ai_statistics(self) -> Dict[str, Any]:
        """ğŸ§  AI í¬ë¡¤ëŸ¬ ì„ íƒ ì‹œìŠ¤í…œ í†µê³„"""
        try:
            ai_selector = await get_ai_crawler_selector()
            ai_stats = ai_selector.get_selection_stats()
            
            return {
                "ai_integration": {
                    "status": "active" if ai_stats.get("ai_analyzer_status") == "active" else "inactive",
                    "total_selections": ai_stats.get("total_selections", 0),
                    "ai_selections": ai_stats.get("ai_selections", 0),
                    "fallback_selections": ai_stats.get("fallback_selections", 0),
                    "ai_usage_rate": ai_stats.get("ai_usage_rate", 0.0),
                    "average_accuracy": ai_stats.get("average_accuracy", 0.0),
                    "learning_domains": ai_stats.get("learning_domains", 0)
                },
                "system_enhancement": {
                    "ai_model": "Gemini 2.0 Flash",
                    "features": [
                        "URL íŒ¨í„´ AI ë¶„ì„",
                        "ë™ì  í¬ë¡¤ëŸ¬ ì„ íƒ",
                        "ì‹¤ì‹œê°„ ì„±ëŠ¥ í•™ìŠµ",
                        "í’ˆì§ˆ ê¸°ë°˜ ìµœì í™”"
                    ],
                    "performance_impact": "í–¥ìƒëœ ì„ íƒ ì •í™•ë„ ë° ì ì‘í˜• í•™ìŠµ"
                }
            }
        except Exception as e:
            self.logger.error(f"âŒ AI í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                "ai_integration": {
                    "status": "error",
                    "error": str(e)
                }
            }
    
    async def get_approval_statistics(self) -> Dict[str, Any]:
        """âœ‹ ì‚¬ìš©ì ìŠ¹ì¸ ì›Œí¬í”Œë¡œìš° í†µê³„"""
        try:
            approval_workflow = await get_approval_workflow()
            approval_stats = approval_workflow.get_approval_statistics()
            
            return {
                "approval_workflow": {
                    "status": "active",
                    "total_requests": approval_stats.get("total_requests", 0),
                    "approved": approval_stats.get("approved", 0),
                    "rejected": approval_stats.get("rejected", 0),
                    "expired": approval_stats.get("expired", 0),
                    "pending_requests": approval_stats.get("pending_requests", 0),
                    "approval_rate": approval_stats.get("approval_rate", 0.0),
                    "auto_approval_rate": approval_stats.get("auto_approval_rate", 0.0),
                    "avg_response_time": approval_stats.get("avg_response_time", 0.0)
                },
                "workflow_features": [
                    "ìœ„í—˜ë„ ê¸°ë°˜ ìë™ ë¶„ë¥˜",
                    "ê¸´ê¸‰ë„ë³„ íƒ€ì„ì•„ì›ƒ ì„¤ì •",
                    "ìë™ ìŠ¹ì¸ ê·œì¹™ ì ìš©",
                    "ì‹¤ì‹œê°„ ì•Œë¦¼ ì‹œìŠ¤í…œ",
                    "ìŠ¹ì¸ ì´ë ¥ ì¶”ì "
                ]
            }
        except Exception as e:
            self.logger.error(f"âŒ ìŠ¹ì¸ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                "approval_workflow": {
                    "status": "error",
                    "error": str(e)
                }
            }


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤

async def quick_crawl(url: str, strategy: str = "auto", **kwargs) -> CrawlResult:
    """ë¹ ë¥¸ í¬ë¡¤ë§ (ì›ìƒ· í•¨ìˆ˜)"""
    crawler = SmartCrawler()
    try:
        await crawler.initialize()
        strategy_enum = CrawlStrategy(strategy.lower())
        return await crawler.crawl(url, strategy_enum, **kwargs)
    finally:
        await crawler.cleanup()


async def smart_batch_crawl(urls: List[str], **kwargs) -> List[CrawlResult]:
    """ìŠ¤ë§ˆíŠ¸ ë°°ì¹˜ í¬ë¡¤ë§"""
    crawler = SmartCrawler()
    try:
        await crawler.initialize()
        return await crawler.batch_crawl(urls, **kwargs)
    finally:
        await crawler.cleanup()


def create_smart_crawler() -> SmartCrawler:
    """ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ëŸ¬ íŒ©í† ë¦¬"""
    return SmartCrawler()