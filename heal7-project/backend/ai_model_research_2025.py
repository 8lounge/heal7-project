#!/usr/bin/env python3
"""
HEAL7 AI ëª¨ë¸ ìƒì„¸ ì—°êµ¬ ë° ë¶„ì„ ì‹œìŠ¤í…œ (2025)
ê° AI ëª¨ë¸ì˜ êµ¬ì²´ì ì¸ ë²„ì „, íŠ¹ì§•, ê°•ì /ì•½ì ì„ ê²Œì„ ìºë¦­í„°ì²˜ëŸ¼ ë¶„ì„
"""

import asyncio
import aiohttp
import json
import os
import time
from datetime import datetime
from typing import Dict, List, Any, Optional
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv('/home/ubuntu/project/backend/api/.env.ai')

class AIModelResearcher:
    """AI ëª¨ë¸ ìƒì„¸ ì—°êµ¬ ë° ë¶„ì„"""
    
    def __init__(self):
        self.models = {
            # Google Gemini ê³„ì—´
            "gemini-2.0-flash-exp": {
                "name": "Gemini 2.0 Flash Experimental",
                "provider": "Google",
                "family": "Gemini",
                "generation": 2.0,
                "variant": "Flash",
                "release_date": "2024-12",
                "endpoint": "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent",
                "api_key": os.getenv("GOOGLE_API_KEY"),
                "is_free": True,
                "cost_per_1k_input": 0.0,
                "cost_per_1k_output": 0.0,
                "context_window": 1000000,  # 1M tokens
                "max_output": 8192,
                "multimodal": True,
                "realtime": False,
                "languages": ["en", "ko", "ja", "zh", "es", "fr", "de", "pt", "ru", "ar"],
                "specialties": ["multimodal", "vision", "code", "creative", "reasoning"],
                "stats": {
                    "reasoning": 9,      # ë…¼ë¦¬ì  ì¶”ë¡ 
                    "creativity": 8,     # ì°½ì˜ì„±
                    "speed": 10,         # ì‘ë‹µ ì†ë„
                    "accuracy": 8,       # ì •í™•ì„±
                    "multimodal": 10,    # ë©€í‹°ëª¨ë‹¬ ëŠ¥ë ¥
                    "cost": 10,          # ë¹„ìš© íš¨ìœ¨ì„± (ë¬´ë£Œ)
                    "reliability": 7,    # ì•ˆì •ì„± (ì‹¤í—˜ ë²„ì „)
                    "korean": 8          # í•œêµ­ì–´ ì„±ëŠ¥
                },
                "strengths": [
                    "ì™„ì „ ë¬´ë£Œ ì‚¬ìš© ê°€ëŠ¥",
                    "ë§¤ìš° ë¹ ë¥¸ ì‘ë‹µ ì†ë„",
                    "ê°•ë ¥í•œ ë©€í‹°ëª¨ë‹¬ ëŠ¥ë ¥ (í…ìŠ¤íŠ¸+ì´ë¯¸ì§€+ë¹„ë””ì˜¤)",
                    "100ë§Œ í† í° ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°",
                    "ìš°ìˆ˜í•œ ì½”ë“œ ìƒì„± ëŠ¥ë ¥",
                    "ìµœì‹  ì •ë³´ í•™ìŠµ (2024ë…„ ë§ê¹Œì§€)"
                ],
                "weaknesses": [
                    "ì‹¤í—˜ ë²„ì „ìœ¼ë¡œ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ",
                    "ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ ë¶ˆê°€",
                    "ì¼ë¶€ ë³µì¡í•œ ì¶”ë¡ ì—ì„œ ì˜¤ë¥˜ ê°€ëŠ¥",
                    "API ì‚¬ìš©ëŸ‰ ì œí•œ ìˆì„ ìˆ˜ ìˆìŒ"
                ],
                "best_for": ["ì¼ë°˜ ì±„íŒ…", "ì½”ë“œ ìƒì„±", "ì´ë¯¸ì§€ ë¶„ì„", "ì°½ì˜ì  ì‘ì—…", "êµìœ¡"],
                "avoid_for": ["ì‹¤ì‹œê°„ ì •ë³´ ê²€ìƒ‰", "ë¯¸ì…˜ í¬ë¦¬í‹°ì»¬í•œ ì‘ì—…"]
            },
            
            "gemini-1.5-pro": {
                "name": "Gemini 1.5 Pro",
                "provider": "Google",
                "family": "Gemini",
                "generation": 1.5,
                "variant": "Pro",
                "release_date": "2024-02",
                "endpoint": "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro:generateContent",
                "api_key": os.getenv("GOOGLE_API_KEY"),
                "is_free": False,
                "cost_per_1k_input": 0.00125,
                "cost_per_1k_output": 0.00375,
                "context_window": 2000000,  # 2M tokens!
                "max_output": 8192,
                "multimodal": True,
                "realtime": False,
                "languages": ["en", "ko", "ja", "zh", "es", "fr", "de", "pt", "ru", "ar"],
                "specialties": ["long-context", "analysis", "reasoning", "multimodal"],
                "stats": {
                    "reasoning": 10,
                    "creativity": 8,
                    "speed": 7,
                    "accuracy": 9,
                    "multimodal": 10,
                    "cost": 8,
                    "reliability": 9,
                    "korean": 8
                },
                "strengths": [
                    "ì—…ê³„ ìµœëŒ€ 200ë§Œ í† í° ì»¨í…ìŠ¤íŠ¸",
                    "ë§¤ìš° ìš°ìˆ˜í•œ ì¶”ë¡  ëŠ¥ë ¥",
                    "ì•ˆì •ì ì¸ ì„±ëŠ¥",
                    "ê°•ë ¥í•œ ë©€í‹°ëª¨ë‹¬ ëŠ¥ë ¥",
                    "ê¸´ ë¬¸ì„œ ë¶„ì„ì— ìµœì í™”"
                ],
                "weaknesses": [
                    "ìœ ë£Œ ëª¨ë¸",
                    "2.0 Flashë³´ë‹¤ ëŠë¦¼",
                    "ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ ë¶ˆê°€"
                ],
                "best_for": ["ê¸´ ë¬¸ì„œ ë¶„ì„", "ë³µì¡í•œ ì¶”ë¡ ", "í•™ìˆ  ì—°êµ¬", "ë²•ë¥  ë¬¸ì„œ"],
                "avoid_for": ["ì‹¤ì‹œê°„ ì •ë³´", "ë‹¨ìˆœ ì±„íŒ…"]
            },

            # OpenAI GPT ê³„ì—´
            "gpt-4o": {
                "name": "GPT-4o (Omni)",
                "provider": "OpenAI",
                "family": "GPT-4",
                "generation": 4.0,
                "variant": "Omni",
                "release_date": "2024-05",
                "endpoint": "https://api.openai.com/v1/chat/completions",
                "api_key": os.getenv("OPENAI_API_KEY"),
                "is_free": False,
                "cost_per_1k_input": 0.005,
                "cost_per_1k_output": 0.015,
                "context_window": 128000,
                "max_output": 4096,
                "multimodal": True,
                "realtime": False,
                "languages": ["en", "ko", "ja", "zh", "es", "fr", "de", "pt", "ru", "ar"],
                "specialties": ["reasoning", "coding", "writing", "analysis"],
                "stats": {
                    "reasoning": 10,
                    "creativity": 9,
                    "speed": 8,
                    "accuracy": 10,
                    "multimodal": 9,
                    "cost": 6,
                    "reliability": 10,
                    "korean": 9
                },
                "strengths": [
                    "ì—…ê³„ ìµœê³  ìˆ˜ì¤€ì˜ ì¶”ë¡  ëŠ¥ë ¥",
                    "ë§¤ìš° ë†’ì€ ì •í™•ì„±",
                    "ìš°ìˆ˜í•œ ì½”ë”© ëŠ¥ë ¥",
                    "ì•ˆì •ì ì¸ API",
                    "ê°•ë ¥í•œ ë©€í‹°ëª¨ë‹¬ ëŠ¥ë ¥"
                ],
                "weaknesses": [
                    "ìƒëŒ€ì ìœ¼ë¡œ ë¹„ì‹¼ ë¹„ìš©",
                    "ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ ë¶ˆê°€",
                    "ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì œí•œì "
                ],
                "best_for": ["ë³µì¡í•œ ë¶„ì„", "ì½”ë”©", "í•™ìˆ  ì‘ì—…", "ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì„œ"],
                "avoid_for": ["ë‹¨ìˆœ ì‘ì—…", "ì‹¤ì‹œê°„ ì •ë³´"]
            },

            "gpt-4o-mini": {
                "name": "GPT-4o Mini",
                "provider": "OpenAI",
                "family": "GPT-4",
                "generation": 4.0,
                "variant": "Mini",
                "release_date": "2024-07",
                "endpoint": "https://api.openai.com/v1/chat/completions",
                "api_key": os.getenv("OPENAI_API_KEY"),
                "is_free": False,
                "cost_per_1k_input": 0.00015,
                "cost_per_1k_output": 0.0006,
                "context_window": 128000,
                "max_output": 16384,
                "multimodal": True,
                "realtime": False,
                "languages": ["en", "ko", "ja", "zh", "es", "fr", "de", "pt", "ru", "ar"],
                "specialties": ["cost-effective", "general", "fast"],
                "stats": {
                    "reasoning": 8,
                    "creativity": 7,
                    "speed": 9,
                    "accuracy": 8,
                    "multimodal": 8,
                    "cost": 10,       # ë§¤ìš° ì €ë ´
                    "reliability": 9,
                    "korean": 8
                },
                "strengths": [
                    "ë§¤ìš° ì €ë ´í•œ ë¹„ìš© (GPT-4o ëŒ€ë¹„ 1/10)",
                    "ë¹ ë¥¸ ì‘ë‹µ ì†ë„",
                    "GPT-4 ìˆ˜ì¤€ì˜ ê¸°ë³¸ ì„±ëŠ¥",
                    "ì•ˆì •ì ì¸ API",
                    "ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ì— ì í•©"
                ],
                "weaknesses": [
                    "ë³µì¡í•œ ì¶”ë¡ ì—ì„œ GPT-4oë³´ë‹¤ ì•½í•¨",
                    "ì°½ì˜ì„± ë¶€ë¶„ì—ì„œ ì œí•œì ",
                    "ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ ë¶ˆê°€"
                ],
                "best_for": ["ëŒ€ìš©ëŸ‰ ì²˜ë¦¬", "ì¼ë°˜ ì±„íŒ…", "ê¸°ë³¸ ì½”ë”©", "ìš”ì•½"],
                "avoid_for": ["ë³µì¡í•œ ë¶„ì„", "ì°½ì˜ì  ì‘ì—…"]
            },

            "gpt-4-turbo": {
                "name": "GPT-4 Turbo",
                "provider": "OpenAI",
                "family": "GPT-4",
                "generation": 4.0,
                "variant": "Turbo",
                "release_date": "2024-04",
                "endpoint": "https://api.openai.com/v1/chat/completions",
                "api_key": os.getenv("OPENAI_API_KEY"),
                "is_free": False,
                "cost_per_1k_input": 0.01,
                "cost_per_1k_output": 0.03,
                "context_window": 128000,
                "max_output": 4096,
                "multimodal": True,
                "realtime": False,
                "languages": ["en", "ko", "ja", "zh", "es", "fr", "de", "pt", "ru", "ar"],
                "specialties": ["reasoning", "coding", "analysis", "vision"],
                "stats": {
                    "reasoning": 10,
                    "creativity": 9,
                    "speed": 7,
                    "accuracy": 10,
                    "multimodal": 9,
                    "cost": 5,
                    "reliability": 10,
                    "korean": 9
                },
                "strengths": [
                    "ìµœê³  ìˆ˜ì¤€ì˜ ì¶”ë¡  ëŠ¥ë ¥",
                    "2024ë…„ 4ì›”ê¹Œì§€ ìµœì‹  ì •ë³´",
                    "ê°•ë ¥í•œ ë¹„ì „ ëŠ¥ë ¥",
                    "ì•ˆì •ì ì¸ ì„±ëŠ¥"
                ],
                "weaknesses": [
                    "ë†’ì€ ë¹„ìš©",
                    "GPT-4oë³´ë‹¤ ëŠë¦¼",
                    "ì‹¤ì‹œê°„ ì •ë³´ ë¶€ì¡±"
                ],
                "best_for": ["ê³ ê¸‰ ë¶„ì„", "ë³µì¡í•œ ì½”ë”©", "ì—°êµ¬", "ì „ë¬¸ ì‘ì—…"],
                "avoid_for": ["ë¹„ìš© ë¯¼ê°í•œ ì‘ì—…", "ì‹¤ì‹œê°„ ì •ë³´"]
            },

            # Anthropic Claude ê³„ì—´
            "claude-3.5-sonnet": {
                "name": "Claude 3.5 Sonnet",
                "provider": "Anthropic",
                "family": "Claude 3",
                "generation": 3.5,
                "variant": "Sonnet",
                "release_date": "2024-10",
                "endpoint": "https://api.anthropic.com/v1/messages",
                "api_key": os.getenv("ANTHROPIC_API_KEY"),
                "is_free": False,
                "cost_per_1k_input": 0.003,
                "cost_per_1k_output": 0.015,
                "context_window": 200000,  # 200K tokens
                "max_output": 8192,
                "multimodal": True,
                "realtime": False,
                "languages": ["en", "ko", "ja", "zh", "es", "fr", "de", "pt"],
                "specialties": ["safety", "analysis", "writing", "reasoning"],
                "stats": {
                    "reasoning": 9,
                    "creativity": 10,      # ClaudeëŠ” ì°½ì˜ì„±ì´ ê°•í•¨
                    "speed": 6,
                    "accuracy": 9,
                    "multimodal": 8,
                    "cost": 7,
                    "reliability": 10,
                    "korean": 7
                },
                "strengths": [
                    "ë§¤ìš° ìš°ìˆ˜í•œ ì°½ì˜ì  ê¸€ì“°ê¸°",
                    "ê°•ë ¥í•œ ì•ˆì „ì„± ë° ìœ¤ë¦¬ì  ì¶”ë¡ ",
                    "ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ëŠ¥ë ¥",
                    "ì •í™•í•˜ê³  ì‹ ì¤‘í•œ ì‘ë‹µ",
                    "ì½”ë“œ ë¶„ì„ ë° ë¦¬ë·° ìš°ìˆ˜"
                ],
                "weaknesses": [
                    "ìƒëŒ€ì ìœ¼ë¡œ ëŠë¦° ì‘ë‹µ",
                    "í•œêµ­ì–´ ì„±ëŠ¥ ì œí•œì ",
                    "ë•Œë¡œ ê³¼ë„í•˜ê²Œ ì‹ ì¤‘í•¨",
                    "ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ ë¶ˆê°€"
                ],
                "best_for": ["ì°½ì˜ì  ê¸€ì“°ê¸°", "ì½”ë“œ ë¦¬ë·°", "ìœ¤ë¦¬ì  ë¶„ì„", "ì•ˆì „í•œ AI"],
                "avoid_for": ["ë¹ ë¥¸ ì‘ë‹µ í•„ìš”", "ì‹¤ì‹œê°„ ì •ë³´"]
            },

            "claude-3-opus": {
                "name": "Claude 3 Opus",
                "provider": "Anthropic",
                "family": "Claude 3",
                "generation": 3.0,
                "variant": "Opus",
                "release_date": "2024-03",
                "endpoint": "https://api.anthropic.com/v1/messages",
                "api_key": os.getenv("ANTHROPIC_API_KEY"),
                "is_free": False,
                "cost_per_1k_input": 0.015,
                "cost_per_1k_output": 0.075,
                "context_window": 200000,
                "max_output": 4096,
                "multimodal": True,
                "realtime": False,
                "languages": ["en", "ko", "ja", "zh", "es", "fr", "de", "pt"],
                "specialties": ["reasoning", "analysis", "research", "complex-tasks"],
                "stats": {
                    "reasoning": 10,
                    "creativity": 10,
                    "speed": 5,
                    "accuracy": 10,
                    "multimodal": 8,
                    "cost": 3,        # ë§¤ìš° ë¹„ìŒˆ
                    "reliability": 10,
                    "korean": 7
                },
                "strengths": [
                    "ìµœê³  ìˆ˜ì¤€ì˜ ì¶”ë¡  ëŠ¥ë ¥",
                    "íƒì›”í•œ ì°½ì˜ì„±",
                    "ë³µì¡í•œ ë¬¸ì œ í•´ê²°",
                    "ë†’ì€ ì •í™•ì„±"
                ],
                "weaknesses": [
                    "ë§¤ìš° ë†’ì€ ë¹„ìš©",
                    "ë§¤ìš° ëŠë¦° ì‘ë‹µ",
                    "í•œêµ­ì–´ ì œí•œì "
                ],
                "best_for": ["ìµœê³  í’ˆì§ˆ í•„ìš”", "ë³µì¡í•œ ì—°êµ¬", "ì¤‘ìš”í•œ ë¶„ì„"],
                "avoid_for": ["ì¼ë°˜ ì‘ì—…", "ë¹„ìš© ë¯¼ê°", "ë¹ ë¥¸ ì‘ë‹µ"]
            },

            # Perplexity ê³„ì—´
            "perplexity-sonar": {
                "name": "Perplexity Sonar",
                "provider": "Perplexity AI",
                "family": "Sonar",
                "generation": 1.0,
                "variant": "Standard",
                "release_date": "2024-01",
                "endpoint": "https://api.perplexity.ai/chat/completions",
                "api_key": os.getenv("PERPLEXITY_API_KEY"),
                "is_free": False,
                "cost_per_1k_input": 0.005,
                "cost_per_1k_output": 0.005,
                "context_window": 4096,
                "max_output": 4096,
                "multimodal": False,
                "realtime": True,     # í•µì‹¬ ê°•ì !
                "languages": ["en", "ko", "ja", "zh", "es", "fr", "de"],
                "specialties": ["realtime-search", "research", "current-events", "citations"],
                "stats": {
                    "reasoning": 7,
                    "creativity": 6,
                    "speed": 6,
                    "accuracy": 8,
                    "multimodal": 3,      # í…ìŠ¤íŠ¸ë§Œ
                    "cost": 7,
                    "reliability": 8,
                    "korean": 6,
                    "realtime": 10        # ì‹¤ì‹œê°„ ì •ë³´ ëŠ¥ë ¥
                },
                "strengths": [
                    "ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ ëŠ¥ë ¥",
                    "ìµœì‹  ì •ë³´ ì œê³µ",
                    "ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜ ì¸ìš©",
                    "ì‚¬ì‹¤ í™•ì¸ ìš°ìˆ˜",
                    "ë‰´ìŠ¤ ë° íŠ¸ë Œë“œ ë¶„ì„"
                ],
                "weaknesses": [
                    "ì œí•œì ì¸ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°",
                    "ë©€í‹°ëª¨ë‹¬ ì§€ì› ì—†ìŒ",
                    "ì°½ì˜ì  ì‘ì—…ì— ì•½í•¨",
                    "í•œêµ­ì–´ ì„±ëŠ¥ ì œí•œì "
                ],
                "best_for": ["ì‹¤ì‹œê°„ ì •ë³´", "ë‰´ìŠ¤ ë¶„ì„", "íŒ©íŠ¸ ì²´í¬", "ì—°êµ¬", "íŠ¸ë Œë“œ"],
                "avoid_for": ["ì°½ì˜ì  ì‘ì—…", "ê¸´ ë¬¸ì„œ", "ë©€í‹°ëª¨ë‹¬"]
            },

            # ë¯¸ë˜ ì¶”ê°€ ì˜ˆì • ëª¨ë¸ë“¤
            "claude-4": {
                "name": "Claude 4 (ì˜ˆì •)",
                "provider": "Anthropic",
                "family": "Claude 4",
                "generation": 4.0,
                "variant": "Standard",
                "release_date": "2025-H1",
                "is_available": False,
                "expected_improvements": [
                    "ë” ë¹ ë¥¸ ì‘ë‹µ ì†ë„",
                    "í–¥ìƒëœ ë©€í‹°ëª¨ë‹¬ ëŠ¥ë ¥",
                    "ë” ê¸´ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°",
                    "ë¹„ìš© íš¨ìœ¨ì„± ê°œì„ "
                ]
            },

            "gpt-5": {
                "name": "GPT-5 (ì˜ˆì •)",
                "provider": "OpenAI",
                "family": "GPT-5",
                "generation": 5.0,
                "release_date": "2025-H2",
                "is_available": False,
                "expected_improvements": [
                    "AGI ìˆ˜ì¤€ì˜ ì¶”ë¡ ",
                    "ë©€í‹°ëª¨ë‹¬ í†µí•©",
                    "ì‹¤ì‹œê°„ í•™ìŠµ ëŠ¥ë ¥",
                    "ì—ì´ì „íŠ¸ ê¸°ëŠ¥"
                ]
            },

            "gemini-3.0": {
                "name": "Gemini 3.0 (ì˜ˆì •)",
                "provider": "Google",
                "family": "Gemini",
                "generation": 3.0,
                "release_date": "2025-H2",
                "is_available": False,
                "expected_improvements": [
                    "ë” ê°•ë ¥í•œ ì¶”ë¡ ",
                    "ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ í†µí•©",
                    "í–¥ìƒëœ ë©€í‹°ëª¨ë‹¬",
                    "ì—ì´ì „íŠ¸ ëŠ¥ë ¥"
                ]
            }
        }
        
        self.research_results = {}
    
    async def test_model_performance(self, model_id: str, test_prompts: List[str]) -> Dict[str, Any]:
        """ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        if model_id not in self.models or not self.models[model_id].get("is_available", True):
            return {"error": "ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
        
        model = self.models[model_id]
        results = []
        
        for prompt in test_prompts:
            try:
                start_time = time.time()
                
                if "gemini" in model_id:
                    response = await self._test_gemini(model, prompt)
                elif "gpt" in model_id:
                    response = await self._test_openai(model, prompt)
                elif "claude" in model_id:
                    response = await self._test_claude(model, prompt)
                elif "perplexity" in model_id:
                    response = await self._test_perplexity(model, prompt)
                else:
                    continue
                    
                elapsed_time = time.time() - start_time
                
                results.append({
                    "prompt": prompt[:50] + "...",
                    "success": response.get("success", False),
                    "response_time": elapsed_time,
                    "response_length": len(response.get("response", "")),
                    "error": response.get("error")
                })
                
            except Exception as e:
                results.append({
                    "prompt": prompt[:50] + "...",
                    "success": False,
                    "error": str(e)
                })
        
        return {
            "model": model_id,
            "total_tests": len(test_prompts),
            "successful_tests": sum(1 for r in results if r["success"]),
            "average_response_time": sum(r.get("response_time", 0) for r in results if r["success"]) / max(1, sum(1 for r in results if r["success"])),
            "results": results
        }
    
    async def _test_gemini(self, model: Dict, prompt: str) -> Dict[str, Any]:
        """Gemini ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        api_key = model["api_key"]
        if not api_key:
            return {"success": False, "error": "API í‚¤ ì—†ìŒ"}
        
        payload = {
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {"maxOutputTokens": 100}
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{model['endpoint']}?key={api_key}",
                json=payload,
                timeout=aiohttp.ClientTimeout(total=10)
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    return {
                        "success": True,
                        "response": data["candidates"][0]["content"]["parts"][0]["text"]
                    }
                else:
                    return {"success": False, "error": f"HTTP {response.status}"}
    
    async def _test_openai(self, model: Dict, prompt: str) -> Dict[str, Any]:
        """OpenAI ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        api_key = model["api_key"]
        if not api_key:
            return {"success": False, "error": "API í‚¤ ì—†ìŒ"}
        
        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
        payload = {
            "model": model["name"].lower().replace(" ", "-").replace("(", "").replace(")", ""),
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 100
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                model["endpoint"],
                headers=headers,
                json=payload,
                timeout=aiohttp.ClientTimeout(total=10)
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    return {
                        "success": True,
                        "response": data["choices"][0]["message"]["content"]
                    }
                else:
                    return {"success": False, "error": f"HTTP {response.status}"}
    
    async def _test_claude(self, model: Dict, prompt: str) -> Dict[str, Any]:
        """Claude ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        api_key = model["api_key"]
        if not api_key:
            return {"success": False, "error": "API í‚¤ ì—†ìŒ"}
        
        headers = {
            "x-api-key": api_key,
            "anthropic-version": "2023-06-01",
            "content-type": "application/json"
        }
        payload = {
            "model": "claude-3-5-sonnet-20241022",
            "max_tokens": 100,
            "messages": [{"role": "user", "content": prompt}]
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                model["endpoint"],
                headers=headers,
                json=payload,
                timeout=aiohttp.ClientTimeout(total=10)
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    return {
                        "success": True,
                        "response": data["content"][0]["text"]
                    }
                else:
                    return {"success": False, "error": f"HTTP {response.status}"}
    
    async def _test_perplexity(self, model: Dict, prompt: str) -> Dict[str, Any]:
        """Perplexity ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        api_key = model["api_key"]
        if not api_key:
            return {"success": False, "error": "API í‚¤ ì—†ìŒ"}
        
        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
        payload = {
            "model": "sonar",
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 100
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                model["endpoint"],
                headers=headers,
                json=payload,
                timeout=aiohttp.ClientTimeout(total=15)
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    return {
                        "success": True,
                        "response": data["choices"][0]["message"]["content"]
                    }
                else:
                    return {"success": False, "error": f"HTTP {response.status}"}
    
    def generate_model_comparison_chart(self) -> Dict[str, Any]:
        """ëª¨ë¸ ë¹„êµ ì°¨íŠ¸ ìƒì„± (ê²Œì„ ìºë¦­í„° ìŠ¤íƒ€ì¼)"""
        comparison = {
            "models": {},
            "categories": ["reasoning", "creativity", "speed", "accuracy", "multimodal", "cost", "reliability", "korean"],
            "analysis": {
                "speed_champions": [],
                "cost_efficient": [],
                "reasoning_masters": [],
                "creative_powerhouses": [],
                "multimodal_experts": [],
                "korean_specialists": []
            }
        }
        
        for model_id, model in self.models.items():
            if not model.get("is_available", True):
                continue
                
            stats = model.get("stats", {})
            comparison["models"][model_id] = {
                "name": model["name"],
                "provider": model["provider"],
                "stats": stats,
                "total_score": sum(stats.values()) if stats else 0,
                "strengths": model.get("strengths", []),
                "weaknesses": model.get("weaknesses", []),
                "best_for": model.get("best_for", []),
                "cost_per_1k": model.get("cost_per_1k_input", 0),
                "is_free": model.get("is_free", False)
            }
            
            # ì¹´í…Œê³ ë¦¬ë³„ ì±”í”¼ì–¸ ë¶„ì„
            if stats.get("speed", 0) >= 9:
                comparison["analysis"]["speed_champions"].append(model_id)
            if stats.get("cost", 0) >= 9 or model.get("is_free", False):
                comparison["analysis"]["cost_efficient"].append(model_id)
            if stats.get("reasoning", 0) >= 9:
                comparison["analysis"]["reasoning_masters"].append(model_id)
            if stats.get("creativity", 0) >= 9:
                comparison["analysis"]["creative_powerhouses"].append(model_id)
            if stats.get("multimodal", 0) >= 9:
                comparison["analysis"]["multimodal_experts"].append(model_id)
            if stats.get("korean", 0) >= 8:
                comparison["analysis"]["korean_specialists"].append(model_id)
        
        return comparison
    
    def get_model_recommendations(self, use_case: str) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ì‚¬ë¡€ë³„ ëª¨ë¸ ì¶”ì²œ"""
        recommendations = {
            "general_chat": ["gemini-2.0-flash-exp", "gpt-4o-mini", "claude-3.5-sonnet"],
            "coding": ["gpt-4o", "claude-3.5-sonnet", "gemini-2.0-flash-exp"],
            "creative_writing": ["claude-3.5-sonnet", "claude-3-opus", "gpt-4o"],
            "research": ["perplexity-sonar", "gpt-4o", "claude-3.5-sonnet"],
            "analysis": ["gpt-4o", "claude-3-opus", "gemini-1.5-pro"],
            "cost_sensitive": ["gemini-2.0-flash-exp", "gpt-4o-mini"],
            "korean": ["gpt-4o", "claude-3.5-sonnet", "gemini-2.0-flash-exp"],
            "realtime_info": ["perplexity-sonar"],
            "multimodal": ["gemini-2.0-flash-exp", "gpt-4o", "gemini-1.5-pro"],
            "long_context": ["gemini-1.5-pro", "claude-3.5-sonnet", "gpt-4o"]
        }
        
        model_list = recommendations.get(use_case, ["gemini-2.0-flash-exp"])
        
        result = []
        for model_id in model_list:
            if model_id in self.models:
                model = self.models[model_id]
                result.append({
                    "model_id": model_id,
                    "name": model["name"],
                    "reason": f"ìµœì í™”ë¨: {use_case}",
                    "cost": model.get("cost_per_1k_input", 0),
                    "is_free": model.get("is_free", False),
                    "stats": model.get("stats", {})
                })
        
        return result
    
    async def run_comprehensive_research(self) -> Dict[str, Any]:
        """ì¢…í•© ì—°êµ¬ ì‹¤í–‰"""
        print("ğŸ”¬ HEAL7 AI ëª¨ë¸ ìƒì„¸ ì—°êµ¬ ì‹œì‘ (2025)")
        print("=" * 60)
        
        # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
        test_prompts = [
            "ì•ˆë…•í•˜ì„¸ìš”. ê°„ë‹¨í•œ ì¸ì‚¬ë¥¼ í•´ì£¼ì„¸ìš”.",
            "1+1ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "íŒŒì´ì¬ìœ¼ë¡œ í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì„ êµ¬í˜„í•´ì£¼ì„¸ìš”.",
            "ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ì–´ë–¤ê°€ìš”?",
            "ì°½ì˜ì ì¸ ì´ì•¼ê¸°ë¥¼ ì§§ê²Œ ì¨ì£¼ì„¸ìš”."
        ]
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í…ŒìŠ¤íŠ¸
        available_models = [mid for mid, model in self.models.items() 
                          if model.get("is_available", True) and model.get("api_key")]
        
        print(f"\nğŸ§ª ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ({len(available_models)}ê°œ ëª¨ë¸)")
        
        for model_id in available_models:
            print(f"\ní…ŒìŠ¤íŠ¸ ì¤‘: {self.models[model_id]['name']}")
            result = await self.test_model_performance(model_id, test_prompts[:2])  # ì‹œê°„ ì ˆì•½ì„ ìœ„í•´ 2ê°œë§Œ
            self.research_results[model_id] = result
            
            if result.get("successful_tests", 0) > 0:
                print(f"  âœ… ì„±ê³µë¥ : {result['successful_tests']}/{result['total_tests']}")
                print(f"  âš¡ í‰ê·  ì‘ë‹µì‹œê°„: {result['average_response_time']:.2f}ì´ˆ")
            else:
                print(f"  âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        
        # ëª¨ë¸ ë¹„êµ ì°¨íŠ¸ ìƒì„±
        print(f"\nğŸ“Š ëª¨ë¸ ë¹„êµ ë¶„ì„ ìƒì„± ì¤‘...")
        comparison = self.generate_model_comparison_chart()
        
        # ì‚¬ìš© ì‚¬ë¡€ë³„ ì¶”ì²œ
        print(f"\nğŸ’¡ ì‚¬ìš© ì‚¬ë¡€ë³„ ì¶”ì²œ ìƒì„± ì¤‘...")
        use_cases = ["general_chat", "coding", "creative_writing", "research", "cost_sensitive"]
        recommendations = {}
        
        for use_case in use_cases:
            recommendations[use_case] = self.get_model_recommendations(use_case)
        
        # ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
        final_report = {
            "timestamp": datetime.now().isoformat(),
            "total_models_analyzed": len(self.models),
            "available_models": len(available_models),
            "performance_tests": self.research_results,
            "model_comparison": comparison,
            "recommendations": recommendations,
            "summary": {
                "speed_champion": max(comparison["models"].items(), 
                                    key=lambda x: x[1]["stats"].get("speed", 0))[0] if comparison["models"] else None,
                "cost_champion": "gemini-2.0-flash-exp",  # ë¬´ë£Œì´ë¯€ë¡œ
                "reasoning_champion": max(comparison["models"].items(), 
                                        key=lambda x: x[1]["stats"].get("reasoning", 0))[0] if comparison["models"] else None,
                "creative_champion": max(comparison["models"].items(), 
                                       key=lambda x: x[1]["stats"].get("creativity", 0))[0] if comparison["models"] else None
            }
        }
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ† ì—°êµ¬ ê²°ê³¼ ìš”ì•½")
        print(f"{'ì¹´í…Œê³ ë¦¬':<15} {'ì±”í”¼ì–¸':<25} {'ì ìˆ˜'}")
        print("-" * 50)
        
        for category in ["speed", "cost", "reasoning", "creativity"]:
            champion = final_report["summary"].get(f"{category}_champion")
            if champion and champion in comparison["models"]:
                score = comparison["models"][champion]["stats"].get(category, 0)
                name = comparison["models"][champion]["name"]
                print(f"{category.upper():<15} {name:<25} {score}/10")
        
        # íŒŒì¼ ì €ì¥
        report_path = f"/home/ubuntu/logs/ai_model_research_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        os.makedirs(os.path.dirname(report_path), exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(final_report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ ìƒì„¸ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
        
        return final_report

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    researcher = AIModelResearcher()
    await researcher.run_comprehensive_research()

if __name__ == "__main__":
    asyncio.run(main())