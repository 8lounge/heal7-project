#!/usr/bin/env python3
"""
ì‚¬ì£¼ ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™” ì„¤ê³„
REDIS/PostgreSQL/SQLite/íŒŒì¼ì‹œìŠ¤í…œ í†µí•© ì•„í‚¤í…ì²˜

ìµœì í™” ì „ëµ:
1. ë©”ëª¨ë¦¬ ìºì‹œ (REDIS) - ìì£¼ ì¡°íšŒë˜ëŠ” ê³„ì‚° ê²°ê³¼
2. ê´€ê³„í˜• DB (PostgreSQL) - ì‚¬ìš©ì ë°ì´í„°, í†µê³„
3. ë¡œì»¬ DB (SQLite) - ì‚¬ìš©ëŸ‰ ì¶”ì , ì„ì‹œ ë°ì´í„°  
4. íŒŒì¼ ì‹œìŠ¤í…œ - ëª…ë¦¬í•™ ìƒìˆ˜, ë¡œì§ ë°©ì •ì‹
"""

import json
import sqlite3
import redis
from pathlib import Path
from typing import Dict, Any, Optional, List
from datetime import datetime, timedelta
import pickle
import hashlib
import logging

from .myeongrihak_constants import CHEONGAN, JIJI

logger = logging.getLogger(__name__)

class PerformanceOptimizer:
    """ë‹¤ì¸µ ìºì‹œ ë° ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        # ìºì‹œ ë ˆì´ì–´ ì„¤ì •
        self.redis_client = None
        self.sqlite_path = "/tmp/saju_cache.db"
        self.constants_path = "/home/ubuntu/project/backend/api/saju_system/constants"
        
        # ìºì‹œ TTL ì„¤ì •
        self.cache_ttl = {
            "saju_result": 86400,      # ì‚¬ì£¼ ê²°ê³¼: 24ì‹œê°„
            "lunar_conversion": 2592000, # ìŒë ¥ ë³€í™˜: 30ì¼
            "frequent_dates": 604800,   # ìì£¼ ì‚¬ìš© ë‚ ì§œ: 7ì¼  
            "system_stats": 3600       # ì‹œìŠ¤í…œ í†µê³„: 1ì‹œê°„
        }
        
        self._init_cache_systems()
        self._load_constants()
    
    def _init_cache_systems(self):
        """ìºì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        
        # Redis ì—°ê²° ì‹œë„
        try:
            self.redis_client = redis.Redis(
                host='localhost', 
                port=6379, 
                db=0, 
                decode_responses=True,
                socket_connect_timeout=5
            )
            self.redis_client.ping()
            logger.info("âœ… Redis ìºì‹œ ì—°ê²° ì„±ê³µ")
            self.redis_available = True
        except Exception as e:
            logger.warning(f"âš ï¸ Redis ì—°ê²° ì‹¤íŒ¨: {e}")
            self.redis_available = False
        
        # SQLite ë¡œì»¬ ìºì‹œ ì´ˆê¸°í™”
        try:
            with sqlite3.connect(self.sqlite_path) as conn:
                conn.execute('''
                    CREATE TABLE IF NOT EXISTS saju_cache (
                        cache_key TEXT PRIMARY KEY,
                        data BLOB NOT NULL,
                        created_at TEXT NOT NULL,
                        expires_at TEXT NOT NULL,
                        access_count INTEGER DEFAULT 1,
                        last_accessed TEXT NOT NULL
                    )
                ''')
                conn.execute('''
                    CREATE INDEX IF NOT EXISTS idx_expires_at ON saju_cache(expires_at)
                ''')
                conn.commit()
                logger.info("âœ… SQLite ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ SQLite ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _load_constants(self):
        """ëª…ë¦¬í•™ ìƒìˆ˜ íŒŒì¼ ë¡œë“œ"""
        
        constants_dir = Path(self.constants_path)
        constants_dir.mkdir(exist_ok=True)
        
        # ìƒìˆ˜ íŒŒì¼ë“¤ì´ ì—†ìœ¼ë©´ ìƒì„±
        self._ensure_constants_files()
        
        try:
            # 60ê°‘ì ë¡œë“œ
            with open(constants_dir / "gapja_60.json", "r", encoding="utf-8") as f:
                self.gapja_60 = json.load(f)
            
            # ì§€ì¥ê°„ ë¡œë“œ  
            with open(constants_dir / "jijanggan.json", "r", encoding="utf-8") as f:
                self.jijanggan_data = json.load(f)
            
            # ì‹œë‘ë²• ë¡œë“œ
            with open(constants_dir / "sidubeop.json", "r", encoding="utf-8") as f:
                self.sidubeop_data = json.load(f)
            
            # ê³„ì‚° ê³µì‹ ë¡œë“œ
            with open(constants_dir / "calculation_formulas.json", "r", encoding="utf-8") as f:
                self.formulas = json.load(f)
                
            logger.info("âœ… ëª…ë¦¬í•™ ìƒìˆ˜ íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ìƒìˆ˜ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self._fallback_to_hardcoded()
    
    def _ensure_constants_files(self):
        """ìƒìˆ˜ íŒŒì¼ë“¤ì´ ì—†ìœ¼ë©´ ìƒì„±"""
        
        constants_dir = Path(self.constants_path)
        
        # 60ê°‘ì ìƒì„±
        gapja_file = constants_dir / "gapja_60.json"
        if not gapja_file.exists():
            gapja_60 = []
            for i in range(60):
                cheongan = CHEONGAN[i % 10]
                jiji = JIJI[i % 12]
                gapja_60.append({
                    "index": i,
                    "gapja": cheongan + jiji,
                    "cheongan": cheongan,
                    "jiji": jiji,
                    "cheongan_index": i % 10,
                    "jiji_index": i % 12
                })
            
            with open(gapja_file, "w", encoding="utf-8") as f:
                json.dump(gapja_60, f, ensure_ascii=False, indent=2)
        
        # ì§€ì¥ê°„ ë°ì´í„° ìƒì„±
        jijanggan_file = constants_dir / "jijanggan.json"
        if not jijanggan_file.exists():
            from .myeongrihak_constants import JIJANGGAN
            
            jijanggan_formatted = {}
            for jiji, data in JIJANGGAN.items():
                jijanggan_formatted[jiji] = [
                    {"gan": gan, "ratio": ratio} for gan, ratio in data
                ]
            
            with open(jijanggan_file, "w", encoding="utf-8") as f:
                json.dump(jijanggan_formatted, f, ensure_ascii=False, indent=2)
        
        # ì‹œë‘ë²• ë°ì´í„° ìƒì„±
        sidubeop_file = constants_dir / "sidubeop.json"
        if not sidubeop_file.exists():
            # ì‹œë‘ë²• ê³µì‹ ë° ê·œì¹™
            sidubeop_data = {
                "description": "ì‹œë‘ë²• - ì¼ê°„ì— ë”°ë¥¸ ì‹œì²œê°„ ê²°ì •ë²•",
                "formula": "ê° ì¼ê°„ë³„ë¡œ ìì‹œë¶€í„° ì‹œì‘í•˜ëŠ” ì²œê°„ ìˆœì„œ ì •ì˜",
                "rules": {
                    "ç”²": ["ç”²", "ä¹™", "ä¸™", "ä¸", "æˆŠ", "å·±", "åºš", "è¾›", "å£¬", "ç™¸", "ç”²", "ä¹™"],
                    "å·±": ["ç”²", "ä¹™", "ä¸™", "ä¸", "æˆŠ", "å·±", "åºš", "è¾›", "å£¬", "ç™¸", "ç”²", "ä¹™"],
                    "ä¹™": ["ä¸™", "ä¸", "æˆŠ", "å·±", "åºš", "è¾›", "å£¬", "ç™¸", "ç”²", "ä¹™", "ä¸™", "ä¸"],
                    "åºš": ["ä¸™", "ä¸", "æˆŠ", "å·±", "åºš", "è¾›", "å£¬", "ç™¸", "ç”²", "ä¹™", "ä¸™", "ä¸"],
                    "ä¸™": ["æˆŠ", "å·±", "åºš", "è¾›", "å£¬", "ç™¸", "ç”²", "ä¹™", "ä¸™", "ä¸", "æˆŠ", "å·±"],
                    "è¾›": ["æˆŠ", "å·±", "åºš", "è¾›", "å£¬", "ç™¸", "ç”²", "ä¹™", "ä¸™", "ä¸", "æˆŠ", "å·±"],
                    "ä¸": ["åºš", "è¾›", "å£¬", "ç™¸", "ç”²", "ä¹™", "ä¸™", "ä¸", "æˆŠ", "å·±", "åºš", "è¾›"],
                    "å£¬": ["åºš", "è¾›", "å£¬", "ç™¸", "ç”²", "ä¹™", "ä¸™", "ä¸", "æˆŠ", "å·±", "åºš", "è¾›"],
                    "æˆŠ": ["å£¬", "ç™¸", "ç”²", "ä¹™", "ä¸™", "ä¸", "æˆŠ", "å·±", "åºš", "è¾›", "å£¬", "ç™¸"],
                    "ç™¸": ["å£¬", "ç™¸", "ç”²", "ä¹™", "ä¸™", "ä¸", "æˆŠ", "å·±", "åºš", "è¾›", "å£¬", "ç™¸"]
                },
                "time_mapping": {
                    "ìì‹œ": {"index": 0, "time_range": "23:30-01:30"},
                    "ì¶•ì‹œ": {"index": 1, "time_range": "01:30-03:30"},
                    "ì¸ì‹œ": {"index": 2, "time_range": "03:30-05:30"},
                    "ë¬˜ì‹œ": {"index": 3, "time_range": "05:30-07:30"},
                    "ì§„ì‹œ": {"index": 4, "time_range": "07:30-09:30"},
                    "ì‚¬ì‹œ": {"index": 5, "time_range": "09:30-11:30"},
                    "ì˜¤ì‹œ": {"index": 6, "time_range": "11:30-13:30"},
                    "ë¯¸ì‹œ": {"index": 7, "time_range": "13:30-15:30"},
                    "ì‹ ì‹œ": {"index": 8, "time_range": "15:30-17:30"},
                    "ìœ ì‹œ": {"index": 9, "time_range": "17:30-19:30"},
                    "ìˆ ì‹œ": {"index": 10, "time_range": "19:30-21:30"},
                    "í•´ì‹œ": {"index": 11, "time_range": "21:30-23:30"}
                }
            }
            
            with open(sidubeop_file, "w", encoding="utf-8") as f:
                json.dump(sidubeop_data, f, ensure_ascii=False, indent=2)
        
        # ê³„ì‚° ê³µì‹ ìƒì„±
        formulas_file = constants_dir / "calculation_formulas.json"
        if not formulas_file.exists():
            calculation_formulas = {
                "ì¼ì£¼_ê³„ì‚°": {
                    "description": "ì¼ì§„ ê³„ì‚° ê³µì‹",
                    "base_formula": "(ê¸°ì¤€ì¼_ê°‘ì_ì¸ë±ìŠ¤ + ë‚ ì§œì°¨ì´) % 60",
                    "reference_points": [
                        {
                            "date": "1985-02-24",
                            "gapja": "ç”²åˆ",
                            "index": 30,
                            "verified": "KASI_API"
                        },
                        {
                            "date": "1955-05-06", 
                            "gapja": "ä¸å¯",
                            "index": 3,
                            "verified": "KASI_API"
                        }
                    ],
                    "corrections": {
                        "solar_time": -32,
                        "description": "ì§„íƒœì–‘ì‹œ ë³´ì • (í•œêµ­ ê²½ë„ 127ë„ ê¸°ì¤€)"
                    }
                },
                "ì›”ì£¼_ê³„ì‚°": {
                    "description": "ì›”ì£¼ ê³„ì‚° - 24ì ˆê¸° ê¸°ì¤€",
                    "formula": "ì ˆê¸° ê¸°ì¤€ìœ¼ë¡œ ì›” ì²œê°„ ê²°ì •",
                    "base_rule": "ë…„ê°„ì— ë”°ë¥¸ ì›”ê°„ ì‹œì‘ì  ê²°ì •"
                },
                "ë…„ì£¼_ê³„ì‚°": {
                    "description": "ë…„ì£¼ ê³„ì‚° - ì…ì¶˜ ê¸°ì¤€",
                    "formula": "ì…ì¶˜ ì´ì „ì€ ì „ë…„ë„ ê°„ì§€ ì‚¬ìš©",
                    "base_year": "ì„œê¸° 4ë…„ì„ ê°‘ìë…„ìœ¼ë¡œ ì„¤ì •"
                },
                "ì‹œì£¼_ê³„ì‚°": {
                    "description": "ì‹œì£¼ ê³„ì‚° - ì‹œë‘ë²• ì ìš©",
                    "formula": "ì¼ê°„ì— ë”°ë¥¸ ì‹œì²œê°„ ë°°ì—´ + ì‹œì§€ ê²°ì •",
                    "time_correction": "ì§„íƒœì–‘ì‹œ ê¸°ì¤€ìœ¼ë¡œ ì‹œê°„ ë³´ì •"
                }
            }
            
            with open(formulas_file, "w", encoding="utf-8") as f:
                json.dump(calculation_formulas, f, ensure_ascii=False, indent=2)
    
    def _fallback_to_hardcoded(self):
        """íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ì‹œ í•˜ë“œì½”ë”© í´ë°±"""
        
        logger.warning("âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ - í•˜ë“œì½”ë”© ë°ì´í„° ì‚¬ìš©")
        
        # ê¸°ë³¸ 60ê°‘ì
        self.gapja_60 = []
        for i in range(60):
            cheongan = CHEONGAN[i % 10]
            jiji = JIJI[i % 12]
            self.gapja_60.append({
                "index": i,
                "gapja": cheongan + jiji,
                "cheongan": cheongan,
                "jiji": jiji
            })
    
    def generate_cache_key(self, **kwargs) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        
        # ì •ë ¬ëœ í‚¤-ê°’ìœ¼ë¡œ ì¼ê´€ëœ í•´ì‹œ ìƒì„±
        sorted_items = sorted(kwargs.items())
        data_string = json.dumps(sorted_items, sort_keys=True)
        return hashlib.md5(data_string.encode()).hexdigest()
    
    def get_from_cache(self, cache_key: str, cache_type: str = "saju_result") -> Optional[Any]:
        """ë‹¤ì¸µ ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ"""
        
        # 1ìˆœìœ„: Redis ìºì‹œ
        if self.redis_available:
            try:
                data = self.redis_client.get(f"saju:{cache_key}")
                if data:
                    logger.debug(f"ğŸ“¦ Redis ìºì‹œ íˆíŠ¸: {cache_key}")
                    return json.loads(data)
            except Exception as e:
                logger.warning(f"Redis ì¡°íšŒ ì˜¤ë¥˜: {e}")
        
        # 2ìˆœìœ„: SQLite ë¡œì»¬ ìºì‹œ
        try:
            with sqlite3.connect(self.sqlite_path) as conn:
                cursor = conn.execute('''
                    SELECT data, expires_at FROM saju_cache 
                    WHERE cache_key = ? AND expires_at > ?
                ''', (cache_key, datetime.now().isoformat()))
                
                row = cursor.fetchone()
                if row:
                    # ì ‘ê·¼ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
                    conn.execute('''
                        UPDATE saju_cache 
                        SET access_count = access_count + 1, last_accessed = ?
                        WHERE cache_key = ?
                    ''', (datetime.now().isoformat(), cache_key))
                    conn.commit()
                    
                    logger.debug(f"ğŸ—ƒï¸ SQLite ìºì‹œ íˆíŠ¸: {cache_key}")
                    return pickle.loads(row[0])
                    
        except Exception as e:
            logger.warning(f"SQLite ì¡°íšŒ ì˜¤ë¥˜: {e}")
        
        return None
    
    def set_to_cache(self, cache_key: str, data: Any, cache_type: str = "saju_result"):
        """ë‹¤ì¸µ ìºì‹œì— ë°ì´í„° ì €ì¥"""
        
        ttl = self.cache_ttl.get(cache_type, 3600)
        expires_at = datetime.now() + timedelta(seconds=ttl)
        
        # Redis ì €ì¥
        if self.redis_available:
            try:
                self.redis_client.setex(
                    f"saju:{cache_key}",
                    ttl,
                    json.dumps(data, default=str)
                )
                logger.debug(f"ğŸ’¾ Redis ìºì‹œ ì €ì¥: {cache_key}")
            except Exception as e:
                logger.warning(f"Redis ì €ì¥ ì˜¤ë¥˜: {e}")
        
        # SQLite ì €ì¥
        try:
            with sqlite3.connect(self.sqlite_path) as conn:
                conn.execute('''
                    INSERT OR REPLACE INTO saju_cache
                    (cache_key, data, created_at, expires_at, last_accessed)
                    VALUES (?, ?, ?, ?, ?)
                ''', (
                    cache_key,
                    pickle.dumps(data),
                    datetime.now().isoformat(),
                    expires_at.isoformat(),
                    datetime.now().isoformat()
                ))
                conn.commit()
                logger.debug(f"ğŸ—ƒï¸ SQLite ìºì‹œ ì €ì¥: {cache_key}")
        except Exception as e:
            logger.warning(f"SQLite ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def cleanup_expired_cache(self):
        """ë§Œë£Œëœ ìºì‹œ ì •ë¦¬"""
        
        try:
            with sqlite3.connect(self.sqlite_path) as conn:
                cursor = conn.execute('DELETE FROM saju_cache WHERE expires_at < ?', 
                                    (datetime.now().isoformat(),))
                deleted = cursor.rowcount
                conn.commit()
                
                if deleted > 0:
                    logger.info(f"ğŸ§¹ ë§Œë£Œëœ ìºì‹œ {deleted}ê°œ ì •ë¦¬ ì™„ë£Œ")
                    
        except Exception as e:
            logger.error(f"ìºì‹œ ì •ë¦¬ ì˜¤ë¥˜: {e}")
    
    def get_cache_statistics(self) -> Dict[str, Any]:
        """ìºì‹œ ì‚¬ìš© í†µê³„"""
        
        stats = {
            "redis_status": "connected" if self.redis_available else "disconnected",
            "sqlite_status": "available",
            "cache_counts": {},
            "hit_rates": {},
            "storage_usage": {}
        }
        
        # SQLite í†µê³„
        try:
            with sqlite3.connect(self.sqlite_path) as conn:
                # ì „ì²´ ìºì‹œ ìˆ˜
                cursor = conn.execute('SELECT COUNT(*) FROM saju_cache')
                stats["cache_counts"]["total"] = cursor.fetchone()[0]
                
                # ìœ íš¨í•œ ìºì‹œ ìˆ˜
                cursor = conn.execute('SELECT COUNT(*) FROM saju_cache WHERE expires_at > ?', 
                                    (datetime.now().isoformat(),))
                stats["cache_counts"]["valid"] = cursor.fetchone()[0]
                
                # ìì£¼ ì‚¬ìš©ë˜ëŠ” ìºì‹œ
                cursor = conn.execute('''
                    SELECT cache_key, access_count 
                    FROM saju_cache 
                    WHERE expires_at > ? 
                    ORDER BY access_count DESC 
                    LIMIT 10
                ''', (datetime.now().isoformat(),))
                stats["frequent_keys"] = cursor.fetchall()
                
        except Exception as e:
            logger.error(f"í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        
        return stats
    
    def optimize_performance(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìµœì í™” ì‹¤í–‰"""
        
        optimization_results = {
            "cleanup_performed": False,
            "cache_reorganized": False,
            "constants_refreshed": False,
            "recommendations": []
        }
        
        # 1. ë§Œë£Œëœ ìºì‹œ ì •ë¦¬
        self.cleanup_expired_cache()
        optimization_results["cleanup_performed"] = True
        
        # 2. ìƒìˆ˜ íŒŒì¼ ê°±ì‹  í™•ì¸
        try:
            self._load_constants()
            optimization_results["constants_refreshed"] = True
        except Exception as e:
            logger.error(f"ìƒìˆ˜ ê°±ì‹  ì‹¤íŒ¨: {e}")
        
        # 3. ì„±ëŠ¥ ê¶Œì¥ì‚¬í•­ ìƒì„±
        stats = self.get_cache_statistics()
        
        if not self.redis_available:
            optimization_results["recommendations"].append(
                "Redis ì„œë²„ ì„¤ì¹˜ë¡œ ë©”ëª¨ë¦¬ ìºì‹œ ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥"
            )
        
        if stats["cache_counts"].get("valid", 0) < 100:
            optimization_results["recommendations"].append(
                "ìºì‹œ ì‚¬ìš©ëŸ‰ì´ ì ìŒ - ë” ì ê·¹ì ì¸ ìºì‹± ì „ëµ ê³ ë ¤"
            )
        
        return optimization_results


# ì „ì—­ ìµœì í™” ì¸ìŠ¤í„´ìŠ¤
performance_optimizer = PerformanceOptimizer()

def get_optimized_cache(cache_key: str, cache_type: str = "saju_result") -> Optional[Any]:
    """ìµœì í™”ëœ ìºì‹œ ì¡°íšŒ"""
    return performance_optimizer.get_from_cache(cache_key, cache_type)

def set_optimized_cache(cache_key: str, data: Any, cache_type: str = "saju_result"):
    """ìµœì í™”ëœ ìºì‹œ ì €ì¥"""
    performance_optimizer.set_to_cache(cache_key, data, cache_type)

def generate_saju_cache_key(year: int, month: int, day: int, hour: int, minute: int, 
                           is_lunar: bool = False) -> str:
    """ì‚¬ì£¼ ê³„ì‚°ìš© ìºì‹œ í‚¤ ìƒì„±"""
    return performance_optimizer.generate_cache_key(
        year=year, month=month, day=day, hour=hour, minute=minute, is_lunar=is_lunar
    )


if __name__ == "__main__":
    print("ğŸš€ ì‚¬ì£¼ ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™” ë„êµ¬")
    print("=" * 50)
    
    optimizer = PerformanceOptimizer()
    
    # ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
    stats = optimizer.get_cache_statistics()
    print(f"Redis ìƒíƒœ: {stats['redis_status']}")
    print(f"ìºì‹œ ìˆ˜: {stats['cache_counts'].get('valid', 0)}/{stats['cache_counts'].get('total', 0)}")
    
    # ìµœì í™” ì‹¤í–‰
    results = optimizer.optimize_performance()
    print(f"\nìµœì í™” ê²°ê³¼:")
    for key, value in results.items():
        if key != "recommendations":
            print(f"  {key}: {'âœ…' if value else 'âŒ'}")
    
    if results["recommendations"]:
        print(f"\nê¶Œì¥ì‚¬í•­:")
        for rec in results["recommendations"]:
            print(f"  - {rec}")