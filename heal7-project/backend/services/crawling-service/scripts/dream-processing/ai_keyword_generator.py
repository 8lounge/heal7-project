#!/usr/bin/env python3
"""
ğŸ¤– AI ê¸°ë°˜ ê¿ˆí’€ì´ í‚¤ì›Œë“œ ìƒì„± ë° í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ
- OpenAI GPT-4, Anthropic Claude, Google Gemini í†µí•©
- í•œêµ­ ì „í†µ ê¿ˆí•´ëª½ ì „ë¬¸ ì§€ì‹ í™œìš©
- ìë™ í’ˆì§ˆ ê²€ì¦ ë° ì¤‘ë³µ ì œê±°
- ë‹¤ì¤‘ í•´ì„ ìƒì„± (ì „í†µ/í˜„ëŒ€/ì‹¬ë¦¬í•™ì )
"""

import os
import json
import logging
import asyncio
import aiohttp
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
import time
import random
from datetime import datetime
import subprocess
import re

@dataclass
class AIKeywordRequest:
    """AI í‚¤ì›Œë“œ ìƒì„± ìš”ì²­"""
    category: str
    base_keywords: List[str]
    count: int
    style: str = "traditional"  # traditional, modern, creative
    quality_threshold: float = 8.0

@dataclass
class GeneratedKeyword:
    """ìƒì„±ëœ í‚¤ì›Œë“œ"""
    keyword: str
    category: str
    traditional_interpretation: str
    modern_interpretation: str
    psychological_interpretation: Optional[str] = None
    related_keywords: List[str] = None
    quality_score: float = 8.0
    confidence: float = 0.8
    source: str = "ai"

class AIKeywordGenerator:
    """AI ê¸°ë°˜ í‚¤ì›Œë“œ ìƒì„±ê¸°"""
    
    def __init__(self):
        self.logger = self._setup_logger()
        self.ai_clients = self._initialize_ai_clients()
        self.korean_dream_context = self._load_korean_dream_context()
        
    def _setup_logger(self):
        """ë¡œê±° ì„¤ì •"""
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.FileHandler('/home/ubuntu/logs/ai_keyword_generator.log')
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            
            console = logging.StreamHandler()
            console.setFormatter(formatter)
            logger.addHandler(console)
        
        return logger
    
    def _initialize_ai_clients(self) -> Dict:
        """AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        clients = {}
        
        # API í‚¤ ë¡œë“œ
        env_path = "/home/ubuntu/.env.ai"
        if os.path.exists(env_path):
            with open(env_path, 'r') as f:
                for line in f:
                    if '=' in line and not line.startswith('#'):
                        key, value = line.strip().split('=', 1)
                        os.environ[key] = value
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸
        if os.getenv('OPENAI_API_KEY'):
            clients['openai'] = {
                'api_key': os.getenv('OPENAI_API_KEY'),
                'model': 'gpt-4',
                'endpoint': 'https://api.openai.com/v1/chat/completions'
            }
            self.logger.info("âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # Anthropic Claude í´ë¼ì´ì–¸íŠ¸  
        if os.getenv('ANTHROPIC_API_KEY'):
            clients['anthropic'] = {
                'api_key': os.getenv('ANTHROPIC_API_KEY'),
                'model': 'claude-3-sonnet-20240229',
                'endpoint': 'https://api.anthropic.com/v1/messages'
            }
            self.logger.info("âœ… Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # Google Gemini í´ë¼ì´ì–¸íŠ¸
        if os.getenv('GEMINI_API_KEY'):
            clients['gemini'] = {
                'api_key': os.getenv('GEMINI_API_KEY'),
                'model': 'gemini-pro',
                'endpoint': f'https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent'
            }
            self.logger.info("âœ… Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        
        if not clients:
            self.logger.error("âŒ AI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        return clients
    
    def _load_korean_dream_context(self) -> str:
        """í•œêµ­ ì „í†µ ê¿ˆí’€ì´ ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ"""
        return """
        ë‹¹ì‹ ì€ í•œêµ­ ì „í†µ ê¿ˆí•´ëª½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ í‚¤ì›Œë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:

        ## í•œêµ­ ì „í†µ ê¿ˆí’€ì´ ì›ë¦¬
        1. ê¿ˆì€ í˜„ì‹¤ì˜ ë°˜ëŒ€: ì¢‹ì€ ê¿ˆì´ ë‚˜ìœ ê²°ê³¼ë¥¼, ë‚˜ìœ ê¿ˆì´ ì¢‹ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜´
        2. ìƒì§•ê³¼ ì€ìœ : ì§ì ‘ì  í‘œí˜„ë³´ë‹¤ ìƒì§•ì  ì˜ë¯¸ê°€ ì¤‘ìš”
        3. ìŒì–‘ì˜¤í–‰ ì‚¬ìƒ: ë¬¼(æ°´), ë¶ˆ(ç«), ë‚˜ë¬´(æœ¨), ê¸ˆ(é‡‘), í™(åœŸ)ì˜ ì¡°í™”
        4. ì‹­ì´ì§€ì‹ : ì¥, ì†Œ, í˜¸ë‘ì´, í† ë¼, ìš©, ë±€, ë§, ì–‘, ì›ìˆ­ì´, ë‹­, ê°œ, ë¼ì§€

        ## ê¿ˆí’€ì´ í•´ì„ ë°©ì‹
        - ì „í†µì  í•´ì„: ì¡°ìƒ ëŒ€ëŒ€ë¡œ ì „í•´ì˜¤ëŠ” ë¯¼ì† ì§€í˜œ
        - í˜„ëŒ€ì  í•´ì„: ì‹¬ë¦¬í•™ì , ê³¼í•™ì  ê´€ì 
        - ìƒí™©ë³„ í•´ì„: ê°™ì€ ê¿ˆì´ë¼ë„ ê¿ˆê¾¸ëŠ” ì‚¬ëŒì˜ ìƒí™©ì— ë”°ë¼ ë‹¬ë¼ì§

        ## í‚¤ì›Œë“œ ìƒì„± ê·œì¹™
        1. í•œêµ­ì–´ ê³ ìœ  í‘œí˜„ ìš°ì„  ì‚¬ìš©
        2. ì¼ìƒìƒí™œì—ì„œ ìì£¼ ê¿ˆê¾¸ëŠ” ë‚´ìš© í¬í•¨
        3. ë¬¸í™”ì  íŠ¹ìˆ˜ì„± ë°˜ì˜ (ì œì‚¬, ì°¨ë¡€, íš¨ë„ ë“±)
        4. ê³„ì ˆê°ê³¼ ì ˆê¸° ê³ ë ¤ (ë´„ê¿ˆ, ê°€ì„ê¿ˆ ë“±)
        """
    
    async def generate_keywords_with_ai(self, request: AIKeywordRequest) -> List[GeneratedKeyword]:
        """AIë¥¼ í™œìš©í•œ í‚¤ì›Œë“œ ìƒì„±"""
        results = []
        
        # ì—¬ëŸ¬ AI ëª¨ë¸ì—ì„œ í‚¤ì›Œë“œ ìƒì„±
        for ai_name, client_info in self.ai_clients.items():
            try:
                keywords = await self._generate_with_specific_ai(ai_name, request)
                results.extend(keywords)
                self.logger.info(f"âœ… {ai_name}ì—ì„œ {len(keywords)}ê°œ í‚¤ì›Œë“œ ìƒì„±")
            except Exception as e:
                self.logger.error(f"âŒ {ai_name} í‚¤ì›Œë“œ ìƒì„± ì‹¤íŒ¨: {e}")
        
        # í’ˆì§ˆ ê²€ì¦ ë° ì¤‘ë³µ ì œê±°
        validated_keywords = self._validate_and_deduplicate(results, request.quality_threshold)
        
        return validated_keywords[:request.count]
    
    async def _generate_with_specific_ai(self, ai_name: str, request: AIKeywordRequest) -> List[GeneratedKeyword]:
        """íŠ¹ì • AIë¡œ í‚¤ì›Œë“œ ìƒì„±"""
        prompt = self._create_prompt(request)
        
        if ai_name == 'openai':
            return await self._generate_with_openai(prompt, request)
        elif ai_name == 'anthropic':
            return await self._generate_with_anthropic(prompt, request)
        elif ai_name == 'gemini':
            return await self._generate_with_gemini(prompt, request)
        else:
            return []
    
    def _create_prompt(self, request: AIKeywordRequest) -> str:
        """AIìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        base_keywords_str = ", ".join(request.base_keywords)
        
        return f"""
        {self.korean_dream_context}

        ## ìš”ì²­ ì‚¬í•­
        ì¹´í…Œê³ ë¦¬: {request.category}
        ê¸°ì¡´ í‚¤ì›Œë“œ: {base_keywords_str}
        ìƒì„± ê°œìˆ˜: {request.count}
        ìŠ¤íƒ€ì¼: {request.style}

        ## ì¶œë ¥ í˜•ì‹ (JSON)
        ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ {request.count}ê°œì˜ ìƒˆë¡œìš´ ê¿ˆí’€ì´ í‚¤ì›Œë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:

        {{
            "keywords": [
                {{
                    "keyword": "ìƒì„±ëœ í‚¤ì›Œë“œ",
                    "traditional_interpretation": "ì „í†µì  í•´ì„ (2-3ë¬¸ì¥)",
                    "modern_interpretation": "í˜„ëŒ€ì  í•´ì„ (2-3ë¬¸ì¥)",
                    "psychological_interpretation": "ì‹¬ë¦¬í•™ì  í•´ì„ (1-2ë¬¸ì¥, ì„ íƒì )",
                    "related_keywords": ["ê´€ë ¨í‚¤ì›Œë“œ1", "ê´€ë ¨í‚¤ì›Œë“œ2", "ê´€ë ¨í‚¤ì›Œë“œ3"],
                    "quality_score": 8.5
                }}
            ]
        }}

        ## ìƒì„± ì¡°ê±´
        1. ê¸°ì¡´ í‚¤ì›Œë“œì™€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ìƒˆë¡œìš´ í‚¤ì›Œë“œ ìƒì„±
        2. í•œêµ­ ë¬¸í™”ì™€ ì „í†µì— ë§ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ ì‚¬ìš©
        3. ì‹¤ì œë¡œ ì‚¬ëŒë“¤ì´ ê¿ˆê¿€ ìˆ˜ ìˆëŠ” í˜„ì‹¤ì ì¸ ë‚´ìš©
        4. ì „í†µì  í•´ì„ê³¼ í˜„ëŒ€ì  í•´ì„ì˜ ê· í˜• ìœ ì§€
        5. í’ˆì§ˆ ì ìˆ˜ 8.0 ì´ìƒì˜ ê³ í’ˆì§ˆ í‚¤ì›Œë“œ
        """
    
    async def _generate_with_openai(self, prompt: str, request: AIKeywordRequest) -> List[GeneratedKeyword]:
        """OpenAI GPT-4ë¡œ í‚¤ì›Œë“œ ìƒì„±"""
        client_info = self.ai_clients['openai']
        
        payload = {
            "model": client_info['model'],
            "messages": [
                {"role": "system", "content": "ë‹¹ì‹ ì€ í•œêµ­ ì „í†µ ê¿ˆí•´ëª½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.8,
            "max_tokens": 2000
        }
        
        headers = {
            "Authorization": f"Bearer {client_info['api_key']}",
            "Content-Type": "application/json"
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(client_info['endpoint'], 
                                   json=payload, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    content = data['choices'][0]['message']['content']
                    return self._parse_ai_response(content, request.category, 'openai')
                else:
                    raise Exception(f"OpenAI API ì˜¤ë¥˜: {response.status}")
    
    async def _generate_with_anthropic(self, prompt: str, request: AIKeywordRequest) -> List[GeneratedKeyword]:
        """Anthropic Claudeë¡œ í‚¤ì›Œë“œ ìƒì„±"""
        client_info = self.ai_clients['anthropic']
        
        payload = {
            "model": client_info['model'],
            "max_tokens": 2000,
            "messages": [
                {"role": "user", "content": prompt}
            ]
        }
        
        headers = {
            "x-api-key": client_info['api_key'],
            "Content-Type": "application/json",
            "anthropic-version": "2023-06-01"
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(client_info['endpoint'], 
                                   json=payload, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    content = data['content'][0]['text']
                    return self._parse_ai_response(content, request.category, 'anthropic')
                else:
                    raise Exception(f"Anthropic API ì˜¤ë¥˜: {response.status}")
    
    async def _generate_with_gemini(self, prompt: str, request: AIKeywordRequest) -> List[GeneratedKeyword]:
        """Google Geminië¡œ í‚¤ì›Œë“œ ìƒì„±"""
        client_info = self.ai_clients['gemini']
        
        payload = {
            "contents": [{
                "parts": [{"text": prompt}]
            }]
        }
        
        params = {"key": client_info['api_key']}
        
        async with aiohttp.ClientSession() as session:
            async with session.post(client_info['endpoint'], 
                                   json=payload, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    content = data['candidates'][0]['content']['parts'][0]['text']
                    return self._parse_ai_response(content, request.category, 'gemini')
                else:
                    raise Exception(f"Gemini API ì˜¤ë¥˜: {response.status}")
    
    def _parse_ai_response(self, content: str, category: str, source: str) -> List[GeneratedKeyword]:
        """AI ì‘ë‹µ íŒŒì‹±"""
        keywords = []
        
        try:
            # JSON ì¶”ì¶œ
            json_match = re.search(r'```json\s*(.*?)\s*```', content, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # JSON ë§ˆì»¤ê°€ ì—†ìœ¼ë©´ ì „ì²´ ë‚´ìš©ì—ì„œ JSON ì°¾ê¸°
                json_str = content
            
            # JSON íŒŒì‹±
            data = json.loads(json_str)
            
            for item in data.get('keywords', []):
                keyword = GeneratedKeyword(
                    keyword=item['keyword'],
                    category=category,
                    traditional_interpretation=item['traditional_interpretation'],
                    modern_interpretation=item['modern_interpretation'],
                    psychological_interpretation=item.get('psychological_interpretation'),
                    related_keywords=item.get('related_keywords', []),
                    quality_score=item.get('quality_score', 8.0),
                    source=source
                )
                keywords.append(keyword)
                
        except Exception as e:
            self.logger.error(f"AI ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜ ({source}): {e}")
            self.logger.debug(f"ì‘ë‹µ ë‚´ìš©: {content[:500]}...")
        
        return keywords
    
    def _validate_and_deduplicate(self, keywords: List[GeneratedKeyword], 
                                 quality_threshold: float) -> List[GeneratedKeyword]:
        """í’ˆì§ˆ ê²€ì¦ ë° ì¤‘ë³µ ì œê±°"""
        validated = []
        seen_keywords = set()
        
        for keyword in keywords:
            # í’ˆì§ˆ ì ìˆ˜ ê²€ì¦
            if keyword.quality_score < quality_threshold:
                continue
            
            # ì¤‘ë³µ ì œê±°
            normalized_keyword = keyword.keyword.strip().lower()
            if normalized_keyword in seen_keywords:
                continue
            
            # í‚¤ì›Œë“œ ìœ íš¨ì„± ê²€ì¦
            if not self._is_valid_korean_keyword(keyword.keyword):
                continue
            
            # í•´ì„ í’ˆì§ˆ ê²€ì¦
            if not self._validate_interpretations(keyword):
                continue
            
            seen_keywords.add(normalized_keyword)
            validated.append(keyword)
        
        # í’ˆì§ˆ ì ìˆ˜ìˆœ ì •ë ¬
        validated.sort(key=lambda x: x.quality_score, reverse=True)
        
        self.logger.info(f"í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ: {len(keywords)}ê°œ â†’ {len(validated)}ê°œ")
        return validated
    
    def _is_valid_korean_keyword(self, keyword: str) -> bool:
        """í•œêµ­ì–´ í‚¤ì›Œë“œ ìœ íš¨ì„± ê²€ì¦"""
        if not keyword or len(keyword) < 2 or len(keyword) > 20:
            return False
        
        # í•œê¸€ í¬í•¨ ì—¬ë¶€ í™•ì¸
        korean_pattern = re.compile(r'[ê°€-í£]')
        if not korean_pattern.search(keyword):
            return False
        
        # ë¶€ì ì ˆí•œ ë¬¸ì ì œì™¸
        invalid_chars = ['!', '@', '#', '$', '%', '^', '&', '*']
        if any(char in keyword for char in invalid_chars):
            return False
        
        return True
    
    def _validate_interpretations(self, keyword: GeneratedKeyword) -> bool:
        """í•´ì„ í’ˆì§ˆ ê²€ì¦"""
        # í•´ì„ ê¸¸ì´ ê²€ì¦
        if len(keyword.traditional_interpretation) < 10:
            return False
        if len(keyword.modern_interpretation) < 10:
            return False
        
        # í•´ì„ ë‚´ìš© ê²€ì¦ (ê¸°ë³¸ì ì¸ í’ˆì§ˆ ì²´í¬)
        required_words = ['ê¿ˆ', 'ì˜ë¯¸', 'ìƒì§•', 'ë‚˜íƒ€']
        traditional_check = any(word in keyword.traditional_interpretation for word in required_words)
        modern_check = any(word in keyword.modern_interpretation for word in required_words)
        
        return traditional_check and modern_check
    
    async def batch_generate_keywords(self, requests: List[AIKeywordRequest]) -> Dict[str, List[GeneratedKeyword]]:
        """ë°°ì¹˜ í‚¤ì›Œë“œ ìƒì„±"""
        results = {}
        
        for request in requests:
            self.logger.info(f"ğŸ¤– {request.category} ì¹´í…Œê³ ë¦¬ AI í‚¤ì›Œë“œ ìƒì„± ì‹œì‘ ({request.count}ê°œ)")
            keywords = await self.generate_keywords_with_ai(request)
            results[request.category] = keywords
            
            # API í˜¸ì¶œ ì œí•œ ê³ ë ¤ (1ì´ˆ ëŒ€ê¸°)
            await asyncio.sleep(1)
        
        return results
    
    def save_generated_keywords_to_db(self, keywords: List[GeneratedKeyword]) -> int:
        """ìƒì„±ëœ í‚¤ì›Œë“œë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        saved_count = 0
        
        for keyword in keywords:
            try:
                # í‚¤ì›Œë“œ ì‚½ì…
                insert_cmd = [
                    'sudo', '-u', 'postgres', 'psql', '-d', 'dream_service', '-c',
                    f"""
                    INSERT INTO dream_keywords 
                    (keyword, keyword_normalized, category_id, quality_score, status)
                    VALUES ('{keyword.keyword.replace("'", "''")}', 
                            '{keyword.keyword.lower().replace("'", "''")}', 
                            '{keyword.category}', {keyword.quality_score}, 'active')
                    ON CONFLICT (keyword, category_id) DO NOTHING
                    RETURNING id;
                    """
                ]
                
                result = subprocess.run(insert_cmd, capture_output=True, text=True)
                if result.returncode == 0 and result.stdout.strip():
                    keyword_id = result.stdout.strip()
                    
                    # í•´ì„ë“¤ ì‚½ì…
                    self._insert_interpretations(keyword_id, keyword)
                    saved_count += 1
                
            except Exception as e:
                self.logger.error(f"í‚¤ì›Œë“œ ì €ì¥ ì˜¤ë¥˜: {keyword.keyword} - {e}")
        
        self.logger.info(f"âœ… {saved_count}ê°œ í‚¤ì›Œë“œ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ")
        return saved_count
    
    def _insert_interpretations(self, keyword_id: str, keyword: GeneratedKeyword):
        """í•´ì„ ë°ì´í„° ì‚½ì…"""
        interpretations = [
            ('traditional', keyword.traditional_interpretation, 'positive'),
            ('modern', keyword.modern_interpretation, 'neutral')
        ]
        
        if keyword.psychological_interpretation:
            interpretations.append(('psychological', keyword.psychological_interpretation, 'neutral'))
        
        for interp_type, text, sentiment in interpretations:
            cmd = [
                'sudo', '-u', 'postgres', 'psql', '-d', 'dream_service', '-c',
                f"""
                INSERT INTO dream_interpretations 
                (keyword_id, interpretation_type, interpretation_text, sentiment, confidence_score)
                VALUES ({keyword_id}, '{interp_type}', '{text.replace("'", "''")}', 
                        '{sentiment}', {keyword.confidence});
                """
            ]
            subprocess.run(cmd, capture_output=True)

# ì‹¤í–‰ í•¨ìˆ˜
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    generator = AIKeywordGenerator()
    
    if not generator.ai_clients:
        print("âŒ AI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    # ìƒ˜í”Œ ìš”ì²­ ìƒì„±
    requests = [
        AIKeywordRequest(
            category='water',
            base_keywords=['ë¬¼', 'ë°”ë‹¤', 'ê°•'],
            count=20,
            style='traditional'
        ),
        AIKeywordRequest(
            category='zodiac_animals',
            base_keywords=['ìš©', 'í˜¸ë‘ì´', 'ë±€'],
            count=15,
            style='creative'
        )
    ]
    
    # AI í‚¤ì›Œë“œ ìƒì„±
    results = await generator.batch_generate_keywords(requests)
    
    # ê²°ê³¼ ì¶œë ¥
    total_generated = 0
    for category, keywords in results.items():
        print(f"\nğŸ“‚ {category} ì¹´í…Œê³ ë¦¬: {len(keywords)}ê°œ ìƒì„±")
        total_generated += len(keywords)
        
        for keyword in keywords[:3]:  # ì²˜ìŒ 3ê°œë§Œ ìƒ˜í”Œ ì¶œë ¥
            print(f"  ğŸ”¸ {keyword.keyword} (í’ˆì§ˆ: {keyword.quality_score})")
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
    all_keywords = []
    for keywords in results.values():
        all_keywords.extend(keywords)
    
    if all_keywords:
        saved_count = generator.save_generated_keywords_to_db(all_keywords)
        print(f"\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥: {saved_count}/{len(all_keywords)}ê°œ ì„±ê³µ")

if __name__ == "__main__":
    asyncio.run(main())