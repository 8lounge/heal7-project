#!/usr/bin/env python3
"""
ğŸ§  ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ëŸ¬ - ì§€ëŠ¥í˜• 3-Tier í†µí•© ì‹œìŠ¤í…œ
ìë™ í´ë°± ë° ìµœì  í¬ë¡¤ëŸ¬ ì„ íƒ

Features:
- ìë™ í¬ë¡¤ëŸ¬ ì„ íƒ
- ìˆœì°¨ í´ë°± ì‹œìŠ¤í…œ
- ì„±ëŠ¥ ê¸°ë°˜ í•™ìŠµ
- ë°°ì¹˜ ì²˜ë¦¬
- ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

Author: HEAL7 Development Team
Version: 1.0.0
Date: 2025-08-30
"""

import asyncio
import logging
import time
import statistics
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict, deque
from urllib.parse import urlparse

from .crawlers import (
    BaseCrawler, CrawlResult, CrawlConfig, CrawlerType,
    HttpxCrawler, PlaywrightCrawler, SeleniumCrawler
)


logger = logging.getLogger(__name__)


class CrawlStrategy(Enum):
    """í¬ë¡¤ë§ ì „ëµ"""
    AUTO = "auto"           # ìë™ ì„ íƒ
    FAST = "fast"          # ì†ë„ ìš°ì„  (httpx â†’ playwright â†’ selenium)
    RENDER = "render"      # ë Œë”ë§ ìš°ì„  (playwright â†’ httpx â†’ selenium)  
    STEALTH = "stealth"    # ìŠ¤í…”ìŠ¤ ìš°ì„  (selenium â†’ playwright â†’ httpx)
    SAFE = "safe"          # ì•ˆì •ì„± ìš°ì„  (ëª¨ë“  í¬ë¡¤ëŸ¬ ì‹œë„)


@dataclass
class CrawlerStats:
    """í¬ë¡¤ëŸ¬ í†µê³„"""
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    average_response_time: float = 0.0
    recent_response_times: deque = field(default_factory=lambda: deque(maxlen=10))
    error_types: Dict[str, int] = field(default_factory=dict)
    last_used: Optional[float] = None
    
    def update(self, result: CrawlResult):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        self.total_requests += 1
        self.last_used = time.time()
        
        if result.success:
            self.successful_requests += 1
        else:
            self.failed_requests += 1
            if result.error_type:
                self.error_types[result.error_type] = self.error_types.get(result.error_type, 0) + 1
        
        if result.response_time:
            self.recent_response_times.append(result.response_time)
            self.average_response_time = statistics.mean(self.recent_response_times)
    
    @property
    def success_rate(self) -> float:
        """ì„±ê³µë¥ """
        if self.total_requests == 0:
            return 0.0
        return self.successful_requests / self.total_requests
    
    @property
    def reliability_score(self) -> float:
        """ì‹ ë¢°ë„ ì ìˆ˜ (ì„±ê³µë¥  + ì†ë„ ê³ ë ¤)"""
        if self.total_requests < 3:  # ìµœì†Œ 3ë²ˆì˜ ì‹œë„ í•„ìš”
            return 0.5
        
        # ì„±ê³µë¥  (70%) + ì†ë„ ì ìˆ˜ (30%)
        speed_score = max(0, 1 - (self.average_response_time / 30))  # 30ì´ˆë¥¼ ê¸°ì¤€
        return (self.success_rate * 0.7) + (speed_score * 0.3)


class SmartCrawler:
    """ğŸ§  ì§€ëŠ¥í˜• 3-Tier í¬ë¡¤ëŸ¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.name = "smart_crawler"
        self.logger = logging.getLogger(f"{__name__}.SmartCrawler")
        
        # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ë“¤
        self.crawlers: Dict[CrawlerType, BaseCrawler] = {}
        self.crawler_stats: Dict[CrawlerType, CrawlerStats] = {
            CrawlerType.HTTPX: CrawlerStats(),
            CrawlerType.PLAYWRIGHT: CrawlerStats(), 
            CrawlerType.SELENIUM: CrawlerStats()
        }
        
        # ë„ë©”ì¸ë³„ ìµœì  í¬ë¡¤ëŸ¬ ìºì‹œ
        self.domain_preferences: Dict[str, CrawlerType] = {}
        
        # ì „ì—­ ì„¤ì •
        self.max_retries = 2
        self.retry_delay = 1.0
        self.is_initialized = False
        
        # ì„±ëŠ¥ ì„ê³„ê°’
        self.performance_thresholds = {
            'min_success_rate': 0.7,
            'max_response_time': 30.0,
            'min_reliability_score': 0.6
        }
    
    async def initialize(self):
        """ëª¨ë“  í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        if self.is_initialized:
            return
        
        self.logger.info("ğŸ§  Smart Crawler ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘")
        
        try:
            # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            self.crawlers = {
                CrawlerType.HTTPX: HttpxCrawler(),
                CrawlerType.PLAYWRIGHT: PlaywrightCrawler(headless=True),
                CrawlerType.SELENIUM: SeleniumCrawler(use_undetected=True, headless=True)
            }
            
            # ë³‘ë ¬ ì´ˆê¸°í™”
            init_tasks = []
            for crawler_type, crawler in self.crawlers.items():
                init_tasks.append(self._safe_initialize_crawler(crawler_type, crawler))
            
            await asyncio.gather(*init_tasks)
            
            self.is_initialized = True
            self.logger.info("âœ… Smart Crawler ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ Smart Crawler ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def _safe_initialize_crawler(self, crawler_type: CrawlerType, crawler: BaseCrawler):
        """ì•ˆì „í•œ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        try:
            await crawler.initialize()
            self.logger.info(f"âœ… {crawler_type.value} í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"âŒ {crawler_type.value} í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ì´ˆê¸°í™” ì‹¤íŒ¨í•œ í¬ë¡¤ëŸ¬ëŠ” ì œê±°
            if crawler_type in self.crawlers:
                del self.crawlers[crawler_type]
    
    async def cleanup(self):
        """ëª¨ë“  í¬ë¡¤ëŸ¬ ì •ë¦¬"""
        cleanup_tasks = []
        for crawler in self.crawlers.values():
            cleanup_tasks.append(crawler.cleanup())
        
        await asyncio.gather(*cleanup_tasks, return_exceptions=True)
        
        self.crawlers.clear()
        self.is_initialized = False
        self.logger.info("ğŸ›‘ Smart Crawler ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")
    
    async def crawl(
        self, 
        url: str, 
        strategy: CrawlStrategy = CrawlStrategy.AUTO,
        **kwargs
    ) -> CrawlResult:
        """ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì‹¤í–‰"""
        if not self.is_initialized:
            await self.initialize()
        
        config = CrawlConfig(url=url, **kwargs)
        
        # í¬ë¡¤ëŸ¬ ìˆœì„œ ê²°ì •
        crawler_order = await self._determine_crawler_order(url, strategy)
        
        if not crawler_order:
            return CrawlResult(
                success=False,
                error="ì‚¬ìš© ê°€ëŠ¥í•œ í¬ë¡¤ëŸ¬ê°€ ì—†ìŠµë‹ˆë‹¤",
                url=url
            )
        
        # ìˆœì°¨ì ìœ¼ë¡œ í¬ë¡¤ëŸ¬ ì‹œë„
        last_result = None
        attempts = 0
        
        for crawler_type in crawler_order:
            if crawler_type not in self.crawlers:
                continue
            
            crawler = self.crawlers[crawler_type]
            attempts += 1
            
            try:
                self.logger.info(f"ğŸ¯ í¬ë¡¤ë§ ì‹œë„ {attempts}: {crawler_type.value} - {url}")
                
                result = await crawler.crawl(config)
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.crawler_stats[crawler_type].update(result)
                
                if result.success:
                    # ì„±ê³µí•œ í¬ë¡¤ëŸ¬ë¥¼ ë„ë©”ì¸ ì„ í˜¸ë„ì— ì €ì¥
                    domain = self._extract_domain(url)
                    self.domain_preferences[domain] = crawler_type
                    
                    self.logger.info(f"âœ… í¬ë¡¤ë§ ì„±ê³µ: {crawler_type.value}")
                    return result
                else:
                    self.logger.warning(f"âŒ {crawler_type.value} ì‹¤íŒ¨: {result.error}")
                    last_result = result
                    
            except Exception as e:
                error_msg = f"{crawler_type.value} ì˜ˆì™¸: {e}"
                self.logger.error(error_msg)
                
                # í†µê³„ ì—…ë°ì´íŠ¸ (ì‹¤íŒ¨)
                dummy_result = CrawlResult(
                    success=False,
                    error=error_msg,
                    crawler_used=crawler_type,
                    url=url
                )
                self.crawler_stats[crawler_type].update(dummy_result)
                
                last_result = dummy_result
        
        # ëª¨ë“  í¬ë¡¤ëŸ¬ ì‹¤íŒ¨
        return last_result or CrawlResult(
            success=False,
            error="ëª¨ë“  í¬ë¡¤ëŸ¬ ì‹¤íŒ¨",
            url=url
        )
    
    async def _determine_crawler_order(self, url: str, strategy: CrawlStrategy) -> List[CrawlerType]:
        """í¬ë¡¤ëŸ¬ ìš°ì„ ìˆœìœ„ ê²°ì •"""
        domain = self._extract_domain(url)
        
        # ë„ë©”ì¸ ê¸°ë°˜ ì„ í˜¸ë„ê°€ ìˆìœ¼ë©´ ìš°ì„  ì ìš©
        if domain in self.domain_preferences:
            preferred = self.domain_preferences[domain]
            if preferred in self.crawlers:
                other_crawlers = [c for c in self.crawlers.keys() if c != preferred]
                # ì„±ëŠ¥ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
                other_crawlers.sort(key=lambda c: self.crawler_stats[c].reliability_score, reverse=True)
                return [preferred] + other_crawlers
        
        # ì „ëµë³„ ìˆœì„œ ê²°ì •
        if strategy == CrawlStrategy.FAST:
            order = [CrawlerType.HTTPX, CrawlerType.PLAYWRIGHT, CrawlerType.SELENIUM]
        elif strategy == CrawlStrategy.RENDER:
            order = [CrawlerType.PLAYWRIGHT, CrawlerType.HTTPX, CrawlerType.SELENIUM]
        elif strategy == CrawlStrategy.STEALTH:
            order = [CrawlerType.SELENIUM, CrawlerType.PLAYWRIGHT, CrawlerType.HTTPX]
        elif strategy == CrawlStrategy.SAFE:
            # ëª¨ë“  í¬ë¡¤ëŸ¬ë¥¼ ì•ˆì •ì„± ìˆœìœ¼ë¡œ
            order = sorted(self.crawlers.keys(), 
                         key=lambda c: self.crawler_stats[c].reliability_score, 
                         reverse=True)
        else:  # AUTO
            order = await self._auto_determine_order(url)
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ í¬ë¡¤ëŸ¬ë§Œ ë°˜í™˜
        return [c for c in order if c in self.crawlers]
    
    async def _auto_determine_order(self, url: str) -> List[CrawlerType]:
        """ìë™ í¬ë¡¤ëŸ¬ ìˆœì„œ ê²°ì •"""
        url_lower = url.lower()
        
        # URL íŒ¨í„´ ê¸°ë°˜ ë¶„ì„
        if any(keyword in url_lower for keyword in ['api', '.json', '/rest/', '/graphql']):
            # API ì—”ë“œí¬ì¸íŠ¸ -> httpx ìš°ì„ 
            return [CrawlerType.HTTPX, CrawlerType.PLAYWRIGHT, CrawlerType.SELENIUM]
        
        elif any(keyword in url_lower for keyword in ['cloudflare', 'protection']):
            # ë³´í˜¸ëœ ì‚¬ì´íŠ¸ -> selenium ìš°ì„ 
            return [CrawlerType.SELENIUM, CrawlerType.PLAYWRIGHT, CrawlerType.HTTPX]
        
        elif '.go.kr' in url_lower or '.gov' in url_lower:
            # ì •ë¶€ ì‚¬ì´íŠ¸ -> selenium ìš°ì„  (ë³´ì•ˆì´ ê°•í•¨)
            return [CrawlerType.SELENIUM, CrawlerType.PLAYWRIGHT, CrawlerType.HTTPX]
        
        else:
            # ì¼ë°˜ ì‚¬ì´íŠ¸ -> ì„±ëŠ¥ ê¸°ì¤€ ì •ë ¬
            available_crawlers = list(self.crawlers.keys())
            available_crawlers.sort(
                key=lambda c: self.crawler_stats[c].reliability_score, 
                reverse=True
            )
            return available_crawlers
    
    def _extract_domain(self, url: str) -> str:
        """URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ"""
        try:
            parsed = urlparse(url)
            return parsed.netloc.lower()
        except:
            return url.lower()
    
    async def batch_crawl(
        self, 
        urls: List[str], 
        strategy: CrawlStrategy = CrawlStrategy.AUTO,
        max_concurrent: int = 5,
        **kwargs
    ) -> List[CrawlResult]:
        """ë°°ì¹˜ í¬ë¡¤ë§"""
        if not self.is_initialized:
            await self.initialize()
        
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def crawl_single(url: str) -> CrawlResult:
            async with semaphore:
                return await self.crawl(url, strategy, **kwargs)
        
        self.logger.info(f"ğŸš€ ë°°ì¹˜ í¬ë¡¤ë§ ì‹œì‘: {len(urls)}ê°œ URL")
        
        tasks = [crawl_single(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì˜ˆì™¸ ì²˜ë¦¬
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append(CrawlResult(
                    success=False,
                    error=str(result),
                    url=urls[i]
                ))
            else:
                processed_results.append(result)
        
        # ì„±ê³µë¥  í†µê³„
        successful = sum(1 for r in processed_results if r.success)
        self.logger.info(f"âœ… ë°°ì¹˜ í¬ë¡¤ë§ ì™„ë£Œ: {successful}/{len(urls)} ì„±ê³µ")
        
        return processed_results
    
    async def health_check(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
        health_status = {
            'system_healthy': True,
            'initialized': self.is_initialized,
            'available_crawlers': list(self.crawlers.keys()),
            'crawler_health': {},
            'performance_summary': {}
        }
        
        if not self.is_initialized:
            health_status['system_healthy'] = False
            return health_status
        
        # ê° í¬ë¡¤ëŸ¬ ìƒíƒœ í™•ì¸
        for crawler_type, crawler in self.crawlers.items():
            try:
                is_healthy = await crawler.health_check()
                health_status['crawler_health'][crawler_type.value] = {
                    'healthy': is_healthy,
                    'stats': self.get_crawler_stats(crawler_type)
                }
                
                if not is_healthy:
                    health_status['system_healthy'] = False
                    
            except Exception as e:
                health_status['crawler_health'][crawler_type.value] = {
                    'healthy': False,
                    'error': str(e)
                }
                health_status['system_healthy'] = False
        
        # ì „ì²´ ì„±ëŠ¥ ìš”ì•½
        health_status['performance_summary'] = self.get_performance_summary()
        
        return health_status
    
    def get_crawler_stats(self, crawler_type: CrawlerType) -> Dict[str, Any]:
        """í¬ë¡¤ëŸ¬ë³„ í†µê³„"""
        stats = self.crawler_stats[crawler_type]
        return {
            'total_requests': stats.total_requests,
            'success_rate': stats.success_rate,
            'average_response_time': stats.average_response_time,
            'reliability_score': stats.reliability_score,
            'error_types': dict(stats.error_types),
            'last_used': stats.last_used
        }
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """ì „ì²´ ì„±ëŠ¥ ìš”ì•½"""
        total_requests = sum(stats.total_requests for stats in self.crawler_stats.values())
        total_successful = sum(stats.successful_requests for stats in self.crawler_stats.values())
        
        if total_requests == 0:
            return {
                'overall_success_rate': 0.0,
                'total_requests': 0,
                'best_performer': None,
                'domain_preferences_count': len(self.domain_preferences)
            }
        
        # ìµœê³  ì„±ëŠ¥ í¬ë¡¤ëŸ¬
        best_performer = max(
            self.crawler_stats.items(),
            key=lambda x: x[1].reliability_score
        )[0].value if self.crawler_stats else None
        
        return {
            'overall_success_rate': total_successful / total_requests,
            'total_requests': total_requests,
            'best_performer': best_performer,
            'domain_preferences_count': len(self.domain_preferences),
            'avg_response_time': statistics.mean([
                stats.average_response_time 
                for stats in self.crawler_stats.values() 
                if stats.average_response_time > 0
            ]) if any(stats.average_response_time > 0 for stats in self.crawler_stats.values()) else 0
        }
    
    def optimize_performance(self):
        """ì„±ëŠ¥ ìµœì í™”"""
        self.logger.info("ğŸ”§ ì„±ëŠ¥ ìµœì í™” ì‹¤í–‰")
        
        # ì„±ëŠ¥ì´ ë‚®ì€ í¬ë¡¤ëŸ¬ì˜ ë„ë©”ì¸ ì„ í˜¸ë„ ì œê±°
        domains_to_remove = []
        for domain, preferred_crawler in self.domain_preferences.items():
            if preferred_crawler in self.crawler_stats:
                stats = self.crawler_stats[preferred_crawler]
                if stats.reliability_score < self.performance_thresholds['min_reliability_score']:
                    domains_to_remove.append(domain)
        
        for domain in domains_to_remove:
            del self.domain_preferences[domain]
            self.logger.info(f"ë„ë©”ì¸ ì„ í˜¸ë„ ì œê±°: {domain}")
        
        self.logger.info("âœ… ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ")


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤

async def quick_crawl(url: str, strategy: str = "auto", **kwargs) -> CrawlResult:
    """ë¹ ë¥¸ í¬ë¡¤ë§ (ì›ìƒ· í•¨ìˆ˜)"""
    crawler = SmartCrawler()
    try:
        await crawler.initialize()
        strategy_enum = CrawlStrategy(strategy.lower())
        return await crawler.crawl(url, strategy_enum, **kwargs)
    finally:
        await crawler.cleanup()


async def smart_batch_crawl(urls: List[str], **kwargs) -> List[CrawlResult]:
    """ìŠ¤ë§ˆíŠ¸ ë°°ì¹˜ í¬ë¡¤ë§"""
    crawler = SmartCrawler()
    try:
        await crawler.initialize()
        return await crawler.batch_crawl(urls, **kwargs)
    finally:
        await crawler.cleanup()


def create_smart_crawler() -> SmartCrawler:
    """ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ëŸ¬ íŒ©í† ë¦¬"""
    return SmartCrawler()