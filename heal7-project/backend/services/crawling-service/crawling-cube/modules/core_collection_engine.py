#!/usr/bin/env python3
"""
ğŸ¯ í•µì‹¬ ìˆ˜ì§‘ ì—”ì§„ - í¬ë¡¤ë§ ì‹œìŠ¤í…œ ê³µí†µ ëª¨ë“ˆ
ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬ë¥¼ ìœ„í•œ í†µí•© ì—”ì§„

Author: HEAL7 Development Team  
Version: 1.0.0
Date: 2025-08-29
"""

import asyncio
import logging
import hashlib
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass, asdict
from enum import Enum

import aiohttp
import asyncpg
from bs4 import BeautifulSoup


logger = logging.getLogger(__name__)


class CollectionStatus(Enum):
    """ìˆ˜ì§‘ ìƒíƒœ ì—´ê±°í˜•"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    DUPLICATE = "duplicate"
    PROCESSING = "processing"


@dataclass
class CollectionItem:
    """ìˆ˜ì§‘ëœ ê°œë³„ ì•„ì´í…œ ë°ì´í„° êµ¬ì¡°"""
    id: Optional[str] = None
    portal_id: str = ""
    title: str = ""
    agency: str = ""
    category: str = ""
    content: Dict[str, Any] = None
    url: str = ""
    scraped_at: datetime = None
    quality_score: float = 0.0
    status: CollectionStatus = CollectionStatus.PENDING
    hash_key: str = ""
    
    def __post_init__(self):
        if self.scraped_at is None:
            self.scraped_at = datetime.now()
        if not self.hash_key:
            self.hash_key = self.generate_hash()
        if self.content is None:
            self.content = {}
    
    def generate_hash(self) -> str:
        """ì»¨í…ì¸  í•´ì‹œ ìƒì„± (ì¤‘ë³µ ê°ì§€ìš©)"""
        content_str = f"{self.portal_id}_{self.title}_{self.agency}_{self.url}"
        return hashlib.md5(content_str.encode('utf-8')).hexdigest()


@dataclass 
class CollectionResult:
    """ìˆ˜ì§‘ ê²°ê³¼ ë°ì´í„°"""
    portal_id: str
    success: bool
    items_found: int
    new_items: int
    duplicates: int
    errors: int
    processing_time: float
    start_time: datetime
    end_time: datetime
    error_messages: List[str] = None
    
    def __post_init__(self):
        if self.error_messages is None:
            self.error_messages = []


class CoreCollectionEngine:
    """ğŸ¯ í•µì‹¬ ìˆ˜ì§‘ ì—”ì§„ - ëª¨ë“  í¬ë¡¤ëŸ¬ì˜ ê³µí†µ ê¸°ë°˜"""
    
    def __init__(self, db_connection_string: str):
        self.db_connection_string = db_connection_string
        self.db_pool = None
        
        # ìˆ˜ì§‘ ì„¤ì •
        self.max_concurrent = 5
        self.request_timeout = 30
        self.max_retries = 3
        self.duplicate_check_days = 7
        
        # í†µê³„
        self.total_requests = 0
        self.successful_requests = 0
        self.failed_requests = 0
        
        # HTTP ì„¸ì…˜
        self.session = None
        self.default_headers = {
            'User-Agent': 'HEAL7-CollectionBot/1.0 (+https://heal7.com/crawler)',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5',
            'Connection': 'keep-alive'
        }
    
    async def initialize(self):
        """ì—”ì§„ ì´ˆê¸°í™”"""
        logger.info("ğŸš€ Core Collection Engine ì´ˆê¸°í™” ì‹œì‘")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ ìƒì„±
        try:
            self.db_pool = await asyncpg.create_pool(
                self.db_connection_string,
                min_size=2,
                max_size=10,
                command_timeout=30
            )
            logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ ìƒì„± ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
        
        # HTTP ì„¸ì…˜ ìƒì„±
        connector = aiohttp.TCPConnector(
            limit=self.max_concurrent,
            limit_per_host=3,
            ttl_dns_cache=300
        )
        
        timeout = aiohttp.ClientTimeout(total=self.request_timeout)
        
        self.session = aiohttp.ClientSession(
            headers=self.default_headers,
            connector=connector, 
            timeout=timeout,
            cookie_jar=aiohttp.CookieJar()
        )
        
        logger.info("âœ… HTTP ì„¸ì…˜ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™”
        await self.initialize_database_schema()
        
        logger.info("ğŸ¯ Core Collection Engine ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def close(self):
        """ì—”ì§„ ì •ë¦¬"""
        if self.session:
            await self.session.close()
        
        if self.db_pool:
            await self.db_pool.close()
        
        logger.info("ğŸ›‘ Core Collection Engine ì •ë¦¬ ì™„ë£Œ")
    
    async def initialize_database_schema(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™”"""
        schema_sql = """
        -- í¬ë¡¤ë§ ë°ì´í„° ì €ì¥ í…Œì´ë¸”
        CREATE TABLE IF NOT EXISTS raw_scraped_data (
            id SERIAL PRIMARY KEY,
            portal_id VARCHAR(50) NOT NULL,
            title TEXT NOT NULL,
            agency VARCHAR(200),
            category VARCHAR(100),
            raw_data JSONB NOT NULL,
            url TEXT,
            hash_key VARCHAR(32) UNIQUE NOT NULL,
            quality_score DECIMAL(3,1) DEFAULT 0.0,
            scraped_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
            processing_status VARCHAR(20) DEFAULT 'pending',
            created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
            updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
        );
        
        -- ì¸ë±ìŠ¤ ìƒì„±
        CREATE INDEX IF NOT EXISTS idx_raw_scraped_portal_id ON raw_scraped_data(portal_id);
        CREATE INDEX IF NOT EXISTS idx_raw_scraped_scraped_at ON raw_scraped_data(scraped_at);
        CREATE INDEX IF NOT EXISTS idx_raw_scraped_status ON raw_scraped_data(processing_status);
        CREATE INDEX IF NOT EXISTS idx_raw_scraped_hash ON raw_scraped_data(hash_key);
        
        -- ìˆ˜ì§‘ í†µê³„ í…Œì´ë¸”
        CREATE TABLE IF NOT EXISTS collection_stats (
            id SERIAL PRIMARY KEY,
            portal_id VARCHAR(50) NOT NULL,
            collection_date DATE NOT NULL,
            items_found INTEGER DEFAULT 0,
            items_new INTEGER DEFAULT 0,
            items_duplicate INTEGER DEFAULT 0,
            items_failed INTEGER DEFAULT 0,
            processing_time_seconds DECIMAL(10,2) DEFAULT 0,
            created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
            UNIQUE(portal_id, collection_date)
        );
        """
        
        async with self.db_pool.acquire() as conn:
            await conn.execute(schema_sql)
        
        logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def collect_from_portal(
        self, 
        portal_id: str,
        extractor_func: Callable,
        pages_to_scan: int = 10,
        force_update: bool = False
    ) -> CollectionResult:
        """í¬í„¸ì—ì„œ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰"""
        start_time = datetime.now()
        
        logger.info(f"ğŸ•·ï¸ ìˆ˜ì§‘ ì‹œì‘: {portal_id} (í˜ì´ì§€: {pages_to_scan})")
        
        collected_items = []
        errors = []
        
        try:
            # 1ë‹¨ê³„: ë°ì´í„° ì¶”ì¶œ
            raw_items = await extractor_func(pages_to_scan)
            logger.info(f"ğŸ“Š ì›ë³¸ ë°ì´í„° {len(raw_items)}ê°œ ì¶”ì¶œ ì™„ë£Œ")
            
            # 2ë‹¨ê³„: ë°ì´í„° ì •ì œ ë° êµ¬ì¡°í™”
            for raw_item in raw_items:
                try:
                    item = CollectionItem(
                        portal_id=portal_id,
                        title=raw_item.get('title', ''),
                        agency=raw_item.get('agency', ''),
                        category=raw_item.get('category', ''),
                        content=raw_item,
                        url=raw_item.get('url', ''),
                        quality_score=self.calculate_quality_score(raw_item)
                    )
                    collected_items.append(item)
                except Exception as e:
                    errors.append(f"ì•„ì´í…œ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            
            # 3ë‹¨ê³„: ì¤‘ë³µ ê²€ì‚¬
            if not force_update:
                collected_items = await self.filter_duplicates(collected_items)
            
            # 4ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
            save_result = await self.save_items_to_database(collected_items)
            
            # 5ë‹¨ê³„: í†µê³„ ì—…ë°ì´íŠ¸
            await self.update_collection_stats(
                portal_id=portal_id,
                items_found=len(raw_items),
                items_new=save_result['new_count'], 
                items_duplicate=save_result['duplicate_count'],
                processing_time=(datetime.now() - start_time).total_seconds()
            )
            
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()
            
            result = CollectionResult(
                portal_id=portal_id,
                success=True,
                items_found=len(raw_items),
                new_items=save_result['new_count'],
                duplicates=save_result['duplicate_count'],
                errors=len(errors),
                processing_time=processing_time,
                start_time=start_time,
                end_time=end_time,
                error_messages=errors
            )
            
            logger.info(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {portal_id} | ì‹ ê·œ {result.new_items}ê°œ | ì¤‘ë³µ {result.duplicates}ê°œ | {processing_time:.1f}ì´ˆ")
            
            return result
            
        except Exception as e:
            end_time = datetime.now() 
            processing_time = (end_time - start_time).total_seconds()
            
            logger.error(f"âŒ ìˆ˜ì§‘ ì‹¤íŒ¨: {portal_id} - {str(e)}")
            
            return CollectionResult(
                portal_id=portal_id,
                success=False,
                items_found=0,
                new_items=0,
                duplicates=0,
                errors=1,
                processing_time=processing_time,
                start_time=start_time,
                end_time=end_time,
                error_messages=[str(e)]
            )
    
    async def filter_duplicates(self, items: List[CollectionItem]) -> List[CollectionItem]:
        """ì¤‘ë³µ í•­ëª© í•„í„°ë§"""
        if not items:
            return items
        
        # í•´ì‹œí‚¤ ê¸°ë°˜ìœ¼ë¡œ ê¸°ì¡´ ë°ì´í„° í™•ì¸
        hash_keys = [item.hash_key for item in items]
        
        async with self.db_pool.acquire() as conn:
            existing_hashes = await conn.fetch(
                """
                SELECT hash_key FROM raw_scraped_data 
                WHERE hash_key = ANY($1::text[])
                AND scraped_at >= $2
                """,
                hash_keys,
                datetime.now() - timedelta(days=self.duplicate_check_days)
            )
        
        existing_hash_set = {row['hash_key'] for row in existing_hashes}
        
        # ì¤‘ë³µë˜ì§€ ì•Šì€ í•­ëª©ë§Œ ë°˜í™˜
        unique_items = []
        for item in items:
            if item.hash_key not in existing_hash_set:
                unique_items.append(item)
            else:
                item.status = CollectionStatus.DUPLICATE
        
        logger.info(f"ğŸ” ì¤‘ë³µ í•„í„°ë§: {len(items)} â†’ {len(unique_items)} (ì¤‘ë³µ {len(items) - len(unique_items)}ê°œ)")
        
        return unique_items
    
    async def save_items_to_database(self, items: List[CollectionItem]) -> Dict[str, int]:
        """ë°ì´í„°ë² ì´ìŠ¤ì— ì•„ì´í…œ ì €ì¥"""
        if not items:
            return {'new_count': 0, 'duplicate_count': 0}
        
        new_count = 0
        duplicate_count = 0
        
        async with self.db_pool.acquire() as conn:
            for item in items:
                try:
                    await conn.execute(
                        """
                        INSERT INTO raw_scraped_data 
                        (portal_id, title, agency, category, raw_data, url, hash_key, 
                         quality_score, scraped_at, processing_status)
                        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                        ON CONFLICT (hash_key) DO NOTHING
                        """,
                        item.portal_id,
                        item.title,
                        item.agency, 
                        item.category,
                        json.dumps(item.content, ensure_ascii=False),
                        item.url,
                        item.hash_key,
                        item.quality_score,
                        item.scraped_at,
                        item.status.value
                    )
                    new_count += 1
                    item.status = CollectionStatus.COMPLETED
                    
                except Exception as e:
                    if "duplicate key" in str(e).lower():
                        duplicate_count += 1
                        item.status = CollectionStatus.DUPLICATE
                    else:
                        logger.error(f"âŒ ì €ì¥ ì‹¤íŒ¨: {item.title} - {str(e)}")
                        item.status = CollectionStatus.FAILED
        
        logger.info(f"ğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: ì‹ ê·œ {new_count}ê°œ, ì¤‘ë³µ {duplicate_count}ê°œ")
        
        return {'new_count': new_count, 'duplicate_count': duplicate_count}
    
    async def update_collection_stats(
        self,
        portal_id: str,
        items_found: int,
        items_new: int, 
        items_duplicate: int,
        processing_time: float
    ):
        """ìˆ˜ì§‘ í†µê³„ ì—…ë°ì´íŠ¸"""
        today = datetime.now().date()
        
        async with self.db_pool.acquire() as conn:
            await conn.execute(
                """
                INSERT INTO collection_stats 
                (portal_id, collection_date, items_found, items_new, 
                 items_duplicate, processing_time_seconds)
                VALUES ($1, $2, $3, $4, $5, $6)
                ON CONFLICT (portal_id, collection_date)
                DO UPDATE SET
                    items_found = collection_stats.items_found + EXCLUDED.items_found,
                    items_new = collection_stats.items_new + EXCLUDED.items_new,
                    items_duplicate = collection_stats.items_duplicate + EXCLUDED.items_duplicate,
                    processing_time_seconds = collection_stats.processing_time_seconds + EXCLUDED.processing_time_seconds
                """,
                portal_id,
                today,
                items_found,
                items_new,
                items_duplicate,
                processing_time
            )
    
    def calculate_quality_score(self, raw_item: Dict[str, Any]) -> float:
        """ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0.0~10.0)"""
        score = 0.0
        
        # ê¸°ë³¸ ì •ë³´ ì¡´ì¬ ì—¬ë¶€ (40%)
        if raw_item.get('title'):
            score += 2.0
        if raw_item.get('agency'):
            score += 1.5
        if raw_item.get('category'):
            score += 1.0
        if raw_item.get('url'):
            score += 0.5
        
        # ë‚´ìš© ì¶©ì‹¤ë„ (40%)
        content_length = len(str(raw_item.get('content', '')))
        if content_length > 100:
            score += 2.0
        elif content_length > 50:
            score += 1.0
        
        if raw_item.get('application_period'):
            score += 1.0
        if raw_item.get('target_audience'):
            score += 1.0
        
        # êµ¬ì¡°í™” ì •ë„ (20%)
        if isinstance(raw_item.get('support_details'), dict):
            score += 1.0
        if raw_item.get('contact_info'):
            score += 1.0
        
        return min(score, 10.0)  # ìµœëŒ€ 10ì 
    
    async def get_recent_collections(
        self,
        portal_id: Optional[str] = None,
        days: int = 7,
        limit: int = 100
    ) -> List[Dict]:
        """ìµœê·¼ ìˆ˜ì§‘ ë°ì´í„° ì¡°íšŒ"""
        since_date = datetime.now() - timedelta(days=days)
        
        query = """
            SELECT 
                id, portal_id, title, agency, category,
                raw_data, url, quality_score, scraped_at, processing_status
            FROM raw_scraped_data
            WHERE scraped_at >= $1
        """
        params = [since_date]
        
        if portal_id:
            query += " AND portal_id = $2"
            params.append(portal_id)
        
        query += " ORDER BY scraped_at DESC LIMIT ${}".format(len(params) + 1)
        params.append(limit)
        
        async with self.db_pool.acquire() as conn:
            rows = await conn.fetch(query, *params)
        
        return [dict(row) for row in rows]
    
    async def get_collection_statistics(self, portal_id: Optional[str] = None) -> Dict:
        """ìˆ˜ì§‘ í†µê³„ ì¡°íšŒ"""
        today = datetime.now().date()
        
        base_query = """
            SELECT 
                portal_id,
                COUNT(*) as total_items,
                COUNT(CASE WHEN DATE(scraped_at) = $1 THEN 1 END) as today_items,
                AVG(quality_score) as avg_quality,
                COUNT(CASE WHEN processing_status = 'duplicate' THEN 1 END) as duplicate_items,
                MIN(scraped_at) as first_scraped,
                MAX(scraped_at) as last_scraped
            FROM raw_scraped_data
            WHERE processing_status != 'failed'
        """
        
        params = [today]
        
        if portal_id:
            base_query += " AND portal_id = $2 GROUP BY portal_id"
            params.append(portal_id)
        else:
            base_query += " GROUP BY portal_id"
        
        async with self.db_pool.acquire() as conn:
            rows = await conn.fetch(base_query, *params)
        
        stats = {}
        total_items = 0
        total_today = 0
        
        for row in rows:
            portal_stats = {
                'total_items': row['total_items'],
                'today_items': row['today_items'],
                'avg_quality': float(row['avg_quality'] or 0),
                'duplicate_items': row['duplicate_items'],
                'first_scraped': row['first_scraped'].isoformat() if row['first_scraped'] else None,
                'last_scraped': row['last_scraped'].isoformat() if row['last_scraped'] else None
            }
            
            stats[row['portal_id']] = portal_stats
            total_items += row['total_items']
            total_today += row['today_items']
        
        return {
            'portals': stats,
            'summary': {
                'total_items': total_items,
                'today_items': total_today,
                'active_portals': len(stats),
                'last_updated': datetime.now().isoformat()
            }
        }


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤

async def create_collection_engine(db_connection_string: str) -> CoreCollectionEngine:
    """ìˆ˜ì§‘ ì—”ì§„ ìƒì„± ë° ì´ˆê¸°í™”"""
    engine = CoreCollectionEngine(db_connection_string)
    await engine.initialize()
    return engine


def extract_text_safely(element, default: str = "") -> str:
    """BeautifulSoup ìš”ì†Œì—ì„œ ì•ˆì „í•˜ê²Œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    if element:
        return element.get_text(strip=True) or default
    return default


def clean_korean_text(text: str) -> str:
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì •ì œ"""
    if not text:
        return ""
    
    # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    text = ' '.join(text.split())
    
    # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
    text = text.replace('\xa0', ' ')  # Non-breaking space
    text = text.replace('\u200b', '')  # Zero-width space
    
    return text.strip()