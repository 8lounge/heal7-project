#!/usr/bin/env python3
"""
ğŸ›¡ï¸ Selenium + Undetected í¬ë¡¤ëŸ¬ (Tier 3)
Anti-bot ìš°íšŒ ë° ìµœëŒ€ í˜¸í™˜ì„± í¬ë¡¤ë§

Features:
- Cloudflare ìš°íšŒ
- Bot ê°ì§€ íšŒí”¼
- ê°•ë ¥í•œ ì‚¬ì´íŠ¸ í˜¸í™˜ì„±
- ìº¡ì°¨ ëŒ€ì‘ ê°€ëŠ¥
- ë ˆê±°ì‹œ ë¸Œë¼ìš°ì € ì§€ì›

Author: HEAL7 Development Team
Version: 1.0.0
Date: 2025-08-30
"""

import asyncio
import time
import json
from typing import Optional, Dict, Any, List
from concurrent.futures import ThreadPoolExecutor
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
from selenium.common.exceptions import (
    TimeoutException, WebDriverException, NoSuchElementException
)

try:
    import undetected_chromedriver as uc
except ImportError:
    uc = None

from .base_crawler import (
    BaseCrawler, CrawlResult, CrawlConfig, CrawlerType,
    CrawlingError, TimeoutError, AntiBotDetectionError, create_default_headers, is_valid_url
)


class SeleniumCrawler(BaseCrawler):
    """ğŸ›¡ï¸ Selenium + Undetected ê¸°ë°˜ ìŠ¤í…”ìŠ¤ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, use_undetected: bool = True, headless: bool = True):
        super().__init__("selenium")
        self.use_undetected = use_undetected
        self.headless = headless
        self.driver = None
        self.executor = ThreadPoolExecutor(max_workers=1)  # Seleniumì€ ë™ê¸°ì‹
        
        # Chrome ì˜µì…˜
        self.chrome_options = self._create_chrome_options()
        
    def _create_chrome_options(self):
        """Chrome ì˜µì…˜ ìƒì„±"""
        if self.use_undetected and uc:
            options = uc.ChromeOptions()
        else:
            from selenium.webdriver.chrome.options import Options
            options = Options()
        
        # ê¸°ë³¸ ì˜µì…˜
        if self.headless:
            options.add_argument('--headless')
        
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--disable-extensions')
        options.add_argument('--disable-logging')
        options.add_argument('--disable-web-security')
        options.add_argument('--allow-running-insecure-content')
        options.add_argument('--ignore-certificate-errors')
        
        # ìŠ¤í…”ìŠ¤ ì˜µì…˜ (Chrome í˜¸í™˜ì„± ìµœì í™”)
        options.add_argument('--disable-blink-features=AutomationControlled')
        # Chrome ìµœì‹  ë²„ì „ í˜¸í™˜ì„±ì„ ìœ„í•´ ì‹¤í—˜ì  ì˜µì…˜ ì œê±°
        # options.add_experimental_option("excludeSwitches", ["enable-automation"])
        # options.add_experimental_option('useAutomationExtension', False)
        
        # ì°½ í¬ê¸°
        options.add_argument('--window-size=1920,1080')
        
        # ì‚¬ìš©ì ì—ì´ì „íŠ¸
        options.add_argument(
            '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
            'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        )
        
        return options
    
    async def initialize(self):
        """Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™”"""
        if self.is_initialized:
            return
            
        def _init_driver():
            try:
                if self.use_undetected and uc:
                    # Undetected Chrome ì‚¬ìš©
                    driver = uc.Chrome(options=self.chrome_options)
                    
                    # ì¶”ê°€ ìŠ¤í…”ìŠ¤ ì„¤ì •
                    driver.execute_script("""
                        Object.defineProperty(navigator, 'webdriver', {
                            get: () => undefined,
                        });
                    """)
                    
                    driver.execute_cdp_cmd('Network.setUserAgentOverride', {
                        "userAgent": driver.execute_script("return navigator.userAgent").replace("Headless", "")
                    })
                    
                else:
                    # ì¼ë°˜ Chrome WebDriver
                    from selenium.webdriver import Chrome
                    driver = Chrome(options=self.chrome_options)
                
                return driver
                
            except Exception as e:
                raise CrawlingError(f"Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        try:
            # ë¹„ë™ê¸°ì ìœ¼ë¡œ ë“œë¼ì´ë²„ ì´ˆê¸°í™”
            self.driver = await asyncio.get_event_loop().run_in_executor(
                self.executor, _init_driver
            )
            
            self.is_initialized = True
            driver_type = "Undetected Chrome" if self.use_undetected else "Chrome"
            self.logger.info(f"âœ… Selenium ({driver_type}) í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ Selenium ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise CrawlingError(f"Selenium ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        def _cleanup_driver():
            if self.driver:
                try:
                    self.driver.quit()
                except:
                    pass
        
        if self.driver:
            await asyncio.get_event_loop().run_in_executor(
                self.executor, _cleanup_driver
            )
            self.driver = None
        
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=True)
        
        self.is_initialized = False
        self.logger.info("ğŸ›‘ Selenium í¬ë¡¤ëŸ¬ ì •ë¦¬ ì™„ë£Œ")
    
    async def crawl(self, config: CrawlConfig) -> CrawlResult:
        """ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜"""
        if not self.is_initialized:
            await self.initialize()
        
        start_time = time.time()
        
        def _crawl_sync():
            try:
                # URL ìœ íš¨ì„± ê²€ì‚¬
                if not is_valid_url(config.url):
                    raise CrawlingError(f"ì˜ëª»ëœ URL: {config.url}")
                
                # í˜ì´ì§€ ë¡œë“œ
                self.driver.set_page_load_timeout(config.timeout)
                self.driver.get(config.url)
                
                # Anti-bot ê°ì§€ í™•ì¸
                if self._is_bot_detected():
                    raise AntiBotDetectionError("Bot ê°ì§€ë¨")
                
                # ì½˜í…ì¸  ë¡œë“œ ëŒ€ê¸°
                self._wait_for_content(config)
                
                # HTML ì¶”ì¶œ
                html = self.driver.page_source
                
                # ìŠ¤í¬ë¦°ìƒ· (ì„ íƒì‚¬í•­)
                screenshot = None
                if config.screenshot:
                    screenshot = self._take_screenshot(config)
                
                # ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
                metadata = self._collect_metadata()
                
                return CrawlResult(
                    success=True,
                    html=html,
                    screenshot=screenshot,
                    metadata=metadata,
                    crawler_used=CrawlerType.SELENIUM,
                    url=config.url,
                    response_time=time.time() - start_time
                )
                
            except TimeoutException:
                raise TimeoutError(f"í˜ì´ì§€ ë¡œë“œ íƒ€ì„ì•„ì›ƒ: {config.url}")
            except AntiBotDetectionError:
                raise
            except Exception as e:
                raise CrawlingError(f"Selenium í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        
        try:
            # ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            result = await asyncio.get_event_loop().run_in_executor(
                self.executor, _crawl_sync
            )
            
            self.update_stats(result)
            return result
            
        except AntiBotDetectionError:
            error_msg = f"Bot ê°ì§€ë¡œ ì¸í•œ í¬ë¡¤ë§ ì‹¤íŒ¨: {config.url}"
            self.logger.warning(error_msg)
            return CrawlResult(
                success=False,
                error=error_msg,
                error_type="antibot_detection",
                crawler_used=CrawlerType.SELENIUM,
                url=config.url,
                response_time=time.time() - start_time
            )
            
        except TimeoutError as e:
            return CrawlResult(
                success=False,
                error=str(e),
                error_type="timeout",
                crawler_used=CrawlerType.SELENIUM,
                url=config.url,
                response_time=time.time() - start_time
            )
            
        except Exception as e:
            return CrawlResult(
                success=False,
                error=str(e),
                error_type="selenium_error",
                crawler_used=CrawlerType.SELENIUM,
                url=config.url,
                response_time=time.time() - start_time
            )
    
    def _is_bot_detected(self) -> bool:
        """Bot ê°ì§€ ì—¬ë¶€ í™•ì¸"""
        try:
            # Cloudflare ê°ì§€
            if "Checking your browser" in self.driver.page_source:
                return True
            
            # ê¸°íƒ€ bot ê°ì§€ íŒ¨í„´
            bot_indicators = [
                "Access denied",
                "Blocked",
                "Captcha",
                "Please verify you are human",
                "Security check"
            ]
            
            page_text = self.driver.page_source.lower()
            return any(indicator.lower() in page_text for indicator in bot_indicators)
            
        except:
            return False
    
    def _wait_for_content(self, config: CrawlConfig):
        """ì½˜í…ì¸  ë¡œë“œ ëŒ€ê¸°"""
        wait = WebDriverWait(self.driver, 10)
        
        try:
            # íŠ¹ì • ì„ íƒì ëŒ€ê¸°
            if config.wait_for_selector:
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, config.wait_for_selector)))
            
            # DOM ë¡œë“œ ì™„ë£Œ ëŒ€ê¸°
            if config.wait_for_load:
                wait.until(lambda driver: driver.execute_script("return document.readyState") == "complete")
                
                # JavaScript ì‹¤í–‰ ëŒ€ê¸° (ì¶”ê°€)
                time.sleep(2)
            
        except TimeoutException:
            # íƒ€ì„ì•„ì›ƒë˜ì–´ë„ ê³„ì† ì§„í–‰
            pass
    
    def _take_screenshot(self, config: CrawlConfig) -> Optional[bytes]:
        """ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜"""
        try:
            if config.screenshot_full_page:
                # ì „ì²´ í˜ì´ì§€ ìŠ¤í¬ë¦°ìƒ·
                total_height = self.driver.execute_script("return document.body.scrollHeight")
                self.driver.set_window_size(1920, total_height)
                time.sleep(1)
            
            return self.driver.get_screenshot_as_png()
            
        except Exception as e:
            self.logger.error(f"ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜ ì‹¤íŒ¨: {e}")
            return None
    
    def _collect_metadata(self) -> Dict[str, Any]:
        """ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘"""
        try:
            metadata = {
                'title': self.driver.title,
                'final_url': self.driver.current_url,
                'window_size': self.driver.get_window_size(),
                'cookies': self.driver.get_cookies(),
                'user_agent': self.driver.execute_script("return navigator.userAgent")
            }
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ (ì§€ì›í•˜ëŠ” ê²½ìš°)
            try:
                performance = self.driver.execute_script("""
                    const perfData = performance.getEntriesByType('navigation')[0];
                    return perfData ? {
                        loadTime: perfData.loadEventEnd - perfData.loadEventStart,
                        domContentLoaded: perfData.domContentLoadedEventEnd - perfData.domContentLoadedEventStart
                    } : {};
                """)
                metadata['performance'] = performance
            except:
                pass
            
            return metadata
            
        except Exception as e:
            self.logger.error(f"ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {}
    
    async def can_handle(self, url: str) -> bool:
        """URL ì²˜ë¦¬ ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨"""
        if not is_valid_url(url):
            return False
        
        url_lower = url.lower()
        
        # Cloudflare ë³´í˜¸ ì‚¬ì´íŠ¸ (ìµœìš°ì„ )
        cloudflare_indicators = ['cloudflare', 'cf-ray', '.cloudflareinsights.com']
        if any(indicator in url_lower for indicator in cloudflare_indicators):
            return True
        
        # Anti-bot ë³´í˜¸ê°€ ì˜ˆìƒë˜ëŠ” ì‚¬ì´íŠ¸
        protected_domains = [
            'amazon.com', 'booking.com', 'ticketmaster.com',
            'stubhub.com', 'nike.com', 'supreme.com'
        ]
        if any(domain in url_lower for domain in protected_domains):
            return True
        
        # ì¼ë°˜ì ìœ¼ë¡œ ëª¨ë“  ì›¹ì‚¬ì´íŠ¸ ì²˜ë¦¬ ê°€ëŠ¥
        return True
    
    async def solve_captcha(self, config: CrawlConfig) -> CrawlResult:
        """ìº¡ì°¨ í•´ê²° ì‹œë„ (ê¸°ë³¸ì ì¸ ëŒ€ê¸° ì „ëµ)"""
        if not self.is_initialized:
            await self.initialize()
        
        def _solve_captcha_sync():
            try:
                self.driver.get(config.url)
                
                # ìº¡ì°¨ ê°ì§€
                captcha_selectors = [
                    'div[class*="captcha"]',
                    'iframe[src*="recaptcha"]',
                    'div[id*="challenge"]'
                ]
                
                wait = WebDriverWait(self.driver, 10)
                captcha_found = False
                
                for selector in captcha_selectors:
                    try:
                        element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
                        if element:
                            captcha_found = True
                            break
                    except:
                        continue
                
                if captcha_found:
                    # ìˆ˜ë™ í•´ê²°ì„ ìœ„í•œ ê¸´ ëŒ€ê¸° (headlessê°€ ì•„ë‹Œ ê²½ìš°)
                    if not self.headless:
                        self.logger.info("ìº¡ì°¨ ê°ì§€ë¨. ìˆ˜ë™ í•´ê²°ì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤... (60ì´ˆ ëŒ€ê¸°)")
                        time.sleep(60)
                    else:
                        raise AntiBotDetectionError("ìº¡ì°¨ ê°ì§€ë¨ - headless ëª¨ë“œì—ì„œëŠ” í•´ê²° ë¶ˆê°€")
                
                # ìµœì¢… HTML ì¶”ì¶œ
                html = self.driver.page_source
                
                return CrawlResult(
                    success=True,
                    html=html,
                    crawler_used=CrawlerType.SELENIUM,
                    url=config.url,
                    metadata={'captcha_encountered': captcha_found}
                )
                
            except Exception as e:
                raise CrawlingError(f"ìº¡ì°¨ í•´ê²° ì‹¤íŒ¨: {e}")
        
        try:
            result = await asyncio.get_event_loop().run_in_executor(
                self.executor, _solve_captcha_sync
            )
            return result
            
        except Exception as e:
            return CrawlResult(
                success=False,
                error=str(e),
                crawler_used=CrawlerType.SELENIUM,
                url=config.url
            )
    
    async def interact_and_crawl(self, url: str, interactions: List[Dict]) -> CrawlResult:
        """í˜ì´ì§€ ìƒí˜¸ì‘ìš© í›„ í¬ë¡¤ë§"""
        if not self.is_initialized:
            await self.initialize()
        
        start_time = time.time()
        
        def _interact_sync():
            try:
                self.driver.get(url)
                self._wait_for_content(CrawlConfig(url=url))
                
                # ìƒí˜¸ì‘ìš© ì‹¤í–‰
                for interaction in interactions:
                    self._execute_interaction(interaction)
                    time.sleep(1)
                
                # ìµœì¢… ê²°ê³¼
                html = self.driver.page_source
                screenshot = None
                
                if interaction.get('screenshot', False):
                    screenshot = self.driver.get_screenshot_as_png()
                
                return CrawlResult(
                    success=True,
                    html=html,
                    screenshot=screenshot,
                    crawler_used=CrawlerType.SELENIUM,
                    url=url,
                    response_time=time.time() - start_time,
                    metadata={'interactions_count': len(interactions)}
                )
                
            except Exception as e:
                raise CrawlingError(f"ìƒí˜¸ì‘ìš© í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        
        try:
            result = await asyncio.get_event_loop().run_in_executor(
                self.executor, _interact_sync
            )
            return result
            
        except Exception as e:
            return CrawlResult(
                success=False,
                error=str(e),
                crawler_used=CrawlerType.SELENIUM,
                url=url,
                response_time=time.time() - start_time
            )
    
    def _execute_interaction(self, interaction: Dict):
        """ê°œë³„ ìƒí˜¸ì‘ìš© ì‹¤í–‰"""
        action = interaction.get('action')
        selector = interaction.get('selector')
        value = interaction.get('value')
        
        wait = WebDriverWait(self.driver, 10)
        
        if action == 'click':
            element = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, selector)))
            element.click()
            
        elif action == 'type':
            element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
            element.clear()
            element.send_keys(value)
            
        elif action == 'scroll':
            if value:
                self.driver.execute_script(f"window.scrollTo(0, {value});")
            else:
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                
        elif action == 'wait':
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
            
        elif action == 'hover':
            element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
            ActionChains(self.driver).move_to_element(element).perform()
    
    def get_supported_features(self) -> List[str]:
        """ì§€ì›í•˜ëŠ” ê¸°ëŠ¥ ëª©ë¡"""
        return [
            "antibot_bypass",
            "cloudflare_bypass",
            "captcha_handling",
            "page_interaction",
            "screenshot_capture",
            "stealth_mode",
            "legacy_support"
        ]


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜

async def bypass_protection(url: str, wait_time: int = 10) -> CrawlResult:
    """ë³´í˜¸ëœ ì‚¬ì´íŠ¸ ìš°íšŒ í¬ë¡¤ë§"""
    crawler = SeleniumCrawler(use_undetected=True)
    try:
        await crawler.initialize()
        config = CrawlConfig(url=url, timeout=wait_time, wait_for_load=True)
        result = await crawler.crawl(config)
        
        # Bot ê°ì§€ì‹œ ìº¡ì°¨ í•´ê²° ì‹œë„
        if not result.success and result.error_type == "antibot_detection":
            result = await crawler.solve_captcha(config)
        
        return result
        
    finally:
        await crawler.cleanup()


async def stealth_screenshot(url: str, save_path: str = None) -> bool:
    """ìŠ¤í…”ìŠ¤ ëª¨ë“œ ìŠ¤í¬ë¦°ìƒ·"""
    crawler = SeleniumCrawler(use_undetected=True, headless=False)
    try:
        await crawler.initialize()
        config = CrawlConfig(url=url, screenshot=True, screenshot_full_page=True)
        result = await crawler.crawl(config)
        
        if result.success and result.screenshot:
            if save_path:
                with open(save_path, 'wb') as f:
                    f.write(result.screenshot)
            return True
        
        return False
        
    finally:
        await crawler.cleanup()