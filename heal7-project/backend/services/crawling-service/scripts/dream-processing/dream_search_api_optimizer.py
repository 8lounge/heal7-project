#!/usr/bin/env python3
"""
ğŸ” ê¿ˆí’€ì´ ê²€ìƒ‰ API ë° í•„í„°ë§ ì‹œìŠ¤í…œ ìµœì í™”
- ê³ ì„±ëŠ¥ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜
- ì¹´í…Œê³ ë¦¬ë³„ í•„í„°ë§
- ë‹¤ì¤‘ í•´ì„ ë°˜í™˜ ì‹œìŠ¤í…œ
- ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- ìºì‹± ìµœì í™”
"""

import json
import time
import logging
import subprocess
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import re
import hashlib

@dataclass
class SearchQuery:
    """ê²€ìƒ‰ ì¿¼ë¦¬"""
    keyword: str
    category: Optional[str] = None
    interpretation_types: List[str] = None  # ['traditional', 'modern', 'psychological']
    sentiment_filter: Optional[str] = None  # 'positive', 'negative', 'neutral'
    quality_threshold: float = 7.0
    limit: int = 10
    offset: int = 0

@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼"""
    keyword_id: int
    keyword: str
    category: str
    quality_score: float
    interpretations: List[Dict]  # [{'type': 'traditional', 'text': '...', 'sentiment': '...'}]
    related_keywords: List[str]
    search_count: int
    match_score: float  # ê²€ìƒ‰ì–´ì™€ì˜ ì¼ì¹˜ë„

@dataclass
class SearchResponse:
    """ê²€ìƒ‰ ì‘ë‹µ"""
    query: str
    total_results: int
    results: List[SearchResult]
    response_time_ms: int
    suggestions: List[str]
    categories: List[Dict]  # [{'category': 'water', 'count': 5}]

class DreamSearchAPIOptimizer:
    """ê¿ˆí’€ì´ ê²€ìƒ‰ API ìµœì í™” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.logger = self._setup_logger()
        self.cache = {}  # ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ìºì‹œ
        self.cache_ttl = 3600  # 1ì‹œê°„ ìºì‹œ
        
    def _setup_logger(self):
        """ë¡œê±° ì„¤ì •"""
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.FileHandler('/home/ubuntu/logs/dream_search_api.log')
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            
            console = logging.StreamHandler()
            console.setFormatter(formatter)
            logger.addHandler(console)
        
        return logger
    
    def search_dreams(self, query: SearchQuery) -> SearchResponse:
        """ê¿ˆí’€ì´ ê²€ìƒ‰ ë©”ì¸ í•¨ìˆ˜"""
        start_time = time.time()
        
        # ìºì‹œ í™•ì¸
        cache_key = self._generate_cache_key(query)
        cached_result = self._get_from_cache(cache_key)
        if cached_result:
            self.logger.info(f"ìºì‹œì—ì„œ ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜: {query.keyword}")
            return cached_result
        
        # ê²€ìƒ‰ ì‹¤í–‰
        results = self._execute_search(query)
        
        # ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        response_time = int((time.time() - start_time) * 1000)
        
        # ê²€ìƒ‰ ì œì•ˆ ìƒì„±
        suggestions = self._generate_suggestions(query.keyword)
        
        # ì¹´í…Œê³ ë¦¬ë³„ ê²°ê³¼ ì§‘ê³„
        categories = self._aggregate_categories(results)
        
        # ê²€ìƒ‰ ë¡œê·¸ ê¸°ë¡
        self._log_search(query.keyword, len(results), response_time)
        
        # ì‘ë‹µ ìƒì„±
        response = SearchResponse(
            query=query.keyword,
            total_results=len(results),
            results=results,
            response_time_ms=response_time,
            suggestions=suggestions,
            categories=categories
        )
        
        # ìºì‹œì— ì €ì¥
        self._save_to_cache(cache_key, response)
        
        return response
    
    def _execute_search(self, query: SearchQuery) -> List[SearchResult]:
        """ê²€ìƒ‰ ì‹¤í–‰"""
        # SQL ì¿¼ë¦¬ êµ¬ì„±
        base_query = """
            SELECT DISTINCT k.id, k.keyword, k.category_id, k.quality_score,
                   k.frequency_score, s.search_count
            FROM dream_keywords k
            LEFT JOIN dream_keyword_stats s ON k.id = s.keyword_id
            WHERE k.status = 'active'
        """
        
        conditions = []
        params = []
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰ ì¡°ê±´
        search_term = query.keyword.strip().lower()
        if search_term:
            # ì •í™•í•œ ì¼ì¹˜ ìš°ì„ , ë¶€ë¶„ ì¼ì¹˜ ë³´ì¡°
            conditions.append("""
                (k.keyword_normalized LIKE %s 
                 OR k.keyword LIKE %s 
                 OR EXISTS (
                     SELECT 1 FROM unnest(k.variations) AS var 
                     WHERE lower(var) LIKE %s
                 ))
            """)
            like_pattern = f"%{search_term}%"
            params.extend([like_pattern, like_pattern, like_pattern])
        
        # ì¹´í…Œê³ ë¦¬ í•„í„°
        if query.category:
            conditions.append("k.category_id = %s")
            params.append(query.category)
        
        # í’ˆì§ˆ ì„ê³„ê°’
        if query.quality_threshold:
            conditions.append("k.quality_score >= %s")
            params.append(query.quality_threshold)
        
        # ì¡°ê±´ ì¶”ê°€
        if conditions:
            base_query += " AND " + " AND ".join(conditions)
        
        # ì •ë ¬ ë° ì œí•œ
        base_query += """
            ORDER BY 
                CASE 
                    WHEN k.keyword_normalized = %s THEN 1  -- ì •í™•í•œ ì¼ì¹˜ ìš°ì„ 
                    WHEN k.keyword_normalized LIKE %s THEN 2  -- ì‹œì‘ ì¼ì¹˜
                    ELSE 3 
                END,
                k.quality_score DESC,
                COALESCE(s.search_count, 0) DESC
            LIMIT %s OFFSET %s
        """
        params.extend([search_term, f"{search_term}%", query.limit, query.offset])
        
        # PostgreSQL ì¿¼ë¦¬ ì‹¤í–‰
        results = self._execute_pg_query(base_query, params)
        
        # ê²°ê³¼ ì²˜ë¦¬
        search_results = []
        for row in results:
            keyword_id, keyword, category, quality_score, frequency_score, search_count = row
            
            # í•´ì„ ë°ì´í„° ì¡°íšŒ
            interpretations = self._get_keyword_interpretations(keyword_id, query.interpretation_types)
            
            # ê´€ë ¨ í‚¤ì›Œë“œ ì¡°íšŒ  
            related_keywords = self._get_related_keywords(keyword_id)
            
            # ë§¤ì¹˜ ìŠ¤ì½”ì–´ ê³„ì‚°
            match_score = self._calculate_match_score(query.keyword, keyword)
            
            # ê°ì • í•„í„° ì ìš©
            if query.sentiment_filter:
                interpretations = [i for i in interpretations if i['sentiment'] == query.sentiment_filter]
                if not interpretations:  # í•´ë‹¹ ê°ì •ì˜ í•´ì„ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                    continue
            
            search_result = SearchResult(
                keyword_id=keyword_id,
                keyword=keyword,
                category=category,
                quality_score=float(quality_score) if quality_score else 8.0,
                interpretations=interpretations,
                related_keywords=related_keywords,
                search_count=search_count or 0,
                match_score=match_score
            )
            
            search_results.append(search_result)
        
        return search_results
    
    def _execute_pg_query(self, query: str, params: List) -> List[Tuple]:
        """PostgreSQL ì¿¼ë¦¬ ì‹¤í–‰"""
        try:
            # íŒŒë¼ë¯¸í„°ë¥¼ SQL ë¬¸ìì—´ì— ì•ˆì „í•˜ê²Œ ì‚½ì…
            # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” psycopg2ë‚˜ ë‹¤ë¥¸ ì•ˆì „í•œ ë°©ë²• ì‚¬ìš© ê¶Œì¥
            safe_params = []
            for param in params:
                if isinstance(param, str):
                    safe_params.append(f"'{param.replace('\'', '\'\'')}'")
                else:
                    safe_params.append(str(param))
            
            # %së¥¼ ì‹¤ì œ ê°’ìœ¼ë¡œ ì¹˜í™˜
            formatted_query = query
            for param in safe_params:
                formatted_query = formatted_query.replace('%s', param, 1)
            
            # PostgreSQL ì‹¤í–‰
            cmd = [
                'sudo', '-u', 'postgres', 'psql', '-d', 'dream_service', 
                '-t', '-A', '-F', '|', '-c', formatted_query
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode != 0:
                self.logger.error(f"PostgreSQL ì¿¼ë¦¬ ì‹¤í–‰ ì˜¤ë¥˜: {result.stderr}")
                return []
            
            # ê²°ê³¼ íŒŒì‹±
            rows = []
            for line in result.stdout.strip().split('\n'):
                if line.strip():
                    # None ê°’ ì²˜ë¦¬
                    values = []
                    for value in line.split('|'):
                        if value.strip() == '':
                            values.append(None)
                        else:
                            # ìˆ«ì ë³€í™˜ ì‹œë„
                            try:
                                if '.' in value:
                                    values.append(float(value))
                                else:
                                    values.append(int(value))
                            except ValueError:
                                values.append(value)
                    rows.append(tuple(values))
            
            return rows
            
        except Exception as e:
            self.logger.error(f"ì¿¼ë¦¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return []
    
    def _get_keyword_interpretations(self, keyword_id: int, 
                                   types_filter: Optional[List[str]] = None) -> List[Dict]:
        """í‚¤ì›Œë“œ í•´ì„ ì¡°íšŒ"""
        query = """
            SELECT interpretation_type, interpretation_text, sentiment, confidence_score
            FROM dream_interpretations
            WHERE keyword_id = %s
        """
        
        if types_filter:
            placeholders = ','.join([f"'{t}'" for t in types_filter])
            query += f" AND interpretation_type IN ({placeholders})"
        
        query += " ORDER BY interpretation_type"
        
        results = self._execute_pg_query(query, [keyword_id])
        
        interpretations = []
        for row in results:
            if row:  # ë¹ˆ í–‰ ì œì™¸
                interp_type, text, sentiment, confidence = row
                interpretations.append({
                    'type': interp_type,
                    'text': text,
                    'sentiment': sentiment,
                    'confidence': float(confidence) if confidence else 8.0
                })
        
        return interpretations
    
    def _get_related_keywords(self, keyword_id: int) -> List[str]:
        """ê´€ë ¨ í‚¤ì›Œë“œ ì¡°íšŒ"""
        query = """
            SELECT k.keyword
            FROM dream_keyword_relations r
            JOIN dream_keywords k ON r.target_keyword_id = k.id
            WHERE r.source_keyword_id = %s
            ORDER BY r.strength DESC
            LIMIT 5
        """
        
        results = self._execute_pg_query(query, [keyword_id])
        return [row[0] for row in results if row]
    
    def _calculate_match_score(self, query_keyword: str, result_keyword: str) -> float:
        """ê²€ìƒ‰ì–´ì™€ ê²°ê³¼ í‚¤ì›Œë“œ ê°„ì˜ ì¼ì¹˜ë„ ê³„ì‚°"""
        query_lower = query_keyword.lower().strip()
        result_lower = result_keyword.lower().strip()
        
        # ì •í™•í•œ ì¼ì¹˜
        if query_lower == result_lower:
            return 1.0
        
        # ì‹œì‘ ì¼ì¹˜
        if result_lower.startswith(query_lower):
            return 0.9
        
        # ë¶€ë¶„ ì¼ì¹˜
        if query_lower in result_lower:
            return 0.7
        
        # ê¸€ì ìˆ˜ ì°¨ì´ ê¸°ë°˜ ìœ ì‚¬ë„
        if len(query_lower) > 0:
            common_chars = sum(1 for a, b in zip(query_lower, result_lower) if a == b)
            max_len = max(len(query_lower), len(result_lower))
            return common_chars / max_len * 0.5
        
        return 0.0
    
    def _generate_suggestions(self, keyword: str) -> List[str]:
        """ê²€ìƒ‰ ì œì•ˆ ìƒì„±"""
        if not keyword or len(keyword) < 2:
            return []
        
        # ìœ ì‚¬í•œ í‚¤ì›Œë“œ ê²€ìƒ‰
        query = """
            SELECT keyword
            FROM dream_keywords
            WHERE keyword_normalized LIKE %s
            AND keyword != %s
            ORDER BY quality_score DESC
            LIMIT 5
        """
        
        like_pattern = f"%{keyword.lower()}%"
        results = self._execute_pg_query(query, [like_pattern, keyword])
        
        return [row[0] for row in results if row]
    
    def _aggregate_categories(self, results: List[SearchResult]) -> List[Dict]:
        """ì¹´í…Œê³ ë¦¬ë³„ ê²°ê³¼ ì§‘ê³„"""
        category_counts = {}
        
        for result in results:
            category = result.category
            if category in category_counts:
                category_counts[category] += 1
            else:
                category_counts[category] = 1
        
        # ì¹´í…Œê³ ë¦¬ëª… í•œêµ­ì–´ ë³€í™˜ (ê°„ë‹¨í•œ ë§¤í•‘)
        category_names = {
            'water': 'ë¬¼ ê´€ë ¨', 'fire': 'ë¶ˆ ê´€ë ¨', 'zodiac_animals': 'ì‹­ì´ì§€ì‹ ',
            'family': 'ê°€ì¡±', 'money': 'ëˆ/ì¬ë¬¼', 'animals': 'ë™ë¬¼',
            'body_parts': 'ì‹ ì²´', 'emotions': 'ê°ì •'
        }
        
        categories = []
        for category, count in sorted(category_counts.items(), key=lambda x: x[1], reverse=True):
            categories.append({
                'category': category,
                'name': category_names.get(category, category),
                'count': count
            })
        
        return categories
    
    def _log_search(self, search_term: str, result_count: int, response_time: int):
        """ê²€ìƒ‰ ë¡œê·¸ ê¸°ë¡"""
        try:
            log_query = """
                INSERT INTO dream_search_logs 
                (search_term, result_count, response_time_ms, created_at)
                VALUES (%s, %s, %s, NOW())
            """
            
            self._execute_pg_query(log_query, [search_term, result_count, response_time])
            
        except Exception as e:
            self.logger.error(f"ê²€ìƒ‰ ë¡œê·¸ ê¸°ë¡ ì˜¤ë¥˜: {e}")
    
    def _generate_cache_key(self, query: SearchQuery) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        query_dict = asdict(query)
        query_str = json.dumps(query_dict, sort_keys=True)
        return hashlib.md5(query_str.encode()).hexdigest()
    
    def _get_from_cache(self, cache_key: str) -> Optional[SearchResponse]:
        """ìºì‹œì—ì„œ ì¡°íšŒ"""
        if cache_key in self.cache:
            cached_data, timestamp = self.cache[cache_key]
            if time.time() - timestamp < self.cache_ttl:
                return cached_data
            else:
                del self.cache[cache_key]  # ë§Œë£Œëœ ìºì‹œ ì‚­ì œ
        return None
    
    def _save_to_cache(self, cache_key: str, response: SearchResponse):
        """ìºì‹œì— ì €ì¥"""
        self.cache[cache_key] = (response, time.time())
        
        # ìºì‹œ í¬ê¸° ì œí•œ (100ê°œ í•­ëª©)
        if len(self.cache) > 100:
            oldest_key = min(self.cache.keys(), key=lambda k: self.cache[k][1])
            del self.cache[oldest_key]
    
    def get_popular_keywords(self, limit: int = 10) -> List[Dict]:
        """ì¸ê¸° í‚¤ì›Œë“œ ì¡°íšŒ"""
        query = """
            SELECT k.keyword, k.category_id, s.search_count
            FROM dream_keywords k
            JOIN dream_keyword_stats s ON k.id = s.keyword_id
            WHERE s.search_count > 0
            ORDER BY s.search_count DESC, k.quality_score DESC
            LIMIT %s
        """
        
        results = self._execute_pg_query(query, [limit])
        
        popular = []
        for row in results:
            if row:
                keyword, category, search_count = row
                popular.append({
                    'keyword': keyword,
                    'category': category,
                    'search_count': search_count
                })
        
        return popular
    
    def get_category_stats(self) -> List[Dict]:
        """ì¹´í…Œê³ ë¦¬ë³„ í†µê³„"""
        query = """
            SELECT k.category_id, COUNT(*) as keyword_count,
                   AVG(k.quality_score) as avg_quality,
                   SUM(COALESCE(s.search_count, 0)) as total_searches
            FROM dream_keywords k
            LEFT JOIN dream_keyword_stats s ON k.id = s.keyword_id
            WHERE k.status = 'active'
            GROUP BY k.category_id
            ORDER BY keyword_count DESC
        """
        
        results = self._execute_pg_query(query, [])
        
        stats = []
        for row in results:
            if row:
                category, count, avg_quality, total_searches = row
                stats.append({
                    'category': category,
                    'keyword_count': count,
                    'avg_quality': float(avg_quality) if avg_quality else 8.0,
                    'total_searches': total_searches or 0
                })
        
        return stats
    
    def optimize_search_performance(self):
        """ê²€ìƒ‰ ì„±ëŠ¥ ìµœì í™” ì‘ì—…"""
        self.logger.info("ê²€ìƒ‰ ì„±ëŠ¥ ìµœì í™” ì‹œì‘")
        
        # 1. ê²€ìƒ‰ í†µê³„ ì—…ë°ì´íŠ¸
        update_stats_query = """
            INSERT INTO dream_keyword_stats (keyword_id, search_count, last_searched)
            SELECT keyword_id, COUNT(*), MAX(created_at)
            FROM dream_search_logs
            WHERE keyword_id IS NOT NULL
            GROUP BY keyword_id
            ON CONFLICT (keyword_id) 
            DO UPDATE SET 
                search_count = dream_keyword_stats.search_count + EXCLUDED.search_count,
                last_searched = EXCLUDED.last_searched
        """
        self._execute_pg_query(update_stats_query, [])
        
        # 2. ì˜¤ë˜ëœ ê²€ìƒ‰ ë¡œê·¸ ì •ë¦¬ (30ì¼ ì´ìƒ)
        cleanup_query = """
            DELETE FROM dream_search_logs 
            WHERE created_at < NOW() - INTERVAL '30 days'
        """
        self._execute_pg_query(cleanup_query, [])
        
        # 3. ìºì‹œ ì´ˆê¸°í™”
        self.cache.clear()
        
        self.logger.info("ê²€ìƒ‰ ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ")

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    optimizer = DreamSearchAPIOptimizer()
    
    print("ğŸ” ê¿ˆí’€ì´ ê²€ìƒ‰ API ìµœì í™” í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
    test_queries = [
        SearchQuery(keyword="ë¬¼", limit=5),
        SearchQuery(keyword="í˜¸ë‘ì´", category="zodiac_animals"),
        SearchQuery(keyword="ì•„ë²„ì§€", interpretation_types=["traditional", "modern"]),
        SearchQuery(keyword="ëˆ", sentiment_filter="positive", limit=3)
    ]
    
    for i, query in enumerate(test_queries, 1):
        print(f"\nğŸ”¸ í…ŒìŠ¤íŠ¸ {i}: '{query.keyword}'")
        
        response = optimizer.search_dreams(query)
        
        print(f"   ê²€ìƒ‰ ê²°ê³¼: {response.total_results}ê°œ")
        print(f"   ì‘ë‹µ ì‹œê°„: {response.response_time_ms}ms")
        
        for j, result in enumerate(response.results[:2]):  # ì²˜ìŒ 2ê°œë§Œ í‘œì‹œ
            print(f"   {j+1}. {result.keyword} (ì¹´í…Œê³ ë¦¬: {result.category}, í’ˆì§ˆ: {result.quality_score:.1f})")
            for interp in result.interpretations[:1]:  # ì²« ë²ˆì§¸ í•´ì„ë§Œ
                print(f"      â”” {interp['type']}: {interp['text'][:50]}...")
        
        if response.suggestions:
            print(f"   ì œì•ˆ: {', '.join(response.suggestions[:3])}")
    
    # ì¸ê¸° í‚¤ì›Œë“œ ì¡°íšŒ
    print(f"\nğŸ“Š ì¸ê¸° í‚¤ì›Œë“œ:")
    popular = optimizer.get_popular_keywords(5)
    for keyword_data in popular:
        print(f"   ğŸ”¥ {keyword_data['keyword']} ({keyword_data['search_count']}íšŒ)")
    
    # ì¹´í…Œê³ ë¦¬ í†µê³„
    print(f"\nğŸ“ˆ ì¹´í…Œê³ ë¦¬ í†µê³„:")
    category_stats = optimizer.get_category_stats()
    for stat in category_stats[:5]:
        print(f"   ğŸ“‚ {stat['category']}: {stat['keyword_count']}ê°œ í‚¤ì›Œë“œ, í’ˆì§ˆ {stat['avg_quality']:.1f}")
    
    # ì„±ëŠ¥ ìµœì í™” ì‹¤í–‰
    print(f"\nâš¡ ì„±ëŠ¥ ìµœì í™” ì‹¤í–‰ ì¤‘...")
    optimizer.optimize_search_performance()
    print("âœ… ìµœì í™” ì™„ë£Œ")