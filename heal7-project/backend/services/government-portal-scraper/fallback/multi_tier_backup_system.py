"""
ë‹¤ì¤‘ í‹°ì–´ í´ë°± ë° ë°±ì—… ì‹œìŠ¤í…œ
ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ ì‹œ ë‹¤ì¤‘ ë°±ì—…ì„ í†µí•œ ì™„ì „í•œ ë°ì´í„° ë³´í˜¸

Author: Paperwork AI Team
Version: 2.0.0
Date: 2025-08-23
"""

import asyncio
import aiofiles
import asyncpg
import json
import logging
import os
import time
import hashlib
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, AsyncGenerator
from dataclasses import dataclass, field
from enum import Enum
import aioredis
import boto3
from uuid import UUID, uuid4
import gzip
import shutil

logger = logging.getLogger(__name__)

class BackupTier(Enum):
    """ë°±ì—… í‹°ì–´ ì •ì˜"""
    PRIMARY = "primary"           # PostgreSQL JSONB
    SECONDARY = "secondary"       # File System JSON
    TERTIARY = "tertiary"         # Redis Cache
    QUATERNARY = "quaternary"     # Remote Backup (S3)

class BackupStatus(Enum):
    """ë°±ì—… ìƒíƒœ"""
    ACTIVE = "active"
    CORRUPTED = "corrupted"
    EXPIRED = "expired"
    RESTORING = "restoring"
    RESTORED = "restored"

@dataclass
class BackupRecord:
    """ë°±ì—… ë ˆì½”ë“œ"""
    backup_id: UUID = field(default_factory=uuid4)
    source_id: str = ""
    tier: BackupTier = BackupTier.PRIMARY
    data: Dict = field(default_factory=dict)
    metadata: Dict = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)
    expires_at: datetime = field(default_factory=lambda: datetime.now() + timedelta(days=30))
    status: BackupStatus = BackupStatus.ACTIVE
    checksum: str = ""
    
    def __post_init__(self):
        """ë°±ì—… ë ˆì½”ë“œ ìƒì„± í›„ ì²´í¬ì„¬ ê³„ì‚°"""
        if not self.checksum:
            self.checksum = self._calculate_checksum()
    
    def _calculate_checksum(self) -> str:
        """ë°ì´í„° ì²´í¬ì„¬ ê³„ì‚°"""
        data_str = json.dumps(self.data, sort_keys=True, ensure_ascii=False)
        return hashlib.sha256(data_str.encode()).hexdigest()
    
    def verify_integrity(self) -> bool:
        """ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦"""
        current_checksum = self._calculate_checksum()
        return current_checksum == self.checksum

@dataclass
class BackupConfig:
    """ë°±ì—… ì„¤ì •"""
    # íŒŒì¼ ì‹œìŠ¤í…œ ì„¤ì •
    filesystem_root: Path = Path("/var/backups/government-scraper")
    max_files_per_directory: int = 1000
    compression_enabled: bool = True
    
    # Redis ì„¤ì •
    redis_url: str = "redis://localhost:6379/1"
    redis_key_prefix: str = "gov_scraper:backup:"
    redis_ttl_seconds: int = 86400 * 7  # 7ì¼
    
    # ì›ê²© ë°±ì—… ì„¤ì • (S3)
    s3_bucket: Optional[str] = None
    s3_prefix: str = "government-scraper-backups/"
    s3_storage_class: str = "STANDARD_IA"
    
    # ì •ë¦¬ ì„¤ì •
    cleanup_interval_hours: int = 24
    max_backup_age_days: int = 30
    max_total_size_gb: float = 10.0

class FileSystemBackupManager:
    """íŒŒì¼ ì‹œìŠ¤í…œ ë°±ì—… ë§¤ë‹ˆì €"""
    
    def __init__(self, config: BackupConfig):
        self.config = config
        self.root_path = config.filesystem_root
        self._ensure_directories()
    
    def _ensure_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        directories = [
            self.root_path,
            self.root_path / "daily",
            self.root_path / "weekly", 
            self.root_path / "monthly",
            self.root_path / "recovery"
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
            
        logger.info(f"âœ… íŒŒì¼ ì‹œìŠ¤í…œ ë°±ì—… ë””ë ‰í† ë¦¬ ì¤€ë¹„ ì™„ë£Œ: {self.root_path}")
    
    async def save_backup(self, record: BackupRecord) -> str:
        """ë°±ì—…ì„ íŒŒì¼ ì‹œìŠ¤í…œì— ì €ì¥"""
        try:
            # ë‚ ì§œë³„ ë””ë ‰í† ë¦¬ êµ¬ì„±
            date_str = record.created_at.strftime("%Y-%m-%d")
            daily_dir = self.root_path / "daily" / date_str
            daily_dir.mkdir(parents=True, exist_ok=True)
            
            # íŒŒì¼ëª… ìƒì„± (ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•œ UUID í¬í•¨)
            filename = f"{record.source_id}_{record.backup_id.hex[:8]}.json"
            if self.config.compression_enabled:
                filename += ".gz"
            
            file_path = daily_dir / filename
            
            # ë°±ì—… ë°ì´í„° ì¤€ë¹„
            backup_data = {
                'backup_id': str(record.backup_id),
                'source_id': record.source_id,
                'tier': record.tier.value,
                'data': record.data,
                'metadata': record.metadata,
                'created_at': record.created_at.isoformat(),
                'expires_at': record.expires_at.isoformat(),
                'status': record.status.value,
                'checksum': record.checksum
            }
            
            # íŒŒì¼ ì €ì¥ (ì••ì¶• ì—¬ë¶€ì— ë”°ë¼)
            if self.config.compression_enabled:
                await self._save_compressed(file_path, backup_data)
            else:
                await self._save_json(file_path, backup_data)
            
            logger.info(f"ğŸ“ íŒŒì¼ ì‹œìŠ¤í…œ ë°±ì—… ì €ì¥ ì™„ë£Œ: {file_path}")
            return str(file_path)
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ì‹œìŠ¤í…œ ë°±ì—… ì‹¤íŒ¨: {str(e)}")
            raise
    
    async def _save_compressed(self, file_path: Path, data: Dict):
        """ì••ì¶•ëœ JSON íŒŒì¼ ì €ì¥"""
        json_str = json.dumps(data, ensure_ascii=False, indent=2)
        compressed_data = gzip.compress(json_str.encode('utf-8'))
        
        async with aiofiles.open(file_path, 'wb') as f:
            await f.write(compressed_data)
    
    async def _save_json(self, file_path: Path, data: Dict):
        """ì¼ë°˜ JSON íŒŒì¼ ì €ì¥"""
        async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(data, ensure_ascii=False, indent=2))
    
    async def load_backup(self, file_path: str) -> Optional[BackupRecord]:
        """íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ë°±ì—… ë¡œë“œ"""
        try:
            path = Path(file_path)
            if not path.exists():
                logger.warning(f"âš ï¸ ë°±ì—… íŒŒì¼ ì—†ìŒ: {file_path}")
                return None
            
            # íŒŒì¼ ë¡œë“œ (ì••ì¶• ì—¬ë¶€ í™•ì¸)
            if path.name.endswith('.gz'):
                data = await self._load_compressed(path)
            else:
                data = await self._load_json(path)
            
            # BackupRecord ê°ì²´ë¡œ ë³€í™˜
            return self._dict_to_backup_record(data)
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ì‹œìŠ¤í…œ ë°±ì—… ë¡œë“œ ì‹¤íŒ¨ {file_path}: {str(e)}")
            return None
    
    async def _load_compressed(self, file_path: Path) -> Dict:
        """ì••ì¶•ëœ JSON íŒŒì¼ ë¡œë“œ"""
        async with aiofiles.open(file_path, 'rb') as f:
            compressed_data = await f.read()
            
        json_str = gzip.decompress(compressed_data).decode('utf-8')
        return json.loads(json_str)
    
    async def _load_json(self, file_path: Path) -> Dict:
        """ì¼ë°˜ JSON íŒŒì¼ ë¡œë“œ"""
        async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
            content = await f.read()
            return json.loads(content)
    
    def _dict_to_backup_record(self, data: Dict) -> BackupRecord:
        """ë”•ì…”ë„ˆë¦¬ë¥¼ BackupRecordë¡œ ë³€í™˜"""
        return BackupRecord(
            backup_id=UUID(data['backup_id']),
            source_id=data['source_id'],
            tier=BackupTier(data['tier']),
            data=data['data'],
            metadata=data['metadata'],
            created_at=datetime.fromisoformat(data['created_at']),
            expires_at=datetime.fromisoformat(data['expires_at']),
            status=BackupStatus(data['status']),
            checksum=data['checksum']
        )
    
    async def list_backups(self, date_range: Optional[tuple] = None) -> List[str]:
        """ë°±ì—… íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        backup_files = []
        
        daily_dir = self.root_path / "daily"
        if not daily_dir.exists():
            return backup_files
        
        for date_dir in daily_dir.iterdir():
            if not date_dir.is_dir():
                continue
            
            # ë‚ ì§œ í•„í„°ë§
            if date_range:
                try:
                    dir_date = datetime.strptime(date_dir.name, "%Y-%m-%d").date()
                    start_date, end_date = date_range
                    if not (start_date <= dir_date <= end_date):
                        continue
                except ValueError:
                    continue
            
            # íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
            for file_path in date_dir.glob("*.json*"):
                backup_files.append(str(file_path))
        
        return sorted(backup_files)
    
    async def cleanup_expired_backups(self) -> int:
        """ë§Œë£Œëœ ë°±ì—… ì •ë¦¬"""
        cleaned_count = 0
        cutoff_date = datetime.now() - timedelta(days=self.config.max_backup_age_days)
        
        for backup_file in await self.list_backups():
            try:
                backup_record = await self.load_backup(backup_file)
                if backup_record and backup_record.expires_at < cutoff_date:
                    os.remove(backup_file)
                    cleaned_count += 1
                    logger.info(f"ğŸ§¹ ë§Œë£Œëœ ë°±ì—… ì‚­ì œ: {backup_file}")
                    
            except Exception as e:
                logger.error(f"âŒ ë°±ì—… ì •ë¦¬ ì‹¤íŒ¨ {backup_file}: {str(e)}")
        
        return cleaned_count

class RedisBackupManager:
    """Redis ìºì‹œ ë°±ì—… ë§¤ë‹ˆì €"""
    
    def __init__(self, config: BackupConfig):
        self.config = config
        self.redis_client = None
        self.key_prefix = config.redis_key_prefix
    
    async def initialize(self):
        """Redis í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            self.redis_client = await aioredis.from_url(
                self.config.redis_url,
                encoding="utf-8",
                decode_responses=True
            )
            await self.redis_client.ping()
            logger.info("âœ… Redis ë°±ì—… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ Redis ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            self.redis_client = None
    
    async def save_backup(self, record: BackupRecord) -> str:
        """Redisì— ë°±ì—… ì €ì¥"""
        if not self.redis_client:
            raise RuntimeError("Redis client not initialized")
        
        try:
            key = f"{self.key_prefix}{record.backup_id}"
            
            backup_data = {
                'backup_id': str(record.backup_id),
                'source_id': record.source_id,
                'tier': record.tier.value,
                'data': json.dumps(record.data, ensure_ascii=False),
                'metadata': json.dumps(record.metadata, ensure_ascii=False),
                'created_at': record.created_at.isoformat(),
                'expires_at': record.expires_at.isoformat(),
                'status': record.status.value,
                'checksum': record.checksum
            }
            
            # Hashë¡œ ì €ì¥í•˜ê³  TTL ì„¤ì •
            await self.redis_client.hset(key, mapping=backup_data)
            await self.redis_client.expire(key, self.config.redis_ttl_seconds)
            
            logger.info(f"ğŸ“‹ Redis ë°±ì—… ì €ì¥ ì™„ë£Œ: {key}")
            return key
            
        except Exception as e:
            logger.error(f"âŒ Redis ë°±ì—… ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            raise
    
    async def load_backup(self, backup_id: UUID) -> Optional[BackupRecord]:
        """Redisì—ì„œ ë°±ì—… ë¡œë“œ"""
        if not self.redis_client:
            logger.warning("âš ï¸ Redis í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ")
            return None
        
        try:
            key = f"{self.key_prefix}{backup_id}"
            data = await self.redis_client.hgetall(key)
            
            if not data:
                return None
            
            # BackupRecord ê°ì²´ë¡œ ë³€í™˜
            return BackupRecord(
                backup_id=UUID(data['backup_id']),
                source_id=data['source_id'],
                tier=BackupTier(data['tier']),
                data=json.loads(data['data']),
                metadata=json.loads(data['metadata']),
                created_at=datetime.fromisoformat(data['created_at']),
                expires_at=datetime.fromisoformat(data['expires_at']),
                status=BackupStatus(data['status']),
                checksum=data['checksum']
            )
            
        except Exception as e:
            logger.error(f"âŒ Redis ë°±ì—… ë¡œë“œ ì‹¤íŒ¨ {backup_id}: {str(e)}")
            return None
    
    async def list_backup_keys(self) -> List[str]:
        """Redis ë°±ì—… í‚¤ ëª©ë¡ ì¡°íšŒ"""
        if not self.redis_client:
            return []
        
        pattern = f"{self.key_prefix}*"
        return await self.redis_client.keys(pattern)
    
    async def close(self):
        """Redis ì—°ê²° ì¢…ë£Œ"""
        if self.redis_client:
            await self.redis_client.close()

class RemoteBackupManager:
    """ì›ê²© ë°±ì—… ë§¤ë‹ˆì € (S3)"""
    
    def __init__(self, config: BackupConfig):
        self.config = config
        self.s3_client = None
        
        if config.s3_bucket:
            try:
                import boto3
                self.s3_client = boto3.client('s3')
                logger.info(f"âœ… S3 ë°±ì—… ì‹œìŠ¤í…œ ì´ˆê¸°í™”: {config.s3_bucket}")
            except Exception as e:
                logger.warning(f"âš ï¸ S3 ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
    
    async def save_backup(self, record: BackupRecord) -> Optional[str]:
        """S3ì— ë°±ì—… ì €ì¥"""
        if not self.s3_client or not self.config.s3_bucket:
            logger.debug("S3 ë°±ì—… ë¹„í™œì„±í™”ë¨")
            return None
        
        try:
            # S3 í‚¤ ìƒì„±
            date_str = record.created_at.strftime("%Y/%m/%d")
            s3_key = f"{self.config.s3_prefix}{date_str}/{record.source_id}_{record.backup_id.hex}.json.gz"
            
            # ë°ì´í„° ì¤€ë¹„ ë° ì••ì¶•
            backup_data = {
                'backup_id': str(record.backup_id),
                'source_id': record.source_id,
                'data': record.data,
                'metadata': record.metadata,
                'created_at': record.created_at.isoformat(),
                'checksum': record.checksum
            }
            
            json_str = json.dumps(backup_data, ensure_ascii=False)
            compressed_data = gzip.compress(json_str.encode('utf-8'))
            
            # S3ì— ì—…ë¡œë“œ
            self.s3_client.put_object(
                Bucket=self.config.s3_bucket,
                Key=s3_key,
                Body=compressed_data,
                StorageClass=self.config.s3_storage_class,
                Metadata={
                    'source-id': record.source_id,
                    'backup-id': str(record.backup_id),
                    'created-at': record.created_at.isoformat()
                }
            )
            
            logger.info(f"â˜ï¸ S3 ë°±ì—… ì €ì¥ ì™„ë£Œ: {s3_key}")
            return s3_key
            
        except Exception as e:
            logger.error(f"âŒ S3 ë°±ì—… ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return None
    
    async def load_backup(self, s3_key: str) -> Optional[BackupRecord]:
        """S3ì—ì„œ ë°±ì—… ë¡œë“œ"""
        if not self.s3_client or not self.config.s3_bucket:
            return None
        
        try:
            response = self.s3_client.get_object(
                Bucket=self.config.s3_bucket,
                Key=s3_key
            )
            
            compressed_data = response['Body'].read()
            json_str = gzip.decompress(compressed_data).decode('utf-8')
            data = json.loads(json_str)
            
            return BackupRecord(
                backup_id=UUID(data['backup_id']),
                source_id=data['source_id'],
                data=data['data'],
                metadata=data['metadata'],
                created_at=datetime.fromisoformat(data['created_at']),
                checksum=data['checksum']
            )
            
        except Exception as e:
            logger.error(f"âŒ S3 ë°±ì—… ë¡œë“œ ì‹¤íŒ¨ {s3_key}: {str(e)}")
            return None

class MultiTierBackupSystem:
    """ë‹¤ì¤‘ í‹°ì–´ ë°±ì—… ì‹œìŠ¤í…œ í†µí•© ë§¤ë‹ˆì €"""
    
    def __init__(self, db_pool: asyncpg.Pool, config: BackupConfig = None):
        self.db_pool = db_pool
        self.config = config or BackupConfig()
        
        # í‹°ì–´ë³„ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.filesystem_manager = FileSystemBackupManager(self.config)
        self.redis_manager = RedisBackupManager(self.config)
        self.remote_manager = RemoteBackupManager(self.config)
        
        # í†µê³„
        self.stats = {
            'backups_created': 0,
            'backups_restored': 0,
            'backup_failures': 0,
            'integrity_checks': 0,
            'corruption_detected': 0
        }
    
    async def initialize(self):
        """ëª¨ë“  ë°±ì—… ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        await self.redis_manager.initialize()
        logger.info("ğŸš€ ë‹¤ì¤‘ í‹°ì–´ ë°±ì—… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def create_full_backup(self, source_id: str, data: Dict, metadata: Dict = None) -> BackupRecord:
        """ëª¨ë“  í‹°ì–´ì— ë°±ì—… ìƒì„±"""
        backup_record = BackupRecord(
            source_id=source_id,
            data=data,
            metadata=metadata or {}
        )
        
        backup_locations = {}
        
        try:
            # Primary: PostgreSQLì— ê¸°ë¡
            await self._save_to_database(backup_record)
            backup_locations[BackupTier.PRIMARY.value] = "database"
            
            # Secondary: íŒŒì¼ ì‹œìŠ¤í…œ
            fs_location = await self.filesystem_manager.save_backup(backup_record)
            backup_locations[BackupTier.SECONDARY.value] = fs_location
            
            # Tertiary: Redis
            redis_key = await self.redis_manager.save_backup(backup_record)
            backup_locations[BackupTier.TERTIARY.value] = redis_key
            
            # Quaternary: ì›ê²© ë°±ì—… (S3)
            s3_location = await self.remote_manager.save_backup(backup_record)
            if s3_location:
                backup_locations[BackupTier.QUATERNARY.value] = s3_location
            
            # ë°±ì—… ìœ„ì¹˜ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
            backup_record.metadata['backup_locations'] = backup_locations
            await self._update_database_backup_locations(backup_record)
            
            self.stats['backups_created'] += 1
            logger.info(f"âœ… ë‹¤ì¤‘ í‹°ì–´ ë°±ì—… ì™„ë£Œ: {backup_record.backup_id}")
            
            return backup_record
            
        except Exception as e:
            self.stats['backup_failures'] += 1
            logger.error(f"âŒ ë‹¤ì¤‘ í‹°ì–´ ë°±ì—… ì‹¤íŒ¨ {source_id}: {str(e)}")
            raise
    
    async def restore_from_any_tier(self, backup_id: UUID) -> Optional[BackupRecord]:
        """ëª¨ë“  í‹°ì–´ì—ì„œ ë°±ì—… ë³µêµ¬ ì‹œë„"""
        logger.info(f"ğŸ”„ ë°±ì—… ë³µêµ¬ ì‹œë„: {backup_id}")
        
        # Primary: PostgreSQL ë¨¼ì € ì‹œë„
        try:
            record = await self._load_from_database(backup_id)
            if record and record.verify_integrity():
                logger.info(f"âœ… Primary(DB)ì—ì„œ ë³µêµ¬ ì„±ê³µ: {backup_id}")
                self.stats['backups_restored'] += 1
                return record
        except Exception as e:
            logger.warning(f"âš ï¸ Primary ë³µêµ¬ ì‹¤íŒ¨: {str(e)}")
        
        # Tertiary: Redis ì‹œë„ (ë¹ ë¥¸ ì•¡ì„¸ìŠ¤)
        try:
            record = await self.redis_manager.load_backup(backup_id)
            if record and record.verify_integrity():
                logger.info(f"âœ… Tertiary(Redis)ì—ì„œ ë³µêµ¬ ì„±ê³µ: {backup_id}")
                # Primaryì— ë³µì›
                await self._save_to_database(record)
                self.stats['backups_restored'] += 1
                return record
        except Exception as e:
            logger.warning(f"âš ï¸ Tertiary ë³µêµ¬ ì‹¤íŒ¨: {str(e)}")
        
        # Secondary: íŒŒì¼ ì‹œìŠ¤í…œ ê²€ìƒ‰
        try:
            backup_files = await self.filesystem_manager.list_backups()
            for file_path in backup_files:
                if backup_id.hex in file_path:
                    record = await self.filesystem_manager.load_backup(file_path)
                    if record and record.backup_id == backup_id and record.verify_integrity():
                        logger.info(f"âœ… Secondary(FS)ì—ì„œ ë³µêµ¬ ì„±ê³µ: {backup_id}")
                        # Primaryì™€ Tertiaryì— ë³µì›
                        await self._save_to_database(record)
                        await self.redis_manager.save_backup(record)
                        self.stats['backups_restored'] += 1
                        return record
        except Exception as e:
            logger.warning(f"âš ï¸ Secondary ë³µêµ¬ ì‹¤íŒ¨: {str(e)}")
        
        # Quaternary: S3ì—ì„œ ì‹œë„ (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
        # TODO: S3ì—ì„œ ë°±ì—… IDë¡œ ê²€ìƒ‰í•˜ëŠ” ë¡œì§ ì¶”ê°€
        
        logger.error(f"âŒ ëª¨ë“  í‹°ì–´ì—ì„œ ë³µêµ¬ ì‹¤íŒ¨: {backup_id}")
        return None
    
    async def verify_backup_integrity(self, backup_id: UUID) -> Dict[str, Any]:
        """ëª¨ë“  í‹°ì–´ì˜ ë°±ì—… ë¬´ê²°ì„± ê²€ì¦"""
        verification_result = {
            'backup_id': str(backup_id),
            'timestamp': datetime.now().isoformat(),
            'tiers_checked': {},
            'corruption_detected': False,
            'recommendations': []
        }
        
        # ê° í‹°ì–´ë³„ ê²€ì¦
        tiers_to_check = [
            (BackupTier.PRIMARY, self._verify_database_backup),
            (BackupTier.TERTIARY, self._verify_redis_backup),
            (BackupTier.SECONDARY, self._verify_filesystem_backup)
        ]
        
        for tier, verify_func in tiers_to_check:
            try:
                tier_result = await verify_func(backup_id)
                verification_result['tiers_checked'][tier.value] = tier_result
                
                if not tier_result.get('integrity_ok', False):
                    verification_result['corruption_detected'] = True
                    
            except Exception as e:
                verification_result['tiers_checked'][tier.value] = {
                    'error': str(e),
                    'integrity_ok': False
                }
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        if verification_result['corruption_detected']:
            verification_result['recommendations'] = [
                "Backup corruption detected",
                "Consider restoring from alternate tier",
                "Run full backup verification"
            ]
        
        self.stats['integrity_checks'] += 1
        if verification_result['corruption_detected']:
            self.stats['corruption_detected'] += 1
        
        return verification_result
    
    async def _save_to_database(self, record: BackupRecord):
        """ë°ì´í„°ë² ì´ìŠ¤ì— ë°±ì—… ë ˆì½”ë“œ ì €ì¥"""
        query = """
            INSERT INTO backup_data_registry (
                backup_id, source_table, source_record_id, backup_method,
                backup_location, backup_data, metadata, backup_status,
                created_at, expires_at
            ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
            ON CONFLICT (backup_id) DO UPDATE SET
                backup_data = EXCLUDED.backup_data,
                metadata = EXCLUDED.metadata
        """
        
        async with self.db_pool.acquire() as conn:
            await conn.execute(
                query,
                record.backup_id,
                'raw_scraped_data',  # ì†ŒìŠ¤ í…Œì´ë¸”
                record.source_id,
                'hybrid',
                'database',
                json.dumps(record.data),
                json.dumps(record.metadata),
                record.status.value,
                record.created_at,
                record.expires_at
            )
    
    async def _load_from_database(self, backup_id: UUID) -> Optional[BackupRecord]:
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°±ì—… ë ˆì½”ë“œ ë¡œë“œ"""
        query = """
            SELECT backup_id, source_record_id, backup_data, metadata,
                   created_at, expires_at, backup_status
            FROM backup_data_registry
            WHERE backup_id = $1 AND backup_status = 'active'
        """
        
        async with self.db_pool.acquire() as conn:
            row = await conn.fetchrow(query, backup_id)
            
            if not row:
                return None
            
            return BackupRecord(
                backup_id=UUID(str(row['backup_id'])),
                source_id=str(row['source_record_id']),
                tier=BackupTier.PRIMARY,
                data=json.loads(row['backup_data']),
                metadata=json.loads(row['metadata']),
                created_at=row['created_at'],
                expires_at=row['expires_at'],
                status=BackupStatus(row['backup_status'])
            )
    
    async def _update_database_backup_locations(self, record: BackupRecord):
        """ë°ì´í„°ë² ì´ìŠ¤ì˜ ë°±ì—… ìœ„ì¹˜ ì •ë³´ ì—…ë°ì´íŠ¸"""
        query = """
            UPDATE backup_data_registry 
            SET metadata = $1
            WHERE backup_id = $2
        """
        
        async with self.db_pool.acquire() as conn:
            await conn.execute(
                query,
                json.dumps(record.metadata),
                record.backup_id
            )
    
    async def _verify_database_backup(self, backup_id: UUID) -> Dict:
        """ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ë¬´ê²°ì„± ê²€ì¦"""
        record = await self._load_from_database(backup_id)
        
        if not record:
            return {'integrity_ok': False, 'error': 'Backup not found in database'}
        
        integrity_ok = record.verify_integrity()
        
        return {
            'integrity_ok': integrity_ok,
            'checksum_match': integrity_ok,
            'found': True
        }
    
    async def _verify_redis_backup(self, backup_id: UUID) -> Dict:
        """Redis ë°±ì—… ë¬´ê²°ì„± ê²€ì¦"""
        record = await self.redis_manager.load_backup(backup_id)
        
        if not record:
            return {'integrity_ok': False, 'error': 'Backup not found in Redis'}
        
        integrity_ok = record.verify_integrity()
        
        return {
            'integrity_ok': integrity_ok,
            'checksum_match': integrity_ok,
            'found': True
        }
    
    async def _verify_filesystem_backup(self, backup_id: UUID) -> Dict:
        """íŒŒì¼ ì‹œìŠ¤í…œ ë°±ì—… ë¬´ê²°ì„± ê²€ì¦"""
        backup_files = await self.filesystem_manager.list_backups()
        
        for file_path in backup_files:
            if backup_id.hex in file_path:
                record = await self.filesystem_manager.load_backup(file_path)
                if record and record.backup_id == backup_id:
                    integrity_ok = record.verify_integrity()
                    return {
                        'integrity_ok': integrity_ok,
                        'checksum_match': integrity_ok,
                        'found': True,
                        'file_path': file_path
                    }
        
        return {'integrity_ok': False, 'error': 'Backup not found in filesystem'}
    
    async def run_cleanup_cycle(self) -> Dict[str, int]:
        """ì •ë¦¬ ì‚¬ì´í´ ì‹¤í–‰"""
        logger.info("ğŸ§¹ ë°±ì—… ì •ë¦¬ ì‚¬ì´í´ ì‹œì‘")
        
        cleanup_results = {
            'filesystem_cleaned': 0,
            'database_cleaned': 0,
            'redis_cleaned': 0
        }
        
        try:
            # íŒŒì¼ ì‹œìŠ¤í…œ ì •ë¦¬
            fs_cleaned = await self.filesystem_manager.cleanup_expired_backups()
            cleanup_results['filesystem_cleaned'] = fs_cleaned
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì •ë¦¬
            db_cleaned = await self._cleanup_database_backups()
            cleanup_results['database_cleaned'] = db_cleaned
            
            # RedisëŠ” TTLë¡œ ìë™ ì •ë¦¬ë¨
            
            logger.info(f"âœ… ë°±ì—… ì •ë¦¬ ì™„ë£Œ: {cleanup_results}")
            
        except Exception as e:
            logger.error(f"âŒ ë°±ì—… ì •ë¦¬ ì‹¤íŒ¨: {str(e)}")
        
        return cleanup_results
    
    async def _cleanup_database_backups(self) -> int:
        """ë°ì´í„°ë² ì´ìŠ¤ì˜ ë§Œë£Œëœ ë°±ì—… ì •ë¦¬"""
        query = """
            DELETE FROM backup_data_registry
            WHERE backup_status = 'expired' OR expires_at < CURRENT_TIMESTAMP
        """
        
        async with self.db_pool.acquire() as conn:
            result = await conn.execute(query)
            # PostgreSQLì˜ ê²½ìš° "DELETE n" í˜•íƒœë¡œ ë°˜í™˜
            count = int(result.split()[-1]) if result.startswith('DELETE') else 0
            return count
    
    def get_backup_statistics(self) -> Dict[str, Any]:
        """ë°±ì—… í†µê³„ ì¡°íšŒ"""
        return {
            'statistics': self.stats.copy(),
            'config': {
                'filesystem_root': str(self.config.filesystem_root),
                'max_backup_age_days': self.config.max_backup_age_days,
                'compression_enabled': self.config.compression_enabled,
                's3_enabled': self.config.s3_bucket is not None
            }
        }
    
    async def close(self):
        """ëª¨ë“  ì—°ê²° ì¢…ë£Œ"""
        await self.redis_manager.close()
        logger.info("ğŸ”’ ë‹¤ì¤‘ í‹°ì–´ ë°±ì—… ì‹œìŠ¤í…œ ì¢…ë£Œ")