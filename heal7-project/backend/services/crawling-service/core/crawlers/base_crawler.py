#!/usr/bin/env python3
"""
ğŸ•·ï¸ í¬ë¡¤ëŸ¬ ê¸°ë³¸ êµ¬ì¡°
ì¶”ìƒ ë² ì´ìŠ¤ í´ë˜ìŠ¤ ë° ë°ì´í„° êµ¬ì¡° ì •ì˜

Author: HEAL7 Development Team
Version: 1.0.0
Date: 2025-08-30
"""

import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Dict, Any, Optional, List
from enum import Enum
from datetime import datetime

logger = logging.getLogger(__name__)


class CrawlerType(Enum):
    """í¬ë¡¤ëŸ¬ íƒ€ì… ì •ì˜"""
    HTTPX = "httpx"
    PLAYWRIGHT = "playwright"
    SELENIUM = "selenium"


class CrawlStatus(Enum):
    """í¬ë¡¤ë§ ìƒíƒœ"""
    SUCCESS = "success"
    PARTIAL_SUCCESS = "partial_success"
    FAILED = "failed"
    ERROR = "error"


@dataclass
class CrawlResult:
    """í¬ë¡¤ë§ ê²°ê³¼ ë°ì´í„° êµ¬ì¡°"""
    success: bool
    status: CrawlStatus = CrawlStatus.FAILED
    
    # í¬ë¡¤ë§ ë°ì´í„°
    html: Optional[str] = None
    status_code: Optional[int] = None
    screenshot: Optional[bytes] = None
    
    # ì—ëŸ¬ ì •ë³´
    error: Optional[str] = None
    error_type: Optional[str] = None
    
    # ë©”íƒ€ë°ì´í„°
    metadata: Dict[str, Any] = field(default_factory=dict)
    crawler_used: Optional[CrawlerType] = None
    url: Optional[str] = None
    timestamp: Optional[str] = None
    
    # ì„±ëŠ¥ ì •ë³´
    response_time: Optional[float] = None
    attempts: int = 1
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now().isoformat()
        
        if self.success:
            self.status = CrawlStatus.SUCCESS
        elif self.html and len(self.html) > 100:
            self.status = CrawlStatus.PARTIAL_SUCCESS
        else:
            self.status = CrawlStatus.FAILED


@dataclass 
class CrawlConfig:
    """í¬ë¡¤ë§ ì„¤ì •"""
    url: str
    timeout: int = 30
    retries: int = 2
    headers: Dict[str, str] = field(default_factory=dict)
    
    # ìŠ¤í¬ë¦°ìƒ· ì„¤ì •
    screenshot: bool = False
    screenshot_full_page: bool = True
    
    # JavaScript ì„¤ì •
    wait_for_load: bool = True
    wait_for_selector: Optional[str] = None
    
    # Stealth ì„¤ì •
    stealth_mode: bool = False
    user_agent: Optional[str] = None
    
    def __post_init__(self):
        if not self.headers:
            self.headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }


class BaseCrawler(ABC):
    """í¬ë¡¤ëŸ¬ ì¶”ìƒ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, name: str):
        self.name = name
        self.logger = logging.getLogger(f"{__name__}.{name}")
        self.is_initialized = False
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'average_response_time': 0.0
        }
    
    @abstractmethod
    async def initialize(self):
        """í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        pass
    
    @abstractmethod
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        pass
    
    @abstractmethod
    async def crawl(self, config: CrawlConfig) -> CrawlResult:
        """ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜"""
        pass
    
    @abstractmethod
    async def can_handle(self, url: str) -> bool:
        """í•´ë‹¹ URL ì²˜ë¦¬ ê°€ëŠ¥ ì—¬ë¶€"""
        pass
    
    async def health_check(self) -> bool:
        """í¬ë¡¤ëŸ¬ ìƒíƒœ í™•ì¸"""
        try:
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìš”ì²­ìœ¼ë¡œ ìƒíƒœ í™•ì¸
            test_config = CrawlConfig(
                url="https://httpbin.org/status/200",
                timeout=10
            )
            result = await self.crawl(test_config)
            return result.success
        except Exception as e:
            self.logger.error(f"Health check failed: {e}")
            return False
    
    def update_stats(self, result: CrawlResult):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        self.stats['total_requests'] += 1
        
        if result.success:
            self.stats['successful_requests'] += 1
        else:
            self.stats['failed_requests'] += 1
        
        if result.response_time:
            # í‰ê·  ì‘ë‹µ ì‹œê°„ ê³„ì‚° (ì´ë™ í‰ê· )
            current_avg = self.stats['average_response_time']
            total_requests = self.stats['total_requests']
            
            self.stats['average_response_time'] = (
                (current_avg * (total_requests - 1) + result.response_time) / total_requests
            )
    
    def get_success_rate(self) -> float:
        """ì„±ê³µë¥  ë°˜í™˜"""
        if self.stats['total_requests'] == 0:
            return 0.0
        return self.stats['successful_requests'] / self.stats['total_requests']
    
    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        return {
            **self.stats,
            'success_rate': self.get_success_rate(),
            'crawler_type': self.name
        }


class CrawlerError(Exception):
    """í¬ë¡¤ëŸ¬ ì—ëŸ¬ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    pass


class InitializationError(CrawlerError):
    """ì´ˆê¸°í™” ì—ëŸ¬"""
    pass


class CrawlingError(CrawlerError):
    """í¬ë¡¤ë§ ì—ëŸ¬"""
    pass


class TimeoutError(CrawlerError):
    """íƒ€ì„ì•„ì›ƒ ì—ëŸ¬"""
    pass


class AntiBotDetectionError(CrawlerError):
    """ì•ˆí‹°ë´‡ ê°ì§€ ì—ëŸ¬"""
    pass


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤

def create_default_headers(user_agent: str = None) -> Dict[str, str]:
    """ê¸°ë³¸ HTTP í—¤ë” ìƒì„±"""
    if user_agent is None:
        user_agent = (
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
            '(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        )
    
    return {
        'User-Agent': user_agent,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }


def is_valid_url(url: str) -> bool:
    """URL ìœ íš¨ì„± ê²€ì‚¬"""
    import re
    url_pattern = re.compile(
        r'^https?://'  # http:// or https://
        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'  # domain...
        r'localhost|'  # localhost...
        r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # ...or ip
        r'(?::\d+)?'  # optional port
        r'(?:/?|[/?]\S+)$', re.IGNORECASE)
    return url_pattern.match(url) is not None


def estimate_content_type(url: str, html: str = None) -> str:
    """URLê³¼ ë‚´ìš©ìœ¼ë¡œ ì½˜í…ì¸  íƒ€ì… ì¶”ì •"""
    url_lower = url.lower()
    
    # API ì—”ë“œí¬ì¸íŠ¸
    if any(keyword in url_lower for keyword in ['api', '.json', '/rest/', '/graphql']):
        return 'api'
    
    # ë¬¸ì„œ íŒŒì¼
    if any(ext in url_lower for ext in ['.pdf', '.doc', '.docx', '.xls', '.xlsx']):
        return 'document'
    
    # ì´ë¯¸ì§€
    if any(ext in url_lower for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
        return 'image'
    
    # HTML ë‚´ìš© ë¶„ì„
    if html:
        html_lower = html.lower()
        
        # JavaScript ë Œë”ë§ í•„ìš”í•œ ì‚¬ì´íŠ¸
        if any(keyword in html_lower for keyword in ['<script', 'javascript:', 'ajax']):
            return 'dynamic'
        
        # ì •ì  HTML
        return 'static'
    
    return 'unknown'


def format_error_message(error: Exception, url: str, crawler_type: str) -> str:
    """ì—ëŸ¬ ë©”ì‹œì§€ í¬ë§·íŒ…"""
    return f"[{crawler_type.upper()}] {url} - {type(error).__name__}: {str(error)}"