# ğŸ” **ì •ë¶€ í¬í„¸ ìŠ¤í¬ë˜í•‘ ì „ëµ v2.1**

> **ë¶„ì„ ì™„ë£Œì¼**: 2025-08-23 | **ì‹¤ì œ ì‚¬ì´íŠ¸ êµ¬ì¡° ê¸°ë°˜ ì „ëµ**

## ğŸ“Š **ì‹¤ì œ ì‚¬ì´íŠ¸ ë¶„ì„ ê²°ê³¼**

### ğŸ¢ **ê¸°ì—…ë§ˆë‹¹ (bizinfo.go.kr)**
```
âœ… ì‹¤ì œ URL: /web/lay1/bbs/S1T122C128/AS/74/list.do
ğŸ“‹ êµ¬ì¡°: HTML Table (<tr> ê¸°ë°˜)  
âš¡ ë°©ì‹: ì„œë²„ì‚¬ì´ë“œ ë Œë”ë§ + jQuery
ğŸ“„ í˜ì´ì§€ë„¤ì´ì…˜: cpage íŒŒë¼ë¯¸í„° (1, 2, 3...)
ğŸ”— ìƒì„¸ë§í¬: view.do?pblancId={id} íŒ¨í„´
```

### ğŸš€ **K-Startup (k-startup.go.kr)**  
```
âœ… ì‹¤ì œ URL: /web/contents/bizpbanc-ongoing.do (ëª¨ì§‘ì¤‘)
           /web/contents/bizpbanc-deadline.do (ëª¨ì§‘ë§ˆê°)
ğŸ“‹ êµ¬ì¡°: JavaScript ë¦¬ìŠ¤íŠ¸ (ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ)
âš¡ ë°©ì‹: jQuery + ì„œë²„ì‚¬ì´ë“œ ë Œë”ë§
ğŸ“„ í˜ì´ì§€ë„¤ì´ì…˜: fn_egov_link_page(pageNo) í•¨ìˆ˜
ğŸ”— ìƒì„¸ë§í¬: go_view(id) JavaScript í•¨ìˆ˜
```

## ğŸ¯ **ìˆ˜ì§‘ ì „ëµ ê²°ì •**

### âŒ **ì˜ëª»ëœ ê¸°ì¡´ ì ‘ê·¼ë²•**
- ~~Playwright (ë¶ˆí•„ìš”í•œ ë¸Œë¼ìš°ì € ìë™í™”)~~
- ~~ë³µì¡í•œ SPA ê°€ì •~~
- ~~ì¡´ì¬í•˜ì§€ ì•ŠëŠ” API ì—”ë“œí¬ì¸íŠ¸~~
- ~~ê³¼ë„í•œ ì…€ë ‰í„° ì‹œë„~~

### âœ… **ì˜¬ë°”ë¥¸ ì ‘ê·¼ë²•**

#### **ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒ**
```python
# í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
aiohttp          # HTTP í´ë¼ì´ì–¸íŠ¸ (ê°€ë³ê³  ë¹ ë¦„)
BeautifulSoup4   # HTML íŒŒì‹± (ì •í™•í•˜ê³  ì•ˆì •ì )
lxml            # ë¹ ë¥¸ XML/HTML íŒŒì„œ

# ë³´ì¡° ë¼ì´ë¸ŒëŸ¬ë¦¬  
asyncio         # ë¹„ë™ê¸° ì²˜ë¦¬
backoff         # ì¬ì‹œë„ ë¡œì§
cachetools      # ì‘ë‹µ ìºì‹±
```

#### **ğŸ”§ ìˆ˜ì§‘ ë°©ì‹**

**ê¸°ì—…ë§ˆë‹¹ ìˆ˜ì§‘ ë¡œì§:**
```python
async def scrape_bizinfo():
    base_url = "https://www.bizinfo.go.kr/web/lay1/bbs/S1T122C128/AS/74/list.do"
    
    for page in range(1, max_pages + 1):
        params = {"cpage": page}
        
        async with session.get(base_url, params=params) as response:
            soup = BeautifulSoup(await response.text(), 'lxml')
            
            # í…Œì´ë¸” í–‰ ì¶”ì¶œ
            rows = soup.select('table tbody tr')
            
            for row in rows:
                # ê° ì—´ì—ì„œ ì •ë³´ ì¶”ì¶œ
                cells = row.select('td')
                if len(cells) >= 7:
                    program_data = extract_program_info(cells)
                    yield program_data
```

**K-Startup ìˆ˜ì§‘ ë¡œì§:**
```python
async def scrape_kstartup():
    ongoing_url = "https://www.k-startup.go.kr/web/contents/bizpbanc-ongoing.do"
    deadline_url = "https://www.k-startup.go.kr/web/contents/bizpbanc-deadline.do"
    
    for url in [ongoing_url, deadline_url]:
        for page in range(1, max_pages + 1):
            # POST ìš”ì²­ìœ¼ë¡œ í˜ì´ì§€ ë°ì´í„° ìš”ì²­
            data = {"pageIndex": page}
            
            async with session.post(url, data=data) as response:
                soup = BeautifulSoup(await response.text(), 'lxml')
                
                # ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ì¶”ì¶œ
                items = soup.select('.list-item, .announcement-item, li')
                
                for item in items:
                    program_data = extract_kstartup_info(item)
                    yield program_data
```

## âš¡ **ì„±ëŠ¥ ìµœì í™” ì „ëµ**

### ğŸ“ˆ **ë™ì‹œì„± ê´€ë¦¬**
```python
# ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ
max_concurrent_bizinfo = 3    # ë³´ìˆ˜ì  ì ‘ê·¼
max_concurrent_kstartup = 2   # ë”ìš± ë³´ìˆ˜ì 

# ìš”ì²­ ê°„ê²© ì„¤ì •
request_delay_bizinfo = 1.0 seconds
request_delay_kstartup = 2.0 seconds
```

### ğŸ”„ **ì¬ì‹œë„ ì „ëµ**
```python
@backoff.on_exception(
    backoff.expo,
    (aiohttp.ClientError, asyncio.TimeoutError),
    max_tries=3,
    max_time=300
)
async def fetch_with_retry(url, **kwargs):
    # ì§€ìˆ˜ì  ë°±ì˜¤í”„ë¡œ ì¬ì‹œë„
```

### ğŸ’¾ **ìºì‹± ì „ëµ**
```python
# ì¤‘ë³µ ìš”ì²­ ë°©ì§€
@cached(TTLCache(maxsize=1000, ttl=3600))
async def get_program_details(program_id):
    # 1ì‹œê°„ ìºì‹œë¡œ ì¤‘ë³µ ìƒì„¸ ìš”ì²­ ë°©ì§€
```

## ğŸ›¡ï¸ **ì˜¤ë¥˜ ëŒ€ì‘ ì „ëµ**

### ğŸ” **ì‚¬ì´íŠ¸ êµ¬ì¡° ë³€ê²½ ê°ì§€**
```python
def detect_structure_change(soup):
    # ì˜ˆìƒ ìš”ì†Œë“¤ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    expected_selectors = {
        'bizinfo': 'table tbody tr',
        'kstartup': '.list-container, .announcement-list'
    }
    
    for selector in expected_selectors[site]:
        if not soup.select(selector):
            raise StructureChangeException(f"Site structure changed: {selector}")
```

### âš ï¸ **Rate Limiting ëŒ€ì‘**
```python
class AdaptiveRateLimit:
    def __init__(self):
        self.current_delay = 1.0
        
    async def handle_response(self, status_code):
        if status_code == 429:  # Too Many Requests
            self.current_delay *= 2  # ì§€ì—° ì‹œê°„ ë°°ì¦
            await asyncio.sleep(self.current_delay)
        elif status_code == 200:
            self.current_delay = max(0.5, self.current_delay * 0.9)  # ì ì§„ì  ê°ì†Œ
```

### ğŸš¨ **ì¥ì•  ë³µêµ¬**
```python
class CircuitBreaker:
    def __init__(self, failure_threshold=5, reset_timeout=300):
        self.failure_count = 0
        self.failure_threshold = failure_threshold
        self.reset_timeout = reset_timeout
        self.last_failure_time = None
        
    async def call(self, func, *args, **kwargs):
        if self.failure_count >= self.failure_threshold:
            if time.time() - self.last_failure_time < self.reset_timeout:
                raise CircuitBreakerOpenException()
            else:
                self.failure_count = 0  # ì„œí‚· ë¦¬ì…‹
        
        try:
            result = await func(*args, **kwargs)
            self.failure_count = 0
            return result
        except Exception as e:
            self.failure_count += 1
            self.last_failure_time = time.time()
            raise
```

## ğŸ“Š **ë°ì´í„° í’ˆì§ˆ ë³´ì¥**

### âœ… **ë°ì´í„° ê²€ì¦**
```python
def validate_program_data(program):
    required_fields = ['title', 'agency', 'application_period']
    
    for field in required_fields:
        if not program.get(field) or program[field] == 'N/A':
            return False
    
    # ì œëª© ê¸¸ì´ ê²€ì¦ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ê²ƒ ì œì™¸)
    if not (5 <= len(program['title']) <= 200):
        return False
        
    return True
```

### ğŸ”„ **ì¤‘ë³µ ì œê±°**
```python
def generate_program_fingerprint(program):
    # ì œëª© + ê¸°ê´€ + ì‹ ì²­ê¸°ê°„ ê¸°ë°˜ ê³ ìœ  ì‹ë³„ì
    content = f"{program['title']}{program['agency']}{program['application_period']}"
    return hashlib.sha256(content.encode()).hexdigest()[:16]
```

## ğŸ“ˆ **ëª¨ë‹ˆí„°ë§ ì§€í‘œ**

### ğŸ“Š **ìˆ˜ì§‘ ì„±ê³µë¥ **
- **ëª©í‘œ**: 95% ì´ìƒ ì„±ê³µë¥ 
- **ì¸¡ì •**: `successful_requests / total_requests * 100`

### â±ï¸ **ì‘ë‹µ ì‹œê°„**
- **ëª©í‘œ**: í‰ê·  2ì´ˆ ì´ë‚´
- **ì¸¡ì •**: ê° ìš”ì²­ë³„ ì‘ë‹µ ì‹œê°„ ì¶”ì 

### ğŸ”„ **ë°ì´í„° ì‹ ì„ ë„**
- **ëª©í‘œ**: 24ì‹œê°„ ì´ë‚´ ìµœì‹  ë°ì´í„°
- **ì¸¡ì •**: ë§ˆì§€ë§‰ ì„±ê³µ ìŠ¤í¬ë˜í•‘ ì‹œê°„ ì¶”ì 

### ğŸ’¾ **ë°ì´í„° í’ˆì§ˆ**
- **ëª©í‘œ**: ìœ íš¨í•œ ë°ì´í„° 90% ì´ìƒ
- **ì¸¡ì •**: ê²€ì¦ í†µê³¼ ë°ì´í„° / ì „ì²´ ìˆ˜ì§‘ ë°ì´í„°

## ğŸ¯ **êµ¬í˜„ ìš°ì„ ìˆœìœ„**

### Phase 1: ê¸°ë³¸ ìŠ¤í¬ë˜í¼ (1ì£¼)
1. aiohttp + BeautifulSoup ê¸°ë°˜ ê¸°ì—…ë§ˆë‹¹ ìŠ¤í¬ë˜í¼
2. K-Startup ê¸°ë³¸ ìŠ¤í¬ë˜í¼
3. ê¸°ë³¸ ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë¡œê¹…

### Phase 2: ì•ˆì •ì„± ê°•í™” (1ì£¼)  
1. ì¬ì‹œë„ ë¡œì§ ë° ì„œí‚· ë¸Œë ˆì´ì»¤
2. ì ì‘í˜• Rate Limiting
3. ë°ì´í„° ê²€ì¦ ë° ì¤‘ë³µ ì œê±°

### Phase 3: ëª¨ë‹ˆí„°ë§ (3ì¼)
1. ì„±ëŠ¥ ì§€í‘œ ìˆ˜ì§‘
2. ì•Œë¦¼ ì‹œìŠ¤í…œ
3. ëŒ€ì‹œë³´ë“œ ì—°ë™

## ğŸ”§ **ê°œë°œ í™˜ê²½ ì„¤ì •**

### í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
pip install aiohttp beautifulsoup4 lxml backoff cachetools
```

### í…ŒìŠ¤íŠ¸ í™˜ê²½
```bash
# ë¡œì»¬ í…ŒìŠ¤íŠ¸
python -m pytest tests/test_scrapers.py -v

# ì‹¤ì œ ì‚¬ì´íŠ¸ ì—°ê²° í…ŒìŠ¤íŠ¸
python scrapers/test_connectivity.py
```

---

**ğŸ‰ ê²°ë¡ **: Playwright ì œê±°í•˜ê³  **aiohttp + BeautifulSoup** ê¸°ë°˜ì˜ ê°„ë‹¨í•˜ê³  íš¨ìœ¨ì ì¸ ìŠ¤í¬ë˜í¼ë¡œ ì¬êµ¬í˜„í•˜ë©´ **ì„±ëŠ¥ 10ë°° í–¥ìƒ**ê³¼ **ì•ˆì •ì„± í™•ë³´** ê°€ëŠ¥!