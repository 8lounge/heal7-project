#!/usr/bin/env python3
"""
ğŸš€ ì‹¤ì œ í¬ë¡¤ë§ ë° JSONB ì €ì¥ í…ŒìŠ¤íŠ¸
- í•˜ë“œì½”ë”© ì œê±°, ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘
- PostgreSQL JSONB ì €ì¥ í™•ì¸
- ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì—°ë™

Author: HEAL7 Development Team
Version: 1.0.0
Date: 2025-08-31
"""

import asyncio
import json
import logging
import hashlib
import uuid
from datetime import datetime
from pathlib import Path
import asyncpg
import sys

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append('.')

from core.smart_crawler import SmartCrawler, CrawlStrategy

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class RealCrawlingService:
    """ì‹¤ì œ í¬ë¡¤ë§ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.crawler = None
        self.db_pool = None
        
    async def initialize(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        logger.info("ğŸš€ ì‹¤ì œ í¬ë¡¤ë§ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹œì‘")
        
        # SmartCrawler ì´ˆê¸°í™”
        self.crawler = SmartCrawler()
        await self.crawler.initialize()
        logger.info("âœ… SmartCrawler ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        try:
            await self.connect_database()
        except Exception as e:
            logger.error(f"âŒ DB ì—°ê²° ì‹¤íŒ¨, ê³„ì† ì§„í–‰: {e}")
            # DB ì—°ê²° ì‹¤íŒ¨í•´ë„ í¬ë¡¤ë§ì€ ê³„ì† ì§„í–‰
        
    async def connect_database(self):
        """PostgreSQL ì—°ê²°"""
        try:
            # ì—¬ëŸ¬ ì—°ê²° ë°©ë²• ì‹œë„
            connection_configs = [
                # 1. íŒ¨ìŠ¤ì›Œë“œ ì—†ì´ ì‹œë„
                {'host': 'localhost', 'port': 5432, 'user': 'postgres', 'database': 'heal7'},
                # 2. ì†Œì¼“ ì—°ê²° ì‹œë„  
                {'host': '/var/run/postgresql', 'port': 5432, 'user': 'postgres', 'database': 'heal7'},
                # 3. ë‹¤ë¥¸ ì‚¬ìš©ìë¡œ ì‹œë„
                {'host': 'localhost', 'port': 5432, 'user': 'ubuntu', 'database': 'heal7'},
            ]
            
            for i, config in enumerate(connection_configs):
                try:
                    logger.info(f"DB ì—°ê²° ì‹œë„ {i+1}: {config}")
                    self.db_pool = await asyncpg.create_pool(min_size=1, max_size=3, **config)
                    logger.info(f"âœ… PostgreSQL ì—°ê²° ì„±ê³µ (ë°©ë²• {i+1})")
                    return
                except Exception as conn_err:
                    logger.warning(f"âš ï¸ ì—°ê²° ë°©ë²• {i+1} ì‹¤íŒ¨: {conn_err}")
                    
            raise Exception("ëª¨ë“  DB ì—°ê²° ë°©ë²• ì‹¤íŒ¨")
            
        except Exception as e:
            logger.error(f"âŒ DB ì—°ê²° ì‹¤íŒ¨: {e}")
            self.db_pool = None
            raise
            
    async def crawl_and_store(self, url: str, source_type: str = "test"):
        """ì‹¤ì œ í¬ë¡¤ë§ ë° JSONB ì €ì¥"""
        logger.info(f"ğŸ“¡ ì‹¤ì œ í¬ë¡¤ë§ ì‹œì‘: {url}")
        
        try:
            # 1. ì‹¤ì œ í¬ë¡¤ë§ ì‹¤í–‰
            result = await self.crawler.crawl(url, strategy=CrawlStrategy.AUTO)
            
            if not result.success:
                logger.error(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {result.error}")
                return False
                
            logger.info(f"âœ… í¬ë¡¤ë§ ì„±ê³µ ({result.crawler_used.value}): {len(result.html)} bytes")
            
            # 2. ë°ì´í„° ì¤€ë¹„
            crawl_data = {
                "id": str(uuid.uuid4()),
                "title": f"í¬ë¡¤ë§ ë°ì´í„° - {url}",
                "content": result.html[:5000],  # ì²˜ìŒ 5000ìë§Œ ì €ì¥
                "url": url,
                "source_type": source_type,
                "metadata": {
                    "crawler_used": result.crawler_used.value,
                    "response_time": result.response_time,
                    "status_code": getattr(result, 'status_code', 200),
                    "html_size": len(result.html) if result.html else 0,
                    "headers": getattr(result, 'headers', {}),
                    "crawl_timestamp": datetime.now().isoformat()
                },
                "ai_processed_data": {
                    "processed": False,
                    "extraction_ready": True,
                    "quality_indicators": {
                        "has_content": bool(result.html and len(result.html) > 100),
                        "response_success": True,
                        "crawler_tier": result.crawler_used.value
                    }
                },
                "hash_key": hashlib.md5(f"{url}_{datetime.now().date()}".encode()).hexdigest(),
                "quality_score": 95.0 if len(result.html) > 1000 else 75.0,
                "processing_status": "completed",
                "collected_at": datetime.now()
            }
            
            # 3. ë°ì´í„° ì €ì¥ (DB ë˜ëŠ” íŒŒì¼)
            if self.db_pool:
                await self.store_to_database(crawl_data)
                logger.info("âœ… PostgreSQL JSONB ì €ì¥ ì™„ë£Œ")
            else:
                # DB ì—°ê²° ì‹¤íŒ¨ ì‹œ íŒŒì¼ë¡œ ì €ì¥
                await self.store_to_file(crawl_data)
                logger.info("âœ… íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ (DB ì—°ê²° ì—†ìŒ)")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ í¬ë¡¤ë§ ë° ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False
            
    async def store_to_database(self, crawl_data):
        """PostgreSQL JSONBì— ì‹¤ì œ ì €ì¥"""
        try:
            async with self.db_pool.acquire() as conn:
                # crawling_service ìŠ¤í‚¤ë§ˆì— ì €ì¥
                query = """
                INSERT INTO crawling_service.crawl_data 
                (id, title, content, url, source_type, metadata, ai_processed_data, 
                 hash_key, quality_score, processing_status, collected_at)
                VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11)
                """
                
                await conn.execute(
                    query,
                    crawl_data["id"],
                    crawl_data["title"], 
                    crawl_data["content"],
                    crawl_data["url"],
                    crawl_data["source_type"],
                    json.dumps(crawl_data["metadata"]),
                    json.dumps(crawl_data["ai_processed_data"]),
                    crawl_data["hash_key"],
                    crawl_data["quality_score"],
                    crawl_data["processing_status"],
                    crawl_data["collected_at"]
                )
                
                logger.info(f"âœ… JSONB ì €ì¥ ì™„ë£Œ: {crawl_data['id']}")
                
        except Exception as e:
            logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
            
    async def store_to_file(self, crawl_data):
        """íŒŒì¼ë¡œ ì‹¤ì œ í¬ë¡¤ë§ ë°ì´í„° ì €ì¥"""
        try:
            # ì‹¤ì œ ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
            data_dir = Path("./data/real_crawling")
            data_dir.mkdir(parents=True, exist_ok=True)
            
            # íŒŒì¼ëª… ìƒì„± (URL ê¸°ë°˜)
            url_safe = crawl_data['url'].replace('https://', '').replace('http://', '').replace('/', '_').replace(':', '_')
            filename = f"real_{crawl_data['source_type']}_{url_safe}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            filepath = data_dir / filename
            
            # JSONB í˜•íƒœë¡œ ì €ì¥ (ì‹¤ì œ í¬ë¡¤ë§ ë°ì´í„°)
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(crawl_data, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"ğŸ’¾ ì‹¤ì œ í¬ë¡¤ë§ ë°ì´í„° íŒŒì¼ ì €ì¥: {filepath}")
            
            # ê°„ë‹¨í•œ í†µê³„ë„ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥
            stats_file = data_dir / "real_crawling_stats.json"
            
            # ê¸°ì¡´ í†µê³„ ë¡œë“œí•˜ê±°ë‚˜ ìƒˆë¡œ ìƒì„±
            if stats_file.exists():
                with open(stats_file, 'r', encoding='utf-8') as f:
                    stats = json.load(f)
            else:
                stats = {"total_crawled": 0, "by_source": {}, "by_crawler": {}, "last_updated": None}
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            stats["total_crawled"] += 1
            stats["by_source"][crawl_data["source_type"]] = stats["by_source"].get(crawl_data["source_type"], 0) + 1
            crawler_used = crawl_data["metadata"]["crawler_used"]
            stats["by_crawler"][crawler_used] = stats["by_crawler"].get(crawler_used, 0) + 1
            stats["last_updated"] = datetime.now().isoformat()
            
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(stats, f, indent=2, ensure_ascii=False)
                
            logger.info(f"ğŸ“Š ì‹¤ì œ í¬ë¡¤ë§ í†µê³„ ì—…ë°ì´íŠ¸: {stats}")
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            
    async def verify_stored_data(self):
        """ì €ì¥ëœ JSONB ë°ì´í„° ê²€ì¦ (DB ë˜ëŠ” íŒŒì¼)"""
        if self.db_pool:
            return await self.verify_database_data()
        else:
            return await self.verify_file_data()
    
    async def verify_database_data(self):
        """PostgreSQL DB ë°ì´í„° ê²€ì¦"""
        try:
            async with self.db_pool.acquire() as conn:
                # ìµœê·¼ ì €ì¥ëœ ë°ì´í„° ì¡°íšŒ
                query = """
                SELECT id, title, url, metadata, ai_processed_data, quality_score, collected_at
                FROM crawling_service.crawl_data 
                ORDER BY collected_at DESC 
                LIMIT 3
                """
                
                rows = await conn.fetch(query)
                
                logger.info(f"ğŸ“Š ì €ì¥ëœ ì‹¤ì œ ë°ì´í„°: {len(rows)}ê±´")
                
                for row in rows:
                    logger.info(f"  ğŸ” ID: {row['id']}")
                    logger.info(f"  ğŸ“ ì œëª©: {row['title']}")
                    logger.info(f"  ğŸŒ URL: {row['url']}")
                    logger.info(f"  ğŸ“Š í’ˆì§ˆì ìˆ˜: {row['quality_score']}")
                    logger.info(f"  ğŸ“… ìˆ˜ì§‘ì‹œê°„: {row['collected_at']}")
                    
                    # JSONB í•„ë“œ ë‚´ìš© í™•ì¸
                    metadata = row['metadata']
                    if isinstance(metadata, str):
                        metadata = json.loads(metadata)
                    logger.info(f"  ğŸ”§ ë©”íƒ€ë°ì´í„°: í¬ë¡¤ëŸ¬={metadata.get('crawler_used')}, í¬ê¸°={metadata.get('html_size')}")
                    
                    ai_data = row['ai_processed_data']
                    if isinstance(ai_data, str):
                        ai_data = json.loads(ai_data)
                    logger.info(f"  ğŸ¤– AIë°ì´í„°: ì²˜ë¦¬={ai_data.get('processed')}, í’ˆì§ˆì§€í‘œ={ai_data.get('quality_indicators')}")
                    
                    logger.info("  " + "-"*50)
                
                return len(rows)
                
        except Exception as e:
            logger.error(f"âŒ DB ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
            return 0
    
    async def verify_file_data(self):
        """íŒŒì¼ ì €ì¥ ë°ì´í„° ê²€ì¦"""
        try:
            data_dir = Path("./data/real_crawling")
            if not data_dir.exists():
                logger.warning("ğŸ“‚ ì‹¤ì œ í¬ë¡¤ë§ ë°ì´í„° ë””ë ‰í† ë¦¬ ì—†ìŒ")
                return 0
            
            # JSON íŒŒì¼ë“¤ ì°¾ê¸°
            json_files = list(data_dir.glob("real_*.json"))
            json_files = [f for f in json_files if 'stats' not in f.name]  # í†µê³„ íŒŒì¼ ì œì™¸
            
            logger.info(f"ğŸ“Š ì €ì¥ëœ ì‹¤ì œ íŒŒì¼ ë°ì´í„°: {len(json_files)}ê°œ")
            
            # ìµœê·¼ 3ê°œ íŒŒì¼ í™•ì¸
            json_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            
            for i, file_path in enumerate(json_files[:3]):
                logger.info(f"  ğŸ” íŒŒì¼ {i+1}: {file_path.name}")
                
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    logger.info(f"  ğŸ“ ì œëª©: {data.get('title', 'N/A')}")
                    logger.info(f"  ğŸŒ URL: {data.get('url', 'N/A')}")
                    logger.info(f"  ğŸ“Š í’ˆì§ˆì ìˆ˜: {data.get('quality_score', 'N/A')}")
                    logger.info(f"  ğŸ“… ìˆ˜ì§‘ì‹œê°„: {data.get('collected_at', 'N/A')}")
                    
                    # ë©”íƒ€ë°ì´í„° í™•ì¸
                    metadata = data.get('metadata', {})
                    logger.info(f"  ğŸ”§ ë©”íƒ€ë°ì´í„°: í¬ë¡¤ëŸ¬={metadata.get('crawler_used')}, í¬ê¸°={metadata.get('html_size')} bytes")
                    
                    # AI ë°ì´í„° í™•ì¸
                    ai_data = data.get('ai_processed_data', {})
                    quality_indicators = ai_data.get('quality_indicators', {})
                    logger.info(f"  ğŸ¤– AIë°ì´í„°: ì²˜ë¦¬={ai_data.get('processed')}, í¬ë¡¤ëŸ¬í‹°ì–´={quality_indicators.get('crawler_tier')}")
                    
                    # ì‹¤ì œ ì½˜í…ì¸  ì¼ë¶€ í™•ì¸
                    content = data.get('content', '')
                    content_preview = content[:100] + "..." if len(content) > 100 else content
                    logger.info(f"  ğŸ“„ ì½˜í…ì¸  ë¯¸ë¦¬ë³´ê¸°: {content_preview}")
                    
                except Exception as file_err:
                    logger.error(f"  âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {file_path}: {file_err}")
                
                logger.info("  " + "-"*50)
            
            # í†µê³„ íŒŒì¼ í™•ì¸
            stats_file = data_dir / "real_crawling_stats.json"
            if stats_file.exists():
                try:
                    with open(stats_file, 'r', encoding='utf-8') as f:
                        stats = json.load(f)
                    logger.info(f"ğŸ“ˆ ì‹¤ì œ í¬ë¡¤ë§ í†µê³„:")
                    logger.info(f"  ì´ í¬ë¡¤ë§: {stats.get('total_crawled', 0)}ê°œ")
                    logger.info(f"  ì†ŒìŠ¤ë³„: {stats.get('by_source', {})}")
                    logger.info(f"  í¬ë¡¤ëŸ¬ë³„: {stats.get('by_crawler', {})}")
                    logger.info(f"  ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {stats.get('last_updated', 'N/A')}")
                except Exception as stats_err:
                    logger.error(f"âŒ í†µê³„ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {stats_err}")
            
            return len(json_files)
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
            return 0
            
    async def get_real_statistics(self):
        """ì‹¤ì œ í†µê³„ ì¡°íšŒ (DB ë˜ëŠ” íŒŒì¼)"""
        if self.db_pool:
            return await self.get_database_statistics()
        else:
            return await self.get_file_statistics()
    
    async def get_database_statistics(self):
        """PostgreSQL DB í†µê³„ ì¡°íšŒ"""
        try:
            async with self.db_pool.acquire() as conn:
                stats_query = """
                SELECT 
                    COUNT(*) as total_count,
                    AVG(quality_score) as avg_quality,
                    COUNT(CASE WHEN processing_status = 'completed' THEN 1 END) as completed_count,
                    COUNT(CASE WHEN collected_at > NOW() - INTERVAL '1 hour' THEN 1 END) as recent_count
                FROM crawling_service.crawl_data
                """
                
                result = await conn.fetchrow(stats_query)
                
                success_rate = (result['completed_count'] / result['total_count'] * 100) if result['total_count'] > 0 else 0
                
                real_stats = {
                    "total_collected": result['total_count'],
                    "avg_success_rate": round(success_rate, 1),
                    "avg_quality": round(result['avg_quality'] or 0, 1),
                    "recent_collections": result['recent_count'],
                    "timestamp": datetime.now().isoformat(),
                    "data_source": "real_database"
                }
                
                logger.info(f"ğŸ“ˆ ì‹¤ì œ í†µê³„: {real_stats}")
                return real_stats
                
        except Exception as e:
            logger.error(f"âŒ DB í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    async def get_file_statistics(self):
        """íŒŒì¼ ê¸°ë°˜ í†µê³„ ì¡°íšŒ"""
        try:
            stats_file = Path("./data/real_crawling/real_crawling_stats.json")
            
            if stats_file.exists():
                with open(stats_file, 'r', encoding='utf-8') as f:
                    file_stats = json.load(f)
                
                # ì‹¤ì œ íŒŒì¼ ê¸°ë°˜ í†µê³„ ê³„ì‚°
                data_dir = Path("./data/real_crawling")
                json_files = list(data_dir.glob("real_*.json"))
                json_files = [f for f in json_files if 'stats' not in f.name]
                
                # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (íŒŒì¼ë“¤ì—ì„œ ì¶”ì¶œ)
                total_quality = 0
                quality_count = 0
                
                for file_path in json_files:
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                        quality_score = data.get('quality_score', 0)
                        if quality_score > 0:
                            total_quality += quality_score
                            quality_count += 1
                    except:
                        continue
                
                avg_quality = (total_quality / quality_count) if quality_count > 0 else 0
                
                real_stats = {
                    "total_collected": file_stats.get('total_crawled', 0),
                    "avg_success_rate": 100.0,  # íŒŒì¼ ì €ì¥ëœ ê²ƒì€ ëª¨ë‘ ì„±ê³µí•œ ê²ƒ
                    "avg_quality": round(avg_quality, 1),
                    "by_source": file_stats.get('by_source', {}),
                    "by_crawler": file_stats.get('by_crawler', {}),
                    "recent_collections": len(json_files),
                    "timestamp": file_stats.get('last_updated', datetime.now().isoformat()),
                    "data_source": "real_files"
                }
                
                logger.info(f"ğŸ“ˆ íŒŒì¼ ê¸°ë°˜ ì‹¤ì œ í†µê³„: {real_stats}")
                return real_stats
            else:
                return {
                    "total_collected": 0,
                    "avg_success_rate": 0,
                    "avg_quality": 0,
                    "message": "ì•„ì§ í¬ë¡¤ë§ëœ ë°ì´í„° ì—†ìŒ",
                    "data_source": "empty"
                }
                
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
            
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.crawler:
            await self.crawler.cleanup()
        if self.db_pool:
            await self.db_pool.close()
        logger.info("ğŸ›‘ ì‹¤ì œ í¬ë¡¤ë§ ì„œë¹„ìŠ¤ ì •ë¦¬ ì™„ë£Œ")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸ‰ ì‹¤ì œ í¬ë¡¤ë§ ë° JSONB ì €ì¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    service = RealCrawlingService()
    
    try:
        await service.initialize()
        
        # ì‹¤ì œ ì‚¬ì´íŠ¸ë“¤ í¬ë¡¤ë§ ë° ì €ì¥
        test_sites = [
            ("https://httpbin.org/json", "api_test"),
            ("https://example.com", "html_test"), 
            ("https://www.bizinfo.go.kr", "government"),
        ]
        
        for url, source_type in test_sites:
            logger.info(f"ğŸ¯ ì‹¤ì œ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸: {url}")
            success = await service.crawl_and_store(url, source_type)
            
            if success:
                logger.info(f"âœ… {url} í¬ë¡¤ë§ ë° ì €ì¥ ì„±ê³µ")
            else:
                logger.warning(f"âš ï¸ {url} í¬ë¡¤ë§ ì‹¤íŒ¨")
                
            await asyncio.sleep(2)  # ê°„ê²© ì¡°ì ˆ
        
        # ì €ì¥ëœ ë°ì´í„° ê²€ì¦
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š ì‹¤ì œ JSONB ì €ì¥ ë°ì´í„° ê²€ì¦")
        logger.info("="*60)
        
        stored_count = await service.verify_stored_data()
        
        if stored_count > 0:
            logger.info("âœ… ì‹¤ì œ ë°ì´í„° ì €ì¥ ë° ê²€ì¦ ì™„ë£Œ")
            
            # ì‹¤ì œ í†µê³„ ì¡°íšŒ
            real_stats = await service.get_real_statistics()
            logger.info(f"ğŸ“ˆ ì‹¤ì œ DB ê¸°ë°˜ í†µê³„: {json.dumps(real_stats, indent=2, ensure_ascii=False)}")
            
        else:
            logger.warning("âš ï¸ ì €ì¥ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        
    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        await service.cleanup()
    
    logger.info("ğŸ ì‹¤ì œ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")


if __name__ == "__main__":
    asyncio.run(main())