#!/usr/bin/env python3
"""
Government Portal Intelligence System
ì •ë¶€ í¬í„¸ ì§€ëŠ¥í™” ì‹œìŠ¤í…œ - ë©”ì¸ ì„œë²„

Author: Paperwork AI Team
Version: 2.0.0  
Date: 2025-08-23
"""

import asyncio
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import uvicorn

from scrapers.bizinfo_scraper import BizinfoScraper
from scrapers.kstartup_scraper import KStartupScraper
from analyzers.pattern_analyzer import KoreanPortalPatternAnalyzer
from database.db_manager import DatabaseManager
from utils.scheduler import UpdateScheduler
from utils.rate_limiter import RateLimiter
from config.settings import settings

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/portal_scraper.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI(
    title="Government Portal Intelligence System",
    description="ì •ë¶€ í¬í„¸ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° í…œí”Œë¦¿ ìë™ ìƒì„± ì‹œìŠ¤í…œ",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS ì„¤ì • (Paperwork AI ì—°ë™ìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://paperwork.heal7.com", "http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"]
)

# ê¸€ë¡œë²Œ ê°ì²´ ì´ˆê¸°í™”
db_manager = DatabaseManager(settings.DATABASE_URL)
pattern_analyzer = KoreanPortalPatternAnalyzer()
scheduler = UpdateScheduler()
scrapers = {}

# Pydantic ëª¨ë¸
class ScrapingRequest(BaseModel):
    portal_ids: List[str]
    force_update: bool = False
    include_analysis: bool = True

class TemplateRequest(BaseModel):
    program_ids: List[str]
    institution: Optional[str] = None
    template_type: Optional[str] = None

class ScrapingResult(BaseModel):
    success: bool
    portal_id: str
    programs_found: int
    new_programs: int
    updated_programs: int
    processing_time_seconds: float
    error_message: Optional[str] = None

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ì‹œ ì´ˆê¸°í™”"""
    logger.info("ğŸš€ Government Portal Intelligence System ì‹œì‘")
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    await db_manager.initialize()
    logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì™„ë£Œ")
    
    # ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”
    scrapers['bizinfo'] = BizinfoScraper(db_manager, rate_limiter=RateLimiter(20))
    scrapers['kstartup'] = KStartupScraper(db_manager, rate_limiter=RateLimiter(15))
    logger.info("âœ… ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™” ì™„ë£Œ")
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
    await scheduler.start()
    
    # ì¼ì¼ ìë™ ìŠ¤í¬ë˜í•‘ ìŠ¤ì¼€ì¤„ ì„¤ì •
    await schedule_daily_scraping()
    logger.info("âœ… ìë™ ìŠ¤ì¼€ì¤„ë§ ì‹œì‘")

@app.on_event("shutdown") 
async def shutdown_event():
    """ì„œë²„ ì¢…ë£Œì‹œ ì •ë¦¬"""
    logger.info("ğŸ›‘ ì‹œìŠ¤í…œ ì¢…ë£Œ ì¤‘...")
    await scheduler.stop()
    await db_manager.close()
    for scraper in scrapers.values():
        if hasattr(scraper, 'close'):
            await scraper.close()

async def schedule_daily_scraping():
    """ë§¤ì¼ ìë™ ìŠ¤í¬ë˜í•‘ ìŠ¤ì¼€ì¤„ ì„¤ì •"""
    
    # ë§¤ì¼ ì˜¤ì „ 6ì‹œì— ê¸°ì—…ë§ˆë‹¹ ìŠ¤í¬ë˜í•‘
    await scheduler.schedule_daily(
        hour=6, minute=0,
        func=run_scheduled_scraping,
        args=(['bizinfo'],),
        job_id="daily_bizinfo_scraping"
    )
    
    # ë§¤ì¼ ì˜¤ì „ 7ì‹œì— K-Startup ìŠ¤í¬ë˜í•‘  
    await scheduler.schedule_daily(
        hour=7, minute=0,
        func=run_scheduled_scraping,
        args=(['kstartup'],),
        job_id="daily_kstartup_scraping"
    )
    
    # ë§¤ì¼ ì˜¤í›„ 1ì‹œì— ì „ì²´ í¬í„¸ ìŠ¤í¬ë˜í•‘
    await scheduler.schedule_daily(
        hour=13, minute=0,
        func=run_scheduled_scraping,
        args=(['bizinfo', 'kstartup'],),
        job_id="daily_full_scraping"
    )

async def run_scheduled_scraping(portal_ids: List[str]):
    """ìŠ¤ì¼€ì¤„ëœ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"""
    logger.info(f"â° ìŠ¤ì¼€ì¤„ëœ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {portal_ids}")
    
    results = []
    for portal_id in portal_ids:
        try:
            result = await execute_single_scraping(portal_id, include_analysis=True)
            results.append(result)
            logger.info(f"âœ… {portal_id} ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {result.programs_found}ê°œ í”„ë¡œê·¸ë¨ ë°œê²¬")
        except Exception as e:
            logger.error(f"âŒ {portal_id} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {str(e)}")
            results.append(ScrapingResult(
                success=False,
                portal_id=portal_id,
                programs_found=0,
                new_programs=0,
                updated_programs=0,
                processing_time_seconds=0,
                error_message=str(e)
            ))
    
    # Paperwork AIì— ê²°ê³¼ ì „ì†¡
    await notify_paperwork_ai(results)

@app.get("/")
async def root():
    """API ìƒíƒœ í™•ì¸"""
    return {
        "service": "Government Portal Intelligence System",
        "version": "2.0.0",
        "status": "running",
        "active_scrapers": list(scrapers.keys()),
        "last_update": await get_last_update_time()
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    try:
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸
        await db_manager.test_connection()
        
        # ìŠ¤í¬ë˜í¼ ìƒíƒœ í™•ì¸
        scraper_status = {}
        for name, scraper in scrapers.items():
            scraper_status[name] = await scraper.get_status()
        
        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "database": "connected",
            "scrapers": scraper_status
        }
    except Exception as e:
        raise HTTPException(status_code=503, detail=f"Service unhealthy: {str(e)}")

@app.post("/scrape/manual", response_model=List[ScrapingResult])
async def manual_scraping(request: ScrapingRequest, background_tasks: BackgroundTasks):
    """ìˆ˜ë™ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"""
    logger.info(f"ğŸ”§ ìˆ˜ë™ ìŠ¤í¬ë˜í•‘ ìš”ì²­: {request.portal_ids}")
    
    if not request.portal_ids:
        raise HTTPException(status_code=400, detail="í¬í„¸ IDë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”")
    
    # ìœ íš¨í•œ í¬í„¸ ID í™•ì¸
    invalid_portals = [pid for pid in request.portal_ids if pid not in scrapers]
    if invalid_portals:
        raise HTTPException(
            status_code=400, 
            detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í¬í„¸: {invalid_portals}. ì§€ì› í¬í„¸: {list(scrapers.keys())}"
        )
    
    results = []
    for portal_id in request.portal_ids:
        try:
            result = await execute_single_scraping(
                portal_id, 
                force_update=request.force_update,
                include_analysis=request.include_analysis
            )
            results.append(result)
        except Exception as e:
            logger.error(f"âŒ {portal_id} ìˆ˜ë™ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {str(e)}")
            results.append(ScrapingResult(
                success=False,
                portal_id=portal_id,
                programs_found=0,
                new_programs=0,
                updated_programs=0,
                processing_time_seconds=0,
                error_message=str(e)
            ))
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ Paperwork AI ì•Œë¦¼
    background_tasks.add_task(notify_paperwork_ai, results)
    
    return results

async def execute_single_scraping(
    portal_id: str, 
    force_update: bool = False,
    include_analysis: bool = True
) -> ScrapingResult:
    """ê°œë³„ í¬í„¸ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"""
    start_time = datetime.now()
    scraper = scrapers[portal_id]
    
    try:
        # 1. ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
        scraped_programs = await scraper.scrape_all_programs(force_update=force_update)
        
        # 2. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ë° ë³€ê²½ì‚¬í•­ ê°ì§€
        save_result = await db_manager.save_programs(portal_id, scraped_programs)
        
        # 3. AI ë¶„ì„ ìˆ˜í–‰ (ì˜µì…˜)
        if include_analysis and save_result['new_programs']:
            await perform_ai_analysis(portal_id, save_result['new_programs'])
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return ScrapingResult(
            success=True,
            portal_id=portal_id,
            programs_found=len(scraped_programs),
            new_programs=save_result['new_count'],
            updated_programs=save_result['updated_count'],
            processing_time_seconds=processing_time
        )
        
    except Exception as e:
        processing_time = (datetime.now() - start_time).total_seconds()
        raise Exception(f"ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {str(e)}")

async def perform_ai_analysis(portal_id: str, new_programs: List[Dict]):
    """ìƒˆë¡œìš´ í”„ë¡œê·¸ë¨ì— ëŒ€í•œ AI ë¶„ì„ ìˆ˜í–‰"""
    logger.info(f"ğŸ¤– AI ë¶„ì„ ì‹œì‘: {portal_id}, {len(new_programs)}ê°œ í”„ë¡œê·¸ë¨")
    
    for program in new_programs:
        try:
            # AI íŒ¨í„´ ë¶„ì„
            analysis_result = await pattern_analyzer.analyze_korean_program_structure(program)
            
            # ë¶„ì„ ê²°ê³¼ ì €ì¥
            await db_manager.save_analysis_result(program['program_id'], analysis_result)
            
            # í…œí”Œë¦¿ ìë™ ìƒì„± (í’ˆì§ˆ ì ìˆ˜ 8ì  ì´ìƒì‹œ)
            if analysis_result.get('quality_score', 0) >= 8.0:
                await generate_auto_template(program, analysis_result)
                
        except Exception as e:
            logger.error(f"âŒ {program['program_id']} AI ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

async def generate_auto_template(program: Dict, analysis_result: Dict):
    """AI ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í…œí”Œë¦¿ ìë™ ìƒì„±"""
    try:
        logger.info(f"ğŸ“‹ í…œí”Œë¦¿ ìƒì„± ì‹œì‘: {program['program_id']}")
        
        # ê¸°ë³¸ í…œí”Œë¦¿ êµ¬ì¡° ìƒì„±
        template_data = {
            'program_id': program['program_id'],
            'title': program.get('title', ''),
            'agency': program.get('agency', ''),
            'support_details': program.get('support_details', {}),
            'application_period': program.get('application_period', ''),
            'target_audience': program.get('target_audience', ''),
            'quality_score': analysis_result.get('quality_score', 0),
            'generated_at': datetime.now().isoformat(),
            'auto_generated': True
        }
        
        # í…œí”Œë¦¿ ì €ì¥ ë¡œì§ (ì‹¤ì œ êµ¬í˜„)
        await db_manager.save_auto_generated_template(template_data)
        logger.info(f"âœ… í…œí”Œë¦¿ ìƒì„± ì™„ë£Œ: {program['program_id']}")
        
    except Exception as e:
        logger.error(f"âŒ í…œí”Œë¦¿ ìƒì„± ì‹¤íŒ¨: {program['program_id']} - {str(e)}")

@app.get("/programs/latest")
async def get_latest_programs(
    portal_id: Optional[str] = None,
    limit: int = 50,
    days: int = 7
):
    """ìµœì‹  ì§€ì›ì‚¬ì—… í”„ë¡œê·¸ë¨ ì¡°íšŒ"""
    since_date = datetime.now() - timedelta(days=days)
    
    programs = await db_manager.get_programs(
        portal_id=portal_id,
        since_date=since_date,
        limit=limit
    )
    
    return {
        "count": len(programs),
        "programs": programs,
        "filters": {
            "portal_id": portal_id,
            "since_date": since_date.isoformat(),
            "limit": limit
        }
    }

@app.get("/templates/auto-generated")
async def get_auto_generated_templates(
    portal_id: Optional[str] = None,
    status: str = "approved"
):
    """ìë™ ìƒì„±ëœ í…œí”Œë¦¿ ì¡°íšŒ"""
    templates = await db_manager.get_auto_generated_templates(
        portal_id=portal_id,
        status=status
    )
    
    return {
        "count": len(templates),
        "templates": templates
    }

@app.post("/paperwork/sync")
async def sync_with_paperwork_ai():
    """Paperwork AIì™€ ìˆ˜ë™ ë™ê¸°í™”"""
    try:
        # ìµœê·¼ ì—…ë°ì´íŠ¸ëœ í…œí”Œë¦¿ê³¼ í”„ë¡œê·¸ë¨ ì •ë³´ ì „ì†¡
        recent_data = await prepare_paperwork_sync_data()
        
        # Paperwork AIì— ì „ì†¡
        sync_result = await send_to_paperwork_ai(recent_data)
        
        return {
            "success": True,
            "sync_result": sync_result,
            "synced_templates": len(recent_data.get('templates', [])),
            "synced_programs": len(recent_data.get('programs', []))
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë™ê¸°í™” ì‹¤íŒ¨: {str(e)}")

async def prepare_paperwork_sync_data() -> Dict:
    """Paperwork AI ë™ê¸°í™”ìš© ë°ì´í„° ì¤€ë¹„"""
    # ìµœê·¼ 7ì¼ê°„ì˜ ë°ì´í„°
    since_date = datetime.now() - timedelta(days=7)
    
    return {
        "templates": await db_manager.get_auto_generated_templates(
            since_date=since_date,
            status="approved"
        ),
        "programs": await db_manager.get_programs(
            since_date=since_date,
            include_analysis=True
        ),
        "sync_timestamp": datetime.now().isoformat()
    }

async def send_to_paperwork_ai(data: Dict) -> Dict:
    """Paperwork AIì— ë°ì´í„° ì „ì†¡"""
    import aiohttp
    
    paperwork_api_url = "https://paperwork.heal7.com/api/government-portal/sync"
    
    async with aiohttp.ClientSession() as session:
        async with session.post(
            paperwork_api_url,
            json=data,
            headers={
                "Authorization": f"Bearer {settings.PAPERWORK_API_KEY}",
                "Content-Type": "application/json"
            }
        ) as response:
            if response.status == 200:
                return await response.json()
            else:
                raise Exception(f"Paperwork AI ì „ì†¡ ì‹¤íŒ¨: {response.status}")

async def notify_paperwork_ai(results: List[ScrapingResult]):
    """ìŠ¤í¬ë˜í•‘ ê²°ê³¼ë¥¼ Paperwork AIì— ì•Œë¦¼"""
    try:
        notification_data = {
            "event": "scraping_completed",
            "timestamp": datetime.now().isoformat(),
            "results": [result.dict() for result in results],
            "summary": {
                "total_portals": len(results),
                "successful_portals": len([r for r in results if r.success]),
                "total_new_programs": sum(r.new_programs for r in results if r.success),
                "total_updated_programs": sum(r.updated_programs for r in results if r.success)
            }
        }
        
        await send_to_paperwork_ai(notification_data)
        logger.info("ğŸ“¡ Paperwork AI ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ Paperwork AI ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {str(e)}")

async def get_last_update_time() -> Optional[str]:
    """ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ ì¡°íšŒ"""
    try:
        last_update = await db_manager.get_last_update_time()
        return last_update.isoformat() if last_update else None
    except:
        return None

if __name__ == "__main__":
    import os
    
    # í™˜ê²½ë³€ìˆ˜ ì„¤ì •
    port = int(os.environ.get("PORT", 8005))
    host = os.environ.get("HOST", "0.0.0.0")
    
    logger.info(f"ğŸŒ Government Portal Intelligence System ì‹œì‘: {host}:{port}")
    
    uvicorn.run(
        "main:app",
        host=host,
        port=port,
        reload=False,  # í”„ë¡œë•ì…˜ì—ì„œëŠ” False
        log_level="info",
        access_log=True
    )