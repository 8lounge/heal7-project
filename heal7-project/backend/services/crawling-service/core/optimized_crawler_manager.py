#!/usr/bin/env python3
"""
ğŸ•·ï¸ ìµœì í™”ëœ Tier 1-3 í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì €
ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•œ 3ê³„ì¸µ í¬ë¡¤ë§ ì‹œìŠ¤í…œ

Tier 1: httpx (ê²½ëŸ‰, API/JSON ì²˜ë¦¬) - 90%+ ìš”ì²­ ì²˜ë¦¬
Tier 2: httpx + BeautifulSoup (ì •ì  HTML íŒŒì‹±) - 5-8% ìš”ì²­ ì²˜ë¦¬  
Tier 3: Playwright (ë™ì  JavaScript í•„ìš”ì‹œë§Œ) - 2-5% ìš”ì²­ ì²˜ë¦¬

Author: HEAL7 Development Team
Version: 2.0.0 (Optimized)
Date: 2025-09-02
"""

import asyncio
import time
import logging
from typing import Dict, List, Any, Optional, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
from urllib.parse import urlparse, urljoin
import re

from .crawlers.base_crawler import (
    BaseCrawler, CrawlResult, CrawlConfig, CrawlerType, 
    CrawlStatus, create_default_headers, is_valid_url
)
from .crawlers.httpx_crawler import HttpxCrawler

logger = logging.getLogger(__name__)


class OptimizedTier(Enum):
    """ìµœì í™”ëœ í¬ë¡¤ë§ ê³„ì¸µ"""
    TIER1 = "tier1"  # httpx only (ê²½ëŸ‰)
    TIER2 = "tier2"  # httpx + BeautifulSoup (ì •ì  HTML)
    TIER3 = "tier3"  # Playwright (ë™ì  JavaScript)


@dataclass
class TierMetrics:
    """ê³„ì¸µë³„ ì„±ëŠ¥ ì§€í‘œ"""
    tier: OptimizedTier
    request_count: int = 0
    success_count: int = 0
    total_response_time: float = 0.0
    memory_usage: float = 0.0
    
    @property
    def success_rate(self) -> float:
        return self.success_count / max(self.request_count, 1)
    
    @property
    def avg_response_time(self) -> float:
        return self.total_response_time / max(self.request_count, 1)


class OptimizedCrawlerManager:
    """ğŸš€ ìµœì í™”ëœ í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì €"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.OptimizedManager")
        
        # ê³„ì¸µë³„ í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
        self.tier1_crawler: Optional[HttpxCrawler] = None
        self.tier2_crawler: Optional[HttpxCrawler] = None  # httpx + BS4
        self.tier3_crawler = None  # Playwright (ì§€ì—° ë¡œë”©)
        
        # ì„±ëŠ¥ ì§€í‘œ
        self.metrics = {
            OptimizedTier.TIER1: TierMetrics(OptimizedTier.TIER1),
            OptimizedTier.TIER2: TierMetrics(OptimizedTier.TIER2),
            OptimizedTier.TIER3: TierMetrics(OptimizedTier.TIER3),
        }
        
        # ì‹œìŠ¤í…œ ì„¤ì •
        self.tier3_threshold = 0.95  # Tier3 ì‚¬ìš©ë¥  ì„ê³„ê°’ (95%)
        self.memory_threshold = 512  # MB ì„ê³„ê°’
        self.is_initialized = False
        
        # URL íŒ¨í„´ ë¶„ë¥˜ê¸°
        self._setup_url_classifiers()
    
    def _setup_url_classifiers(self):
        """URL íŒ¨í„´ë³„ ë¶„ë¥˜ ê·œì¹™ ì„¤ì •"""
        self.tier1_patterns = [
            r'.*\.(json|xml|txt|csv)(\?.*)?$',  # ë°ì´í„° íŒŒì¼
            r'.*/api/.*',                        # API ì—”ë“œí¬ì¸íŠ¸
            r'.*/rest/.*',                       # REST API
            r'.*/v\d+/.*',                       # API ë²„ì „
            r'.*\.(pdf|doc|docx|xls|xlsx)(\?.*)?$',  # ë¬¸ì„œ íŒŒì¼
        ]
        
        self.tier2_patterns = [
            r'.*\.(html?|htm)(\?.*)?$',          # HTML íŒŒì¼
            r'.*/.*\.php(\?.*)?$',               # PHP í˜ì´ì§€
            r'.*/.*\.jsp(\?.*)?$',               # JSP í˜ì´ì§€
            r'.*/news/.*',                       # ë‰´ìŠ¤ ì‚¬ì´íŠ¸
            r'.*/article/.*',                    # ê¸°ì‚¬ í˜ì´ì§€
            r'.*/blog/.*',                       # ë¸”ë¡œê·¸ í˜ì´ì§€
        ]
        
        # Tier3ì€ ë™ì  ì»¨í…ì¸ ê°€ í™•ì‹¤í•œ ê²½ìš°ë§Œ
        self.tier3_patterns = [
            r'.*react.*',                        # React ì•±
            r'.*angular.*',                      # Angular ì•±
            r'.*vue.*',                          # Vue ì•±
            r'.*/spa/.*',                        # SPA ì•±
        ]
        
        # ì»´íŒŒì¼ëœ ì •ê·œì‹
        self.tier1_regex = [re.compile(p, re.I) for p in self.tier1_patterns]
        self.tier2_regex = [re.compile(p, re.I) for p in self.tier2_patterns]
        self.tier3_regex = [re.compile(p, re.I) for p in self.tier3_patterns]
    
    async def initialize(self):
        """ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        if self.is_initialized:
            return
        
        try:
            # Tier 1 (httpx) ì´ˆê¸°í™”
            self.tier1_crawler = HttpxCrawler()
            await self.tier1_crawler.initialize()
            
            # Tier 2 (httpx + BeautifulSoup) ì´ˆê¸°í™”
            self.tier2_crawler = HttpxCrawler()
            await self.tier2_crawler.initialize()
            
            # Tier 3ëŠ” í•„ìš”ì‹œì—ë§Œ ì§€ì—° ë¡œë”©
            
            self.is_initialized = True
            self.logger.info("âœ… ìµœì í™”ëœ í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            await self.cleanup()
            raise
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self.tier1_crawler:
                await self.tier1_crawler.cleanup()
                
            if self.tier2_crawler:
                await self.tier2_crawler.cleanup()
            
            if self.tier3_crawler:
                await self.tier3_crawler.cleanup()
            
            self.is_initialized = False
            self.logger.info("ğŸ›‘ ìµœì í™”ëœ í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì € ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def classify_url(self, url: str) -> OptimizedTier:
        """URLì„ ìµœì  ê³„ì¸µìœ¼ë¡œ ë¶„ë¥˜"""
        url_lower = url.lower()
        
        # Tier 1 íŒ¨í„´ í™•ì¸ (ìµœìš°ì„ )
        for pattern in self.tier1_regex:
            if pattern.match(url):
                return OptimizedTier.TIER1
        
        # Tier 3 íŒ¨í„´ í™•ì¸ (ë™ì  ì»¨í…ì¸ )
        for pattern in self.tier3_regex:
            if pattern.match(url):
                return OptimizedTier.TIER3
        
        # Tier 2 íŒ¨í„´ í™•ì¸
        for pattern in self.tier2_regex:
            if pattern.match(url):
                return OptimizedTier.TIER2
        
        # ê¸°ë³¸ê°’: ë¨¼ì € Tier 1ë¡œ ì‹œë„
        return OptimizedTier.TIER1
    
    async def smart_crawl(self, config: CrawlConfig) -> CrawlResult:
        """ì§€ëŠ¥í˜• í¬ë¡¤ë§ - ìµœì  ê³„ì¸µ ìë™ ì„ íƒ"""
        if not self.is_initialized:
            await self.initialize()
        
        # 1. URL ë¶„ë¥˜ë¡œ ì´ˆê¸° ê³„ì¸µ ê²°ì •
        initial_tier = self.classify_url(config.url)
        
        # 2. ê³„ì¸µë³„ ìˆœì°¨ ì‹œë„
        return await self._crawl_with_fallback(config, initial_tier)
    
    async def _crawl_with_fallback(self, config: CrawlConfig, start_tier: OptimizedTier) -> CrawlResult:
        """ê³„ì¸µë³„ í´ë°± í¬ë¡¤ë§"""
        tiers = [OptimizedTier.TIER1, OptimizedTier.TIER2, OptimizedTier.TIER3]
        
        # ì‹œì‘ ê³„ì¸µë¶€í„° ìˆœì°¨ ì‹œë„
        start_index = tiers.index(start_tier)
        
        for i in range(start_index, len(tiers)):
            tier = tiers[i]
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
            if i >= 2 and await self._check_memory_usage():
                self.logger.warning("ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì„ê³„ê°’ ì´ˆê³¼, Tier3 ìƒëµ")
                break
            
            result = await self._crawl_single_tier(config, tier)
            
            # ì„±ê³µí•˜ë©´ ë°˜í™˜
            if result.success or result.status == CrawlStatus.PARTIAL_SUCCESS:
                return result
            
            # Tier 1,2 ì‹¤íŒ¨ì‹œ ë‹¤ìŒ ê³„ì¸µìœ¼ë¡œ
            if i < len(tiers) - 1:
                self.logger.info(f"ğŸ”„ {tier.value} ì‹¤íŒ¨, ë‹¤ìŒ ê³„ì¸µìœ¼ë¡œ ì‹œë„")
        
        # ëª¨ë“  ê³„ì¸µ ì‹¤íŒ¨
        return CrawlResult(
            success=False,
            error="ëª¨ë“  í¬ë¡¤ë§ ê³„ì¸µì—ì„œ ì‹¤íŒ¨",
            error_type="all_tiers_failed",
            url=config.url
        )
    
    async def _crawl_single_tier(self, config: CrawlConfig, tier: OptimizedTier) -> CrawlResult:
        """ë‹¨ì¼ ê³„ì¸µìœ¼ë¡œ í¬ë¡¤ë§"""
        start_time = time.time()
        
        try:
            if tier == OptimizedTier.TIER1:
                result = await self._tier1_crawl(config)
            elif tier == OptimizedTier.TIER2:
                result = await self._tier2_crawl(config)
            else:  # TIER3
                result = await self._tier3_crawl(config)
            
            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self._update_metrics(tier, result, time.time() - start_time)
            
            return result
            
        except Exception as e:
            error_result = CrawlResult(
                success=False,
                error=f"{tier.value} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}",
                crawler_used=CrawlerType.HTTPX if tier != OptimizedTier.TIER3 else CrawlerType.PLAYWRIGHT,
                url=config.url,
                response_time=time.time() - start_time
            )
            
            self._update_metrics(tier, error_result, time.time() - start_time)
            return error_result
    
    async def _tier1_crawl(self, config: CrawlConfig) -> CrawlResult:
        """Tier 1: httpx ê²½ëŸ‰ í¬ë¡¤ë§"""
        result = await self.tier1_crawler.crawl(config)
        
        # HTMLì¸ ê²½ìš° ê°„ë‹¨í•œ ê²€ì¦
        if result.success and result.html:
            html_len = len(result.html)
            
            # ë„ˆë¬´ ì ì€ ë‚´ìš©ì´ë©´ ë™ì  ì»¨í…ì¸ ì¼ ê°€ëŠ¥ì„±
            if html_len < 500 and '<script' in result.html.lower():
                result.success = False
                result.error = "ë™ì  ì»¨í…ì¸  ê°ì§€, ìƒìœ„ ê³„ì¸µ í•„ìš”"
                result.status = CrawlStatus.PARTIAL_SUCCESS
        
        return result
    
    async def _tier2_crawl(self, config: CrawlConfig) -> CrawlResult:
        """Tier 2: httpx + BeautifulSoup ì •ì  HTML ì²˜ë¦¬"""
        from bs4 import BeautifulSoup
        
        # httpxë¡œ ê¸°ë³¸ í¬ë¡¤ë§
        result = await self.tier2_crawler.crawl(config)
        
        if result.success and result.html:
            try:
                # BeautifulSoupë¡œ HTML íŒŒì‹± ë° ì •ì œ
                soup = BeautifulSoup(result.html, 'html.parser')
                
                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                metadata = result.metadata or {}
                metadata.update({
                    'title': soup.title.text.strip() if soup.title else '',
                    'meta_description': '',
                    'links_count': len(soup.find_all('a')),
                    'images_count': len(soup.find_all('img')),
                    'scripts_count': len(soup.find_all('script')),
                    'processing_tier': 'tier2'
                })
                
                # ë©”íƒ€ ì„¤ëª… ì¶”ì¶œ
                meta_desc = soup.find('meta', attrs={'name': 'description'})
                if meta_desc:
                    metadata['meta_description'] = meta_desc.get('content', '')[:200]
                
                # JavaScriptê°€ ë§ìœ¼ë©´ ë™ì  ì»¨í…ì¸  ê°€ëŠ¥ì„± í‘œì‹œ
                if metadata['scripts_count'] > 10:
                    metadata['dynamic_content_likely'] = True
                    
                    # ë‚´ìš©ì´ ì ìœ¼ë©´ Tier3 í•„ìš” ì‹œì‚¬
                    visible_text = soup.get_text().strip()
                    if len(visible_text) < 1000:
                        result.success = False
                        result.error = "JavaScript ë Œë”ë§ í•„ìš” ê°€ëŠ¥ì„±"
                        result.status = CrawlStatus.PARTIAL_SUCCESS
                
                result.metadata = metadata
                
            except Exception as e:
                self.logger.warning(f"BeautifulSoup ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                # ì›ë³¸ ê²°ê³¼ ìœ ì§€
        
        return result
    
    async def _tier3_crawl(self, config: CrawlConfig) -> CrawlResult:
        """Tier 3: Playwright ë™ì  ì»¨í…ì¸  ì²˜ë¦¬ (ì§€ì—° ë¡œë”©)"""
        if not self.tier3_crawler:
            await self._initialize_tier3()
        
        return await self.tier3_crawler.crawl(config)
    
    async def _initialize_tier3(self):
        """Tier 3 ì§€ì—° ì´ˆê¸°í™”"""
        try:
            from .crawlers.playwright_crawler import PlaywrightCrawler
            self.tier3_crawler = PlaywrightCrawler(headless=True)
            await self.tier3_crawler.initialize()
            
            self.logger.info("ğŸ­ Tier 3 (Playwright) ì§€ì—° ì´ˆê¸°í™” ì™„ë£Œ")
            
        except ImportError:
            self.logger.warning("âš ï¸ Playwright ëª¨ë“ˆ ì—†ìŒ, Tier 3 ë¹„í™œì„±í™”")
            raise
        except Exception as e:
            self.logger.error(f"âŒ Tier 3 ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def _check_memory_usage(self) -> bool:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸"""
        try:
            import psutil
            memory = psutil.virtual_memory()
            used_mb = memory.used / 1024 / 1024
            return used_mb > self.memory_threshold
        except ImportError:
            return False
        except Exception:
            return False
    
    def _update_metrics(self, tier: OptimizedTier, result: CrawlResult, response_time: float):
        """ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        metric = self.metrics[tier]
        metric.request_count += 1
        metric.total_response_time += response_time
        
        if result.success:
            metric.success_count += 1
    
    async def batch_crawl(self, urls: List[str], max_concurrent: int = 5) -> List[CrawlResult]:
        """ë°°ì¹˜ í¬ë¡¤ë§ (ìµœì í™”ëœ ë™ì‹œ ì²˜ë¦¬)"""
        if not self.is_initialized:
            await self.initialize()
        
        # ë™ì‹œ ì‹¤í–‰ ì œí•œ
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def crawl_single(url: str) -> CrawlResult:
            async with semaphore:
                config = CrawlConfig(url=url)
                return await self.smart_crawl(config)
        
        tasks = [crawl_single(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì˜ˆì™¸ ì²˜ë¦¬
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append(CrawlResult(
                    success=False,
                    error=str(result),
                    url=urls[i]
                ))
            else:
                processed_results.append(result)
        
        return processed_results
    
    def get_performance_report(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±"""
        report = {
            'total_requests': sum(m.request_count for m in self.metrics.values()),
            'tier_distribution': {},
            'performance_summary': {}
        }
        
        for tier, metric in self.metrics.items():
            if metric.request_count > 0:
                report['tier_distribution'][tier.value] = {
                    'requests': metric.request_count,
                    'success_rate': f"{metric.success_rate:.2%}",
                    'avg_response_time': f"{metric.avg_response_time:.2f}s"
                }
        
        # ì„±ëŠ¥ ìš”ì•½
        tier1_pct = (self.metrics[OptimizedTier.TIER1].request_count / 
                    max(report['total_requests'], 1) * 100)
        
        report['performance_summary'] = {
            'tier1_usage': f"{tier1_pct:.1f}%",
            'optimization_score': min(100, tier1_pct * 1.2),  # Tier1 ì‚¬ìš©ë¥  ê¸°ë°˜ ì ìˆ˜
            'memory_efficient': tier1_pct > 80
        }
        
        return report
    
    def get_optimization_recommendations(self) -> List[str]:
        """ìµœì í™” ê¶Œì¥ì‚¬í•­"""
        recommendations = []
        
        tier3_usage = (self.metrics[OptimizedTier.TIER3].request_count / 
                      max(sum(m.request_count for m in self.metrics.values()), 1))
        
        if tier3_usage > 0.2:  # 20% ì´ìƒ
            recommendations.append("ğŸ­ Tier3 ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤. URL íŒ¨í„´ ë¶„ë¥˜ ê·œì¹™ì„ ê°œì„ í•˜ì„¸ìš”.")
        
        if tier3_usage < 0.05:  # 5% ë¯¸ë§Œ
            recommendations.append("âœ… ìµœì í™”ê°€ ì˜ ë˜ì–´ ìˆìŠµë‹ˆë‹¤. Tier1/2 ìœ„ì£¼ ì²˜ë¦¬ ì¤‘")
        
        return recommendations


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜

async def quick_smart_crawl(url: str, **kwargs) -> CrawlResult:
    """ë¹ ë¥¸ ì§€ëŠ¥í˜• í¬ë¡¤ë§"""
    manager = OptimizedCrawlerManager()
    try:
        await manager.initialize()
        config = CrawlConfig(url=url, **kwargs)
        return await manager.smart_crawl(config)
    finally:
        await manager.cleanup()


async def analyze_url_tier(url: str) -> Dict[str, Any]:
    """URL ìµœì  ê³„ì¸µ ë¶„ì„"""
    manager = OptimizedCrawlerManager()
    tier = manager.classify_url(url)
    
    return {
        'url': url,
        'recommended_tier': tier.value,
        'expected_performance': 'high' if tier == OptimizedTier.TIER1 else 'medium' if tier == OptimizedTier.TIER2 else 'low',
        'memory_usage': 'low' if tier != OptimizedTier.TIER3 else 'high'
    }