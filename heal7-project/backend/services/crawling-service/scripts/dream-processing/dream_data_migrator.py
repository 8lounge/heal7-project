#!/usr/bin/env python3
"""
HEAL7 ê¿ˆí’€ì´ ë°ì´í„° ì •í˜•í™” ë° ì´ì „ ì‹œìŠ¤í…œ
3,557ê°œ dream_interpretations â†’ clean_dream_interpretations ì •í˜•í™” ì´ì „
"""

import asyncio
import asyncpg
import logging
import re
from typing import List, Dict, Set, Tuple
from datetime import datetime
import json

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/tmp/dream_migrator.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DreamDataMigrator:
    def __init__(self):
        self.conn = None
        self.category_mapping = {
            'ë¬¼': 'ìì—°',
            'ë°”ë‹¤': 'ìì—°',
            'ê°•': 'ìì—°',
            'í˜¸ìˆ˜': 'ìì—°',
            'ì‚°': 'ìì—°',
            'ë‚˜ë¬´': 'ìì—°',
            'ê½ƒ': 'ìì—°',
            'í•´': 'ìì—°',
            'ë‹¬': 'ìì—°',
            'ë³„': 'ìì—°',
            'ëˆˆ': 'ìì—°',
            'ë¹„': 'ìì—°',
            'ë°”ëŒ': 'ìì—°',
            'ëˆ': 'ì¬ë¬¼',
            'ê¸ˆ': 'ì¬ë¬¼',
            'ë³´ì„': 'ì¬ë¬¼',
            'í†µì¥': 'ì¬ë¬¼',
            'ë±€': 'ë™ë¬¼',
            'í˜¸ë‘ì´': 'ë™ë¬¼',
            'ìš©': 'ë™ë¬¼',
            'ìƒˆ': 'ë™ë¬¼',
            'ê°œ': 'ë™ë¬¼',
            'ê³ ì–‘ì´': 'ë™ë¬¼',
            'ì£½ìŒ': 'ìƒì‚¬',
            'ì‚´ì¸': 'ìƒì‚¬',
            'ì£½ì´ë‹¤': 'ìƒì‚¬',
            'ë³‘ì›': 'ê±´ê°•',
            'ì˜ì‚¬': 'ê±´ê°•',
            'ì•½': 'ê±´ê°•',
            'ì§‘': 'ì£¼ê±°',
            'ë°©': 'ì£¼ê±°',
            'ë¬¸': 'ì£¼ê±°',
            'ê³„ë‹¨': 'ì£¼ê±°',
            'ì°¨': 'êµí†µ',
            'ê¸°ì°¨': 'êµí†µ',
            'ë¹„í–‰ê¸°': 'êµí†µ',
            'ë°°': 'êµí†µ',
            'ìŒì‹': 'ìŒì‹',
            'ë°¥': 'ìŒì‹',
            'ê³¼ì¼': 'ìŒì‹',
            'ê³ ê¸°': 'ìŒì‹',
        }
        
        self.quality_score_mapping = {
            'traditional': 8.5,
            'modern': 8.0,
            'psychological': 9.0
        }

    async def connect_db(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        try:
            self.conn = await asyncpg.connect(
                host='localhost',
                database='dream_service',
                user='postgres',
                password='heal7!',
                port=5432
            )
            logger.info("âœ… PostgreSQL ì—°ê²° ì„±ê³µ")
        except Exception as e:
            logger.error(f"âŒ DB ì—°ê²° ì‹¤íŒ¨: {e}")
            raise

    async def analyze_existing_data(self) -> Dict:
        """ê¸°ì¡´ ë°ì´í„° ë¶„ì„"""
        logger.info("ğŸ” ê¸°ì¡´ ë°ì´í„° ë¶„ì„ ì‹œì‘...")
        
        # í‚¤ì›Œë“œë³„ í•´ì„ í†µê³„
        query = """
        SELECT 
            dk.keyword,
            di.interpretation_type,
            COUNT(*) as count,
            AVG(di.confidence_score) as avg_confidence
        FROM dream_interpretations di
        JOIN dream_keywords dk ON di.keyword_id = dk.id
        GROUP BY dk.keyword, di.interpretation_type
        ORDER BY COUNT(*) DESC
        """
        
        results = await self.conn.fetch(query)
        analysis = {}
        
        for row in results:
            keyword = row['keyword']
            if keyword not in analysis:
                analysis[keyword] = {}
            analysis[keyword][row['interpretation_type']] = {
                'count': row['count'],
                'avg_confidence': float(row['avg_confidence']) if row['avg_confidence'] else 8.0
            }
        
        logger.info(f"ğŸ“Š ë¶„ì„ ì™„ë£Œ: {len(analysis)}ê°œ í‚¤ì›Œë“œ")
        return analysis

    def categorize_keyword(self, keyword: str) -> str:
        """í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        # ì§ì ‘ ë§¤í•‘
        if keyword in self.category_mapping:
            return self.category_mapping[keyword]
        
        # íŒ¨í„´ ê¸°ë°˜ ë¶„ë¥˜
        if any(word in keyword for word in ['ëœ¨ê±°ìš´', 'ì°¨ê°€ìš´', 'ê¹¨ë—í•œ', 'ë”ëŸ¬ìš´']):
            return 'ìƒíƒœ'
        elif any(word in keyword for word in ['í¬ë‹¤', 'ì‘ë‹¤', 'í°', 'ì‘ì€', 'ê±°ëŒ€í•œ']):
            return 'í¬ê¸°'
        elif any(word in keyword for word in ['ë¹¨ê°„', 'íŒŒë€', 'ë…¸ë€', 'ê²€ì€', 'í°']):
            return 'ìƒ‰ê¹”'
        elif any(word in keyword for word in ['ì•„ë¦„ë‹¤ìš´', 'ë¬´ì„œìš´', 'ìŠ¬í”ˆ', 'ê¸°ìœ']):
            return 'ê°ì •'
        else:
            return 'ê¸°íƒ€'

    def extract_related_keywords(self, keyword: str, all_keywords: Set[str]) -> List[str]:
        """ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        related = []
        base_keyword = re.sub(r'(ëœ¨ê±°ìš´|ì°¨ê°€ìš´|í°|ì‘ì€|ì•„ë¦„ë‹¤ìš´|ë”ëŸ¬ìš´|ê¹¨ë—í•œ)\s*', '', keyword).strip()
        
        for other_keyword in all_keywords:
            if other_keyword != keyword:
                # ê°™ì€ ë² ì´ìŠ¤ í‚¤ì›Œë“œ
                other_base = re.sub(r'(ëœ¨ê±°ìš´|ì°¨ê°€ìš´|í°|ì‘ì€|ì•„ë¦„ë‹¤ìš´|ë”ëŸ¬ìš´|ê¹¨ë—í•œ)\s*', '', other_keyword).strip()
                if base_keyword == other_base:
                    related.append(other_keyword)
                # í¬í•¨ ê´€ê³„
                elif base_keyword in other_keyword or other_keyword in base_keyword:
                    if len(abs(len(base_keyword) - len(other_keyword))) <= 2:
                        related.append(other_keyword)
        
        return related[:5]  # ìµœëŒ€ 5ê°œ

    def calculate_quality_score(self, interpretations: Dict, keyword: str) -> float:
        """í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        base_score = 8.0
        
        # í•´ì„ íƒ€ì… ë‹¤ì–‘ì„±
        type_bonus = len(interpretations) * 0.5
        
        # í‚¤ì›Œë“œ ê¸¸ì´ ë³´ë„ˆìŠ¤ (êµ¬ì²´ì ì¼ìˆ˜ë¡ ë†’ìŒ)
        length_bonus = min(len(keyword) / 10, 1.0)
        
        # ì‹ ë¢°ë„ ì ìˆ˜
        confidence_avg = sum(
            data['avg_confidence'] for data in interpretations.values()
        ) / len(interpretations)
        
        final_score = min(base_score + type_bonus + length_bonus + (confidence_avg - 8.0), 10.0)
        return round(final_score, 1)

    async def migrate_data(self, analysis: Dict):
        """ë°ì´í„° ì´ì „ ì‹¤í–‰"""
        logger.info("ğŸš€ ë°ì´í„° ì´ì „ ì‹œì‘...")
        
        all_keywords = set(analysis.keys())
        migrated_count = 0
        
        for keyword, interpretations in analysis.items():
            try:
                # í•´ì„ í…ìŠ¤íŠ¸ í†µí•©
                traditional_texts = []
                modern_texts = []
                psychological_texts = []
                
                # ê° íƒ€ì…ë³„ í•´ì„ ìˆ˜ì§‘
                for interp_type, data in interpretations.items():
                    query = """
                    SELECT di.interpretation_text
                    FROM dream_interpretations di
                    JOIN dream_keywords dk ON di.keyword_id = dk.id
                    WHERE dk.keyword = $1 AND di.interpretation_type = $2
                    """
                    
                    texts = await self.conn.fetch(query, keyword, interp_type)
                    text_list = [row['interpretation_text'] for row in texts]
                    
                    if interp_type == 'traditional':
                        traditional_texts.extend(text_list)
                    elif interp_type == 'modern':
                        modern_texts.extend(text_list)
                    elif interp_type == 'psychological':
                        psychological_texts.extend(text_list)
                
                # ì¤‘ë³µ ì œê±° ë° í†µí•©
                traditional_interpretation = ' | '.join(set(traditional_texts)) if traditional_texts else None
                modern_interpretation = ' | '.join(set(modern_texts)) if modern_texts else None
                
                # ì‹¬ë¦¬í•™ì  í•´ì„ì´ ìˆìœ¼ë©´ í˜„ëŒ€ì  í•´ì„ì— í¬í•¨
                if psychological_texts:
                    psych_text = ' | '.join(set(psychological_texts))
                    if modern_interpretation:
                        modern_interpretation += f" | {psych_text}"
                    else:
                        modern_interpretation = psych_text
                
                # ê´€ë ¨ í‚¤ì›Œë“œ ë° ë©”íƒ€ë°ì´í„°
                related_keywords = self.extract_related_keywords(keyword, all_keywords)
                category = self.categorize_keyword(keyword)
                quality_score = self.calculate_quality_score(interpretations, keyword)
                
                # clean_dream_interpretationsì— ì‚½ì…
                insert_query = """
                INSERT INTO clean_dream_interpretations 
                (keyword, modern_interpretation, traditional_interpretation, 
                 related_keywords, category, quality_score, created_at, updated_at)
                VALUES ($1, $2, $3, $4, $5, $6, $7, $7)
                ON CONFLICT (keyword) DO UPDATE SET
                    modern_interpretation = EXCLUDED.modern_interpretation,
                    traditional_interpretation = EXCLUDED.traditional_interpretation,
                    related_keywords = EXCLUDED.related_keywords,
                    category = EXCLUDED.category,
                    quality_score = EXCLUDED.quality_score,
                    updated_at = EXCLUDED.updated_at
                """
                
                await self.conn.execute(
                    insert_query,
                    keyword,
                    modern_interpretation,
                    traditional_interpretation,
                    related_keywords,
                    category,
                    quality_score,
                    datetime.now()
                )
                
                migrated_count += 1
                if migrated_count % 100 == 0:
                    logger.info(f"ğŸ“Š ì§„í–‰ ìƒí™©: {migrated_count}/{len(analysis)} ì™„ë£Œ")
                
            except Exception as e:
                logger.error(f"âŒ {keyword} ì´ì „ ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"âœ… ì´ì „ ì™„ë£Œ: {migrated_count}ê°œ í‚¤ì›Œë“œ ì²˜ë¦¬")
        return migrated_count

    async def verify_migration(self) -> Dict:
        """ì´ì „ ê²°ê³¼ ê²€ì¦"""
        logger.info("ğŸ” ì´ì „ ê²°ê³¼ ê²€ì¦...")
        
        # ê¸°ë³¸ í†µê³„
        count_query = "SELECT COUNT(*) FROM clean_dream_interpretations"
        total_count = await self.conn.fetchval(count_query)
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬
        category_query = """
        SELECT category, COUNT(*) as count 
        FROM clean_dream_interpretations 
        GROUP BY category 
        ORDER BY count DESC
        """
        categories = await self.conn.fetch(category_query)
        
        # í’ˆì§ˆ ì ìˆ˜ ë¶„í¬
        quality_query = """
        SELECT 
            ROUND(quality_score) as score_range,
            COUNT(*) as count
        FROM clean_dream_interpretations 
        GROUP BY ROUND(quality_score)
        ORDER BY score_range
        """
        quality_dist = await self.conn.fetch(quality_query)
        
        # ìƒ˜í”Œ ë°ì´í„°
        sample_query = """
        SELECT keyword, category, quality_score, 
               LEFT(traditional_interpretation, 50) as traditional_preview,
               LEFT(modern_interpretation, 50) as modern_preview
        FROM clean_dream_interpretations 
        ORDER BY quality_score DESC 
        LIMIT 5
        """
        samples = await self.conn.fetch(sample_query)
        
        verification = {
            'total_count': total_count,
            'categories': {row['category']: row['count'] for row in categories},
            'quality_distribution': {row['score_range']: row['count'] for row in quality_dist},
            'top_samples': [dict(row) for row in samples]
        }
        
        logger.info(f"ğŸ“Š ê²€ì¦ ì™„ë£Œ: {total_count}ê°œ ë°ì´í„°")
        logger.info(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬: {len(verification['categories'])}ê°œ")
        
        return verification

    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.conn:
            await self.conn.close()
            logger.info("ğŸ” DB ì—°ê²° í•´ì œ")

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    migrator = DreamDataMigrator()
    
    try:
        # 1. DB ì—°ê²°
        await migrator.connect_db()
        
        # 2. ê¸°ì¡´ ë°ì´í„° ë¶„ì„
        analysis = await migrator.analyze_existing_data()
        
        # 3. ë°ì´í„° ì´ì „
        migrated_count = await migrator.migrate_data(analysis)
        
        # 4. ê²°ê³¼ ê²€ì¦
        verification = await migrator.verify_migration()
        
        # 5. ê²°ê³¼ ë¦¬í¬íŠ¸
        print("\n" + "="*60)
        print("ğŸ‰ HEAL7 ê¿ˆí’€ì´ ë°ì´í„° ì´ì „ ì™„ë£Œ!")
        print("="*60)
        print(f"ğŸ“Š ì´ì „ëœ í‚¤ì›Œë“œ: {migrated_count}ê°œ")
        print(f"ğŸ“Š ìµœì¢… ë°ì´í„°: {verification['total_count']}ê°œ")
        print(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬ ë¶„í¬:")
        for category, count in verification['categories'].items():
            print(f"   - {category}: {count}ê°œ")
        
        print(f"\nğŸŒŸ ìƒìœ„ í’ˆì§ˆ í‚¤ì›Œë“œ ìƒ˜í”Œ:")
        for sample in verification['top_samples']:
            print(f"   - {sample['keyword']} ({sample['category']}) - ì ìˆ˜: {sample['quality_score']}")
        
        # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œë„ ì €ì¥
        with open('/tmp/dream_migration_report.json', 'w', encoding='utf-8') as f:
            json.dump(verification, f, ensure_ascii=False, indent=2)
        
        logger.info("ğŸ“„ ìƒì„¸ ë¦¬í¬íŠ¸: /tmp/dream_migration_report.json")
        
    except Exception as e:
        logger.error(f"ğŸ’¥ ì´ì „ ê³¼ì • ì˜¤ë¥˜: {e}")
        raise
    finally:
        await migrator.close()

if __name__ == "__main__":
    asyncio.run(main())