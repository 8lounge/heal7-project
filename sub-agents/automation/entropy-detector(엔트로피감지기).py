#!/usr/bin/env python3
"""
ğŸŒªï¸ HEAL7 ì—”íŠ¸ë¡œí”¼ ê°ì§€ ë° ì •ë¦¬ ì‹œìŠ¤í…œ
ì‹œìŠ¤í…œ ë³µì¡ë„ ëª¨ë‹ˆí„°ë§ ë° ìë™ ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸

Author: AI Agent Team
Created: 2025-08-20
"""

import os
import shutil
import json
import re
from pathlib import Path
from typing import Dict, List, Any, Tuple
from datetime import datetime, timedelta
import logging
import hashlib

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class EntropyDetector:
    def __init__(self):
        self.timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        self.entropy_report = {
            'timestamp': self.timestamp,
            'entropy_level': 'low',
            'issues_found': [],
            'cleanup_actions': [],
            'recommendations': []
        }
        
        # í—ˆìš©ëœ íŒŒì¼/í´ë” (í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸)
        self.whitelist_files = {
            '/home/ubuntu/CLAUDE.md',
            '/home/ubuntu/.env.ai',
            '/home/ubuntu/.bashrc',
            '/home/ubuntu/.bash_history',
            '/home/ubuntu/.gitconfig',
            '/home/ubuntu/.ssh/',
            '/home/ubuntu/.local/',
            '/home/ubuntu/.cache/',
            '/home/ubuntu/snap/'
        }
        
        self.whitelist_patterns = [
            r'.*\.pem$',           # SSH í‚¤
            r'.*\.key$',           # í‚¤ íŒŒì¼
            r'.*\.crt$',           # ì¸ì¦ì„œ
            r'.*\.log$',           # ë¡œê·¸ íŒŒì¼ (logs/ í´ë” ì™¸ë¶€)
        ]
        
        # ì •ë¦¬ ëŒ€ìƒ ë””ë ‰í† ë¦¬
        self.scan_directories = [
            '/home/ubuntu/',
            '/tmp/',
            '/var/tmp/'
        ]
        
        # ì—”íŠ¸ë¡œí”¼ ì„ê³„ê°’
        self.thresholds = {
            'max_top_level_files': 3,       # ìµœìƒìœ„ í´ë” íŒŒì¼ ìˆ˜
            'max_top_level_folders': 1,     # ìµœìƒìœ„ í´ë” ë¹„í—ˆê°€ í´ë” ìˆ˜
            'max_temp_file_age_days': 7,    # ì„ì‹œ íŒŒì¼ ìµœëŒ€ ë³´ê´€ ì¼ìˆ˜
            'max_duplicate_files': 10,      # ì¤‘ë³µ íŒŒì¼ ìˆ˜
            'max_orphaned_configs': 5       # ê³ ì•„ ì„¤ì • íŒŒì¼ ìˆ˜
        }
    
    def scan_top_level_entropy(self) -> Dict[str, Any]:
        """ìµœìƒìœ„ ë””ë ‰í† ë¦¬ ì—”íŠ¸ë¡œí”¼ ìŠ¤ìº”"""
        home_dir = Path('/home/ubuntu')
        
        # í—ˆê°€ë˜ì§€ ì•Šì€ íŒŒì¼ë“¤
        unauthorized_files = []
        unauthorized_dirs = []
        
        for item in home_dir.iterdir():
            item_path = str(item)
            
            # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì²´í¬
            if item_path in self.whitelist_files:
                continue
            
            # íŒ¨í„´ ì²´í¬
            if any(re.match(pattern, item_path) for pattern in self.whitelist_patterns):
                continue
            
            # í—ˆê°€ëœ í”„ë¡œì íŠ¸ í´ë”ë“¤
            allowed_dirs = {
                'heal7-system', 'REFERENCE_LIBRARY', 'docs', 'scripts', 
                'logs', 'backups', 'database', 'archive', '.heal7-session'
            }
            
            if item.is_file():
                unauthorized_files.append({
                    'path': item_path,
                    'size': item.stat().st_size,
                    'modified': datetime.fromtimestamp(item.stat().st_mtime).strftime('%Y-%m-%d %H:%M:%S'),
                    'type': 'file'
                })
            elif item.is_dir() and item.name not in allowed_dirs:
                unauthorized_dirs.append({
                    'path': item_path,
                    'item_count': len(list(item.rglob('*'))),
                    'modified': datetime.fromtimestamp(item.stat().st_mtime).strftime('%Y-%m-%d %H:%M:%S'),
                    'type': 'directory'
                })
        
        return {
            'unauthorized_files': unauthorized_files,
            'unauthorized_dirs': unauthorized_dirs,
            'file_count': len(unauthorized_files),
            'dir_count': len(unauthorized_dirs)
        }
    
    def scan_temp_files(self) -> Dict[str, Any]:
        """ì„ì‹œ íŒŒì¼ ìŠ¤ìº”"""
        temp_patterns = [
            '*.tmp', '*.temp', '*.bak', '*.backup', '*.old',
            '*.swp', '*.swo', '*~', '.DS_Store', 'Thumbs.db'
        ]
        
        temp_files = []
        cutoff_date = datetime.now() - timedelta(days=self.thresholds['max_temp_file_age_days'])
        
        for scan_dir in ['/home/ubuntu', '/tmp', '/var/tmp']:
            scan_path = Path(scan_dir)
            if not scan_path.exists():
                continue
                
            for pattern in temp_patterns:
                for temp_file in scan_path.rglob(pattern):
                    if temp_file.is_file():
                        modified_time = datetime.fromtimestamp(temp_file.stat().st_mtime)
                        if modified_time < cutoff_date:
                            temp_files.append({
                                'path': str(temp_file),
                                'size': temp_file.stat().st_size,
                                'age_days': (datetime.now() - modified_time).days,
                                'pattern': pattern
                            })
        
        return {
            'temp_files': temp_files,
            'total_count': len(temp_files),
            'total_size_mb': sum(f['size'] for f in temp_files) / (1024 * 1024)
        }
    
    def scan_duplicate_files(self) -> Dict[str, Any]:
        """ì¤‘ë³µ íŒŒì¼ ìŠ¤ìº” (í•´ì‹œ ê¸°ë°˜)"""
        file_hashes = {}
        duplicates = []
        
        # ìŠ¤ìº”í•  ë””ë ‰í† ë¦¬ (node_modules ì œì™¸)
        scan_dirs = [
            '/home/ubuntu/heal7-system',
            '/home/ubuntu/REFERENCE_LIBRARY',
            '/home/ubuntu/scripts'
        ]
        
        for scan_dir in scan_dirs:
            scan_path = Path(scan_dir)
            if not scan_path.exists():
                continue
            
            for file_path in scan_path.rglob('*'):
                if (file_path.is_file() and 
                    'node_modules' not in str(file_path) and 
                    '.git' not in str(file_path) and
                    file_path.stat().st_size > 1024):  # 1KB ì´ìƒ íŒŒì¼ë§Œ
                    
                    try:
                        # íŒŒì¼ í•´ì‹œ ê³„ì‚°
                        hasher = hashlib.md5()
                        with open(file_path, 'rb') as f:
                            hasher.update(f.read())
                        file_hash = hasher.hexdigest()
                        
                        if file_hash in file_hashes:
                            # ì¤‘ë³µ ë°œê²¬
                            duplicates.append({
                                'hash': file_hash,
                                'files': [file_hashes[file_hash], str(file_path)],
                                'size': file_path.stat().st_size
                            })
                        else:
                            file_hashes[file_hash] = str(file_path)
                            
                    except Exception as e:
                        logging.warning(f"íŒŒì¼ í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨ {file_path}: {e}")
        
        return {
            'duplicates': duplicates,
            'duplicate_count': len(duplicates),
            'wasted_space_mb': sum(d['size'] for d in duplicates) / (1024 * 1024)
        }
    
    def scan_orphaned_configs(self) -> Dict[str, Any]:
        """ê³ ì•„ ì„¤ì • íŒŒì¼ ìŠ¤ìº”"""
        config_patterns = [
            '*.conf', '*.config', '*.cfg', '*.ini',
            '.env.*', '*.yaml', '*.yml', '*.json'
        ]
        
        orphaned_configs = []
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ê°€ ì•„ë‹Œ ìœ„ì¹˜ì˜ ì„¤ì • íŒŒì¼ë“¤
        scan_dirs = ['/home/ubuntu']
        
        for scan_dir in scan_dirs:
            scan_path = Path(scan_dir)
            
            for pattern in config_patterns:
                for config_file in scan_path.glob(pattern):  # í•˜ìœ„ ë””ë ‰í† ë¦¬ ì œì™¸, ìµœìƒìœ„ë§Œ
                    if config_file.is_file():
                        # ì•Œë ¤ì§„ ì„¤ì • íŒŒì¼ ì œì™¸
                        known_configs = {'.env.ai', 'CLAUDE.md'}
                        if config_file.name not in known_configs:
                            orphaned_configs.append({
                                'path': str(config_file),
                                'size': config_file.stat().st_size,
                                'modified': datetime.fromtimestamp(config_file.stat().st_mtime).strftime('%Y-%m-%d %H:%M:%S')
                            })
        
        return {
            'orphaned_configs': orphaned_configs,
            'count': len(orphaned_configs)
        }
    
    def scan_log_accumulation(self) -> Dict[str, Any]:
        """ë¡œê·¸ íŒŒì¼ ëˆ„ì  ìŠ¤ìº”"""
        log_dirs = [
            '/home/ubuntu/logs',
            '/var/log',
            '/tmp'
        ]
        
        large_logs = []
        old_logs = []
        cutoff_date = datetime.now() - timedelta(days=30)
        
        for log_dir in log_dirs:
            log_path = Path(log_dir)
            if not log_path.exists():
                continue
            
            for log_file in log_path.rglob('*.log'):
                if log_file.is_file():
                    file_size = log_file.stat().st_size
                    modified_time = datetime.fromtimestamp(log_file.stat().st_mtime)
                    
                    # í° ë¡œê·¸ íŒŒì¼ (10MB ì´ìƒ)
                    if file_size > 10 * 1024 * 1024:
                        large_logs.append({
                            'path': str(log_file),
                            'size_mb': file_size / (1024 * 1024),
                            'modified': modified_time.strftime('%Y-%m-%d %H:%M:%S')
                        })
                    
                    # ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼
                    if modified_time < cutoff_date:
                        old_logs.append({
                            'path': str(log_file),
                            'age_days': (datetime.now() - modified_time).days,
                            'size_mb': file_size / (1024 * 1024)
                        })
        
        return {
            'large_logs': large_logs,
            'old_logs': old_logs,
            'large_count': len(large_logs),
            'old_count': len(old_logs)
        }
    
    def scan_broken_symlinks(self) -> Dict[str, Any]:
        """ê¹¨ì§„ ì‹¬ë³¼ë¦­ ë§í¬ ìŠ¤ìº”"""
        broken_links = []
        
        for scan_dir in ['/home/ubuntu']:
            scan_path = Path(scan_dir)
            
            for item in scan_path.rglob('*'):
                if item.is_symlink() and not item.exists():
                    broken_links.append({
                        'path': str(item),
                        'target': str(item.readlink()) if item.readlink() else 'unknown'
                    })
        
        return {
            'broken_links': broken_links,
            'count': len(broken_links)
        }
    
    def calculate_entropy_level(self, scan_results: Dict[str, Any]) -> str:
        """ì—”íŠ¸ë¡œí”¼ ë ˆë²¨ ê³„ì‚°"""
        entropy_score = 0
        
        # ìµœìƒìœ„ íŒŒì¼/í´ë” ìˆ˜
        top_level = scan_results.get('top_level', {})
        if top_level.get('file_count', 0) > self.thresholds['max_top_level_files']:
            entropy_score += 3
        if top_level.get('dir_count', 0) > self.thresholds['max_top_level_folders']:
            entropy_score += 5
        
        # ì„ì‹œ íŒŒì¼ ìˆ˜
        temp_files = scan_results.get('temp_files', {})
        if temp_files.get('total_count', 0) > 20:
            entropy_score += 2
        
        # ì¤‘ë³µ íŒŒì¼
        duplicates = scan_results.get('duplicates', {})
        if duplicates.get('duplicate_count', 0) > self.thresholds['max_duplicate_files']:
            entropy_score += 2
        
        # ê³ ì•„ ì„¤ì • íŒŒì¼
        orphaned = scan_results.get('orphaned_configs', {})
        if orphaned.get('count', 0) > self.thresholds['max_orphaned_configs']:
            entropy_score += 2
        
        # ë¡œê·¸ ëˆ„ì 
        logs = scan_results.get('log_accumulation', {})
        if logs.get('large_count', 0) > 5 or logs.get('old_count', 0) > 10:
            entropy_score += 1
        
        # ê¹¨ì§„ ë§í¬
        broken = scan_results.get('broken_symlinks', {})
        if broken.get('count', 0) > 0:
            entropy_score += 1
        
        # ì—”íŠ¸ë¡œí”¼ ë ˆë²¨ ê²°ì •
        if entropy_score >= 10:
            return 'critical'
        elif entropy_score >= 7:
            return 'high'
        elif entropy_score >= 4:
            return 'medium'
        else:
            return 'low'
    
    def generate_cleanup_plan(self, scan_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ì •ë¦¬ ê³„íš ìƒì„±"""
        cleanup_actions = []
        
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        temp_files = scan_results.get('temp_files', {})
        if temp_files.get('total_count', 0) > 0:
            cleanup_actions.append({
                'type': 'temp_files_cleanup',
                'description': f"{temp_files['total_count']}ê°œì˜ ì˜¤ë˜ëœ ì„ì‹œ íŒŒì¼ ì‚­ì œ",
                'files': temp_files.get('temp_files', []),
                'priority': 'medium',
                'safe': True
            })
        
        # ì¤‘ë³µ íŒŒì¼ ì •ë¦¬ (ìˆ˜ë™ í™•ì¸ í•„ìš”)
        duplicates = scan_results.get('duplicates', {})
        if duplicates.get('duplicate_count', 0) > 0:
            cleanup_actions.append({
                'type': 'duplicate_files_review',
                'description': f"{duplicates['duplicate_count']}ê°œì˜ ì¤‘ë³µ íŒŒì¼ ê²€í†  í•„ìš”",
                'files': duplicates.get('duplicates', []),
                'priority': 'low',
                'safe': False  # ìˆ˜ë™ ê²€í†  í•„ìš”
            })
        
        # ê³ ì•„ ì„¤ì • íŒŒì¼ ì •ë¦¬
        orphaned = scan_results.get('orphaned_configs', {})
        if orphaned.get('count', 0) > 0:
            cleanup_actions.append({
                'type': 'orphaned_configs_review',
                'description': f"{orphaned['count']}ê°œì˜ ê³ ì•„ ì„¤ì • íŒŒì¼ ê²€í† ",
                'files': orphaned.get('orphaned_configs', []),
                'priority': 'medium',
                'safe': False  # ìˆ˜ë™ ê²€í†  í•„ìš”
            })
        
        # í° ë¡œê·¸ íŒŒì¼ ì••ì¶•
        logs = scan_results.get('log_accumulation', {})
        if logs.get('large_count', 0) > 0:
            cleanup_actions.append({
                'type': 'log_compression',
                'description': f"{logs['large_count']}ê°œì˜ í° ë¡œê·¸ íŒŒì¼ ì••ì¶•",
                'files': logs.get('large_logs', []),
                'priority': 'low',
                'safe': True
            })
        
        # ê¹¨ì§„ ì‹¬ë³¼ë¦­ ë§í¬ ì •ë¦¬
        broken = scan_results.get('broken_symlinks', {})
        if broken.get('count', 0) > 0:
            cleanup_actions.append({
                'type': 'broken_symlinks_cleanup',
                'description': f"{broken['count']}ê°œì˜ ê¹¨ì§„ ì‹¬ë³¼ë¦­ ë§í¬ ì‚­ì œ",
                'files': broken.get('broken_links', []),
                'priority': 'low',
                'safe': True
            })
        
        return cleanup_actions
    
    def execute_safe_cleanup(self, cleanup_actions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì•ˆì „í•œ ì •ë¦¬ ì‘ì—… ì‹¤í–‰"""
        executed_actions = []
        skipped_actions = []
        
        for action in cleanup_actions:
            if not action.get('safe', False):
                skipped_actions.append({
                    'action': action,
                    'reason': 'Manual review required'
                })
                continue
            
            try:
                if action['type'] == 'temp_files_cleanup':
                    self._cleanup_temp_files(action['files'])
                elif action['type'] == 'log_compression':
                    self._compress_large_logs(action['files'])
                elif action['type'] == 'broken_symlinks_cleanup':
                    self._cleanup_broken_symlinks(action['files'])
                
                executed_actions.append(action)
                logging.info(f"ì •ë¦¬ ì‘ì—… ì™„ë£Œ: {action['description']}")
                
            except Exception as e:
                logging.error(f"ì •ë¦¬ ì‘ì—… ì‹¤íŒ¨ {action['type']}: {e}")
                skipped_actions.append({
                    'action': action,
                    'reason': f'Error: {str(e)}'
                })
        
        return {
            'executed': executed_actions,
            'skipped': skipped_actions,
            'executed_count': len(executed_actions),
            'skipped_count': len(skipped_actions)
        }
    
    def _cleanup_temp_files(self, temp_files: List[Dict[str, Any]]):
        """ì„ì‹œ íŒŒì¼ ì‚­ì œ"""
        for temp_file in temp_files:
            try:
                file_path = Path(temp_file['path'])
                if file_path.exists() and file_path.is_file():
                    file_path.unlink()
                    logging.debug(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ: {file_path}")
            except Exception as e:
                logging.warning(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ {temp_file['path']}: {e}")
    
    def _compress_large_logs(self, large_logs: List[Dict[str, Any]]):
        """í° ë¡œê·¸ íŒŒì¼ ì••ì¶•"""
        for log_info in large_logs:
            try:
                log_path = Path(log_info['path'])
                if log_path.exists() and log_path.suffix == '.log':
                    # gzipìœ¼ë¡œ ì••ì¶•
                    import gzip
                    compressed_path = log_path.with_suffix('.log.gz')
                    
                    with open(log_path, 'rb') as f_in:
                        with gzip.open(compressed_path, 'wb') as f_out:
                            shutil.copyfileobj(f_in, f_out)
                    
                    # ì›ë³¸ íŒŒì¼ ì‚­ì œ
                    log_path.unlink()
                    logging.info(f"ë¡œê·¸ íŒŒì¼ ì••ì¶•: {log_path} -> {compressed_path}")
                    
            except Exception as e:
                logging.warning(f"ë¡œê·¸ ì••ì¶• ì‹¤íŒ¨ {log_info['path']}: {e}")
    
    def _cleanup_broken_symlinks(self, broken_links: List[Dict[str, Any]]):
        """ê¹¨ì§„ ì‹¬ë³¼ë¦­ ë§í¬ ì‚­ì œ"""
        for link_info in broken_links:
            try:
                link_path = Path(link_info['path'])
                if link_path.is_symlink() and not link_path.exists():
                    link_path.unlink()
                    logging.debug(f"ê¹¨ì§„ ì‹¬ë³¼ë¦­ ë§í¬ ì‚­ì œ: {link_path}")
            except Exception as e:
                logging.warning(f"ì‹¬ë³¼ë¦­ ë§í¬ ì‚­ì œ ì‹¤íŒ¨ {link_info['path']}: {e}")
    
    def generate_recommendations(self, scan_results: Dict[str, Any], entropy_level: str) -> List[str]:
        """ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if entropy_level in ['critical', 'high']:
            recommendations.append("ğŸš¨ ì‹œìŠ¤í…œ ì—”íŠ¸ë¡œí”¼ê°€ ë†’ìŠµë‹ˆë‹¤. ì¦‰ì‹œ ì •ë¦¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ì„¸ìš”")
            
        # ìµœìƒìœ„ ë””ë ‰í† ë¦¬ ì •ë¦¬
        top_level = scan_results.get('top_level', {})
        if top_level.get('file_count', 0) > 0:
            recommendations.append(f"ğŸ“ ìµœìƒìœ„ ë””ë ‰í† ë¦¬ì— {top_level['file_count']}ê°œì˜ ë¯¸í—ˆê°€ íŒŒì¼ì´ ìˆìŠµë‹ˆë‹¤")
        
        if top_level.get('dir_count', 0) > 0:
            recommendations.append(f"ğŸ“‚ ìµœìƒìœ„ ë””ë ‰í† ë¦¬ì— {top_level['dir_count']}ê°œì˜ ë¯¸í—ˆê°€ í´ë”ê°€ ìˆìŠµë‹ˆë‹¤")
        
        # ì €ì¥ê³µê°„ ì ˆì•½
        duplicates = scan_results.get('duplicates', {})
        if duplicates.get('wasted_space_mb', 0) > 10:
            recommendations.append(f"ğŸ’¾ ì¤‘ë³µ íŒŒì¼ë¡œ {duplicates['wasted_space_mb']:.1f}MB ë‚­ë¹„ë˜ê³  ìˆìŠµë‹ˆë‹¤")
        
        # ì •ê¸° ì •ë¦¬ ì œì•ˆ
        if entropy_level == 'low':
            recommendations.append("âœ… ì‹œìŠ¤í…œì´ ê¹”ë”í•©ë‹ˆë‹¤. ì£¼ê°„ ì •ë¦¬ ìŠ¤ì¼€ì¤„ì„ ìœ ì§€í•˜ì„¸ìš”")
        
        return recommendations
    
    def run_full_scan(self) -> Dict[str, Any]:
        """ì „ì²´ ì—”íŠ¸ë¡œí”¼ ìŠ¤ìº” ì‹¤í–‰"""
        logging.info("ì—”íŠ¸ë¡œí”¼ ìŠ¤ìº” ì‹œì‘...")
        
        scan_results = {
            'top_level': self.scan_top_level_entropy(),
            'temp_files': self.scan_temp_files(),
            'duplicates': self.scan_duplicate_files(),
            'orphaned_configs': self.scan_orphaned_configs(),
            'log_accumulation': self.scan_log_accumulation(),
            'broken_symlinks': self.scan_broken_symlinks()
        }
        
        # ì—”íŠ¸ë¡œí”¼ ë ˆë²¨ ê³„ì‚°
        entropy_level = self.calculate_entropy_level(scan_results)
        
        # ì •ë¦¬ ê³„íš ìƒì„±
        cleanup_plan = self.generate_cleanup_plan(scan_results)
        
        # ì¶”ì²œì‚¬í•­ ìƒì„±
        recommendations = self.generate_recommendations(scan_results, entropy_level)
        
        # ê²°ê³¼ êµ¬ì„±
        self.entropy_report.update({
            'entropy_level': entropy_level,
            'scan_results': scan_results,
            'cleanup_plan': cleanup_plan,
            'recommendations': recommendations
        })
        
        logging.info(f"ì—”íŠ¸ë¡œí”¼ ìŠ¤ìº” ì™„ë£Œ. ë ˆë²¨: {entropy_level}")
        
        return self.entropy_report
    
    def save_report(self, report: Dict[str, Any]):
        """ì—”íŠ¸ë¡œí”¼ ë¦¬í¬íŠ¸ ì €ì¥"""
        report_dir = Path('/home/ubuntu/logs/entropy-reports')
        report_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = report_dir / f"entropy_report_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logging.info(f"ì—”íŠ¸ë¡œí”¼ ë¦¬í¬íŠ¸ ì €ì¥ë¨: {report_file}")
        
        # ìµœì‹  ë¦¬í¬íŠ¸ ë§í¬ ìƒì„±
        latest_link = report_dir / "latest.json"
        if latest_link.exists():
            latest_link.unlink()
        latest_link.symlink_to(report_file.name)
    
    def print_summary(self, report: Dict[str, Any]):
        """ì—”íŠ¸ë¡œí”¼ ë¦¬í¬íŠ¸ ìš”ì•½ ì¶œë ¥"""
        entropy_emoji = {
            'low': 'ğŸŸ¢',
            'medium': 'ğŸŸ¡', 
            'high': 'ğŸŸ ',
            'critical': 'ğŸ”´'
        }
        
        print(f"\n{'='*60}")
        print(f"ğŸŒªï¸ HEAL7 ì—”íŠ¸ë¡œí”¼ ê°ì§€ ë¦¬í¬íŠ¸")
        print(f"{'='*60}")
        print(f"ğŸ“… ìŠ¤ìº” ì‹œê°„: {report['timestamp']}")
        print(f"ğŸ“Š ì—”íŠ¸ë¡œí”¼ ë ˆë²¨: {entropy_emoji.get(report['entropy_level'], 'â“')} {report['entropy_level'].upper()}")
        print(f"{'='*60}")
        
        # ì£¼ìš” ì´ìŠˆ ìš”ì•½
        scan_results = report.get('scan_results', {})
        
        top_level = scan_results.get('top_level', {})
        print(f"ğŸ“ ìµœìƒìœ„ ë¯¸í—ˆê°€ íŒŒì¼: {top_level.get('file_count', 0)}ê°œ")
        print(f"ğŸ“‚ ìµœìƒìœ„ ë¯¸í—ˆê°€ í´ë”: {top_level.get('dir_count', 0)}ê°œ")
        
        temp_files = scan_results.get('temp_files', {})
        print(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ ì„ì‹œ íŒŒì¼: {temp_files.get('total_count', 0)}ê°œ ({temp_files.get('total_size_mb', 0):.1f}MB)")
        
        duplicates = scan_results.get('duplicates', {})
        print(f"ğŸ“‹ ì¤‘ë³µ íŒŒì¼: {duplicates.get('duplicate_count', 0)}ê°œ ({duplicates.get('wasted_space_mb', 0):.1f}MB ë‚­ë¹„)")
        
        # ì •ë¦¬ ê³„íš
        cleanup_plan = report.get('cleanup_plan', [])
        if cleanup_plan:
            print(f"\nğŸ§¹ ì •ë¦¬ ê³„íš:")
            for i, action in enumerate(cleanup_plan, 1):
                safe_indicator = "âœ…" if action.get('safe') else "âš ï¸"
                print(f"   {i}. {safe_indicator} {action['description']}")
        
        # ì¶”ì²œì‚¬í•­
        recommendations = report.get('recommendations', [])
        if recommendations:
            print(f"\nğŸ“‹ ì¶”ì²œì‚¬í•­:")
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")
        
        print(f"{'='*60}\n")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        detector = EntropyDetector()
        report = detector.run_full_scan()
        detector.save_report(report)
        detector.print_summary(report)
        
        # ìë™ ì •ë¦¬ ì˜µì…˜ (ì•ˆì „í•œ ì‘ì—…ë§Œ)
        cleanup_plan = report.get('cleanup_plan', [])
        safe_actions = [action for action in cleanup_plan if action.get('safe', False)]
        
        if safe_actions:
            print(f"ğŸ¤– ì•ˆì „í•œ ì •ë¦¬ ì‘ì—… {len(safe_actions)}ê°œë¥¼ ìë™ ì‹¤í–‰í•©ë‹ˆë‹¤...")
            cleanup_result = detector.execute_safe_cleanup(safe_actions)
            print(f"âœ… {cleanup_result['executed_count']}ê°œ ì‘ì—… ì™„ë£Œ, {cleanup_result['skipped_count']}ê°œ ê±´ë„ˆëœ€")
        
        # í¬ë¦¬í‹°ì»¬ ì—”íŠ¸ë¡œí”¼ë©´ exit code 1 ë°˜í™˜
        if report['entropy_level'] == 'critical':
            return 1
        
        return 0
        
    except Exception as e:
        logging.error(f"ì—”íŠ¸ë¡œí”¼ ìŠ¤ìº” ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return 1

if __name__ == "__main__":
    exit(main())