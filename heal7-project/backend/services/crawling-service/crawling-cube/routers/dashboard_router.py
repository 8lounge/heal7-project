#!/usr/bin/env python3
"""
ğŸš€ í¬ë¡¤ë§ ëŒ€ì‹œë³´ë“œ API ë¼ìš°í„°
Frontend ëŒ€ì‹œë³´ë“œ ì»´í¬ë„ŒíŠ¸ë“¤ê³¼ ì—°ë™ë˜ëŠ” ì‹¤ì œ API ì—”ë“œí¬ì¸íŠ¸ë“¤

Created: 2025-09-01
Author: HEAL7 Development Team
"""

import logging
import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from fastapi import APIRouter, HTTPException, WebSocket, WebSocketDisconnect
from pydantic import BaseModel, Field
import uuid
import json
import random

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# API ë¼ìš°í„° ìƒì„±
router = APIRouter(prefix="/api", tags=["dashboard"])

# ================================
# ë°ì´í„° ëª¨ë¸ë“¤
# ================================

class MonitoringData(BaseModel):
    """ëª¨ë‹ˆí„°ë§ ë°ì´í„° ëª¨ë¸"""
    timestamp: str
    active_crawlers: int = 0
    success_rate: float = 0.0
    requests_per_minute: int = 0
    avg_response_time: float = 0.0
    error_rate: float = 0.0
    total_data_collected: int = 0

class CrawlerStatus(BaseModel):
    """í¬ë¡¤ëŸ¬ ìƒíƒœ ëª¨ë¸"""
    id: str
    name: str
    status: str  # 'running', 'idle', 'error', 'stopped'
    tier: str   # 'httpx', 'playwright' (3ë‹¨ê³„ ê°„ì†Œí™” ì‹œìŠ¤í…œ)
    current_url: Optional[str] = None
    requests_today: int = 0
    success_rate: float = 0.0
    last_active: str
    performance_score: float = 0.0

class AIModelStats(BaseModel):
    """AI ëª¨ë¸ í†µê³„ ëª¨ë¸"""
    model_name: str
    requests_count: int = 0
    success_rate: float = 0.0
    avg_response_time: float = 0.0
    cost_usd: float = 0.0
    last_used: str

class CrawlingJob(BaseModel):
    """í¬ë¡¤ë§ ì‘ì—… ëª¨ë¸"""
    id: str
    name: str
    status: str  # 'running', 'scheduled', 'paused', 'completed', 'failed'
    target_url: str
    crawler_tier: str
    created_at: str
    scheduled_at: Optional[str] = None
    completed_at: Optional[str] = None
    progress: int = 0  # 0-100
    items_collected: int = 0
    error_message: Optional[str] = None

class DataItem(BaseModel):
    """ë°ì´í„° í•­ëª© ëª¨ë¸"""
    id: str
    title: str
    content: str
    source_url: str
    crawler_tier: str
    data_type: str
    quality: str
    collected_at: str
    size: int
    processing_status: str
    ai_analyzed: bool = False
    tags: List[str] = []
    metadata: Dict[str, Any] = {}

# ================================
# ë©”ëª¨ë¦¬ ì €ì¥ì†Œ (ì‹¤ì œ DB ì—°ë™ê¹Œì§€ì˜ ì„ì‹œ)
# ================================

# ì‹¤ì‹œê°„ ë°ì´í„° ì €ì¥ì†Œ
monitoring_data_store: List[MonitoringData] = []
crawler_status_store: List[CrawlerStatus] = []
ai_stats_store: List[AIModelStats] = []
crawling_jobs_store: List[CrawlingJob] = []
data_items_store: List[DataItem] = []

def init_sample_data():
    """ì´ˆê¸° ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    global crawler_status_store, ai_stats_store
    
    # í¬ë¡¤ëŸ¬ ìƒíƒœ ì´ˆê¸°í™”
    crawler_tiers = ['httpx', 'playwright']  # 3ë‹¨ê³„ ê°„ì†Œí™” ì‹œìŠ¤í…œ
    crawler_statuses = ['running', 'idle', 'error', 'stopped']
    
    crawler_status_store = []
    for i in range(6):
        crawler_status_store.append(CrawlerStatus(
            id=f"crawler_{i+1}",
            name=f"{random.choice(crawler_tiers).title()} Crawler {i+1}",
            status=random.choice(crawler_statuses),
            tier=random.choice(crawler_tiers),
            current_url=f"https://example-site-{i+1}.com" if i % 2 == 0 else None,
            requests_today=random.randint(0, 500),
            success_rate=random.uniform(75.0, 99.0),
            last_active=datetime.now().isoformat(),
            performance_score=random.uniform(60.0, 95.0)
        ))
    
    # AI ëª¨ë¸ í†µê³„ ì´ˆê¸°í™”
    ai_models = ['Gemini Flash 2.0', 'GPT-4o', 'Claude Sonnet 3.5']
    ai_stats_store = []
    for model in ai_models:
        ai_stats_store.append(AIModelStats(
            model_name=model,
            requests_count=random.randint(10, 100),
            success_rate=random.uniform(85.0, 99.0),
            avg_response_time=random.uniform(0.5, 3.0),
            cost_usd=random.uniform(0.50, 5.00),
            last_used=datetime.now().isoformat()
        ))

# ì´ˆê¸° ë°ì´í„° ìƒì„±
init_sample_data()

# ================================
# ëŒ€ì‹œë³´ë“œ ëª¨ë‹ˆí„°ë§ API
# ================================

@router.get("/crawling/dashboard/monitoring")
async def get_monitoring_data():
    """ğŸ“Š ëŒ€ì‹œë³´ë“œ ëª¨ë‹ˆí„°ë§ ë°ì´í„° ì¡°íšŒ"""
    try:
        logger.info("[API] ëª¨ë‹ˆí„°ë§ ë°ì´í„° ìš”ì²­ ì²˜ë¦¬ ì¤‘...")
        
        # í˜„ì¬ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§ ë°ì´í„° ìƒì„±
        current_data = MonitoringData(
            timestamp=datetime.now().isoformat(),
            active_crawlers=len([c for c in crawler_status_store if c.status == 'running']),
            success_rate=sum(c.success_rate for c in crawler_status_store) / len(crawler_status_store) if crawler_status_store else 0,
            requests_per_minute=sum(c.requests_today for c in crawler_status_store) // 1440 if crawler_status_store else 0,  # ëŒ€ëµì ì¸ ë¶„ë‹¹ ìš”ì²­
            avg_response_time=random.uniform(0.8, 2.5),
            error_rate=random.uniform(0.1, 5.0),
            total_data_collected=len(data_items_store)
        )
        
        # ìµœê·¼ ë°ì´í„° ì €ì¥ (ìµœëŒ€ 100ê°œ)
        monitoring_data_store.append(current_data)
        if len(monitoring_data_store) > 100:
            monitoring_data_store.pop(0)
        
        logger.info(f"[API] ëª¨ë‹ˆí„°ë§ ë°ì´í„° ë°˜í™˜ - í™œì„± í¬ë¡¤ëŸ¬: {current_data.active_crawlers}")
        return {"data": current_data.dict()}
        
    except Exception as e:
        logger.error(f"[API] ëª¨ë‹ˆí„°ë§ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ëª¨ë‹ˆí„°ë§ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@router.get("/crawling/dashboard/crawlers")
async def get_crawler_status():
    """ğŸ¤– í¬ë¡¤ëŸ¬ ìƒíƒœ ëª©ë¡ ì¡°íšŒ"""
    try:
        logger.info("[API] í¬ë¡¤ëŸ¬ ìƒíƒœ ìš”ì²­ ì²˜ë¦¬ ì¤‘...")
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸ (ì‹¤ì œë¡œëŠ” DBì—ì„œ ì¡°íšŒ)
        for crawler in crawler_status_store:
            crawler.last_active = datetime.now().isoformat()
        
        logger.info(f"[API] í¬ë¡¤ëŸ¬ ìƒíƒœ ë°˜í™˜ - ì´ {len(crawler_status_store)}ê°œ")
        return {"crawlers": [c.dict() for c in crawler_status_store]}
        
    except Exception as e:
        logger.error(f"[API] í¬ë¡¤ëŸ¬ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"í¬ë¡¤ëŸ¬ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

# ================================
# AI ëª¨ë¸ í†µê³„ API
# ================================

@router.get("/ai/models/stats")
async def get_ai_model_stats():
    """ğŸ§  AI ëª¨ë¸ í†µê³„ ì¡°íšŒ"""
    try:
        logger.info("[API] AI ëª¨ë¸ í†µê³„ ìš”ì²­ ì²˜ë¦¬ ì¤‘...")
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        for stat in ai_stats_store:
            stat.last_used = datetime.now().isoformat()
        
        # ì§‘ê³„ ë°ì´í„° ê³„ì‚°
        total_stats = {
            "total_processed": sum(s.requests_count for s in ai_stats_store),
            "gemini_flash": next((s.requests_count for s in ai_stats_store if "Gemini" in s.model_name), 0),
            "gpt4o": next((s.requests_count for s in ai_stats_store if "GPT" in s.model_name), 0),
            "claude_sonnet": next((s.requests_count for s in ai_stats_store if "Claude" in s.model_name), 0),
            "success_rate": sum(s.success_rate for s in ai_stats_store) / len(ai_stats_store) if ai_stats_store else 0
        }
        
        logger.info(f"[API] AI ëª¨ë¸ í†µê³„ ë°˜í™˜ - ì´ ì²˜ë¦¬: {total_stats['total_processed']}")
        return {
            "models": [s.dict() for s in ai_stats_store],
            "aggregated": total_stats
        }
        
    except Exception as e:
        logger.error(f"[API] AI ëª¨ë¸ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"AI ëª¨ë¸ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@router.get("/ai/models/recent-jobs")
async def get_recent_ai_jobs():
    """ğŸ”„ ìµœê·¼ AI ì‘ì—… ëª©ë¡ ì¡°íšŒ"""
    try:
        logger.info("[API] ìµœê·¼ AI ì‘ì—… ìš”ì²­ ì²˜ë¦¬ ì¤‘...")
        
        # ìµœê·¼ ì™„ë£Œëœ ì‘ì—…ë“¤ (ì‹¤ì œë¡œëŠ” DBì—ì„œ ì¡°íšŒ)
        recent_jobs = [
            {
                "id": str(uuid.uuid4()),
                "model": random.choice(["Gemini Flash 2.0", "GPT-4o", "Claude Sonnet 3.5"]),
                "task": random.choice(["ë°ì´í„° ì¶”ì¶œ", "ì½˜í…ì¸  ë¶„ì„", "ì „ëµ ìƒì„±"]),
                "status": "completed",
                "duration": random.uniform(0.5, 3.0),
                "completed_at": (datetime.now() - timedelta(minutes=random.randint(1, 60))).isoformat()
            }
            for _ in range(10)
        ]
        
        logger.info(f"[API] ìµœê·¼ AI ì‘ì—… ë°˜í™˜ - {len(recent_jobs)}ê°œ")
        return {"jobs": recent_jobs}
        
    except Exception as e:
        logger.error(f"[API] ìµœê·¼ AI ì‘ì—… ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ìµœê·¼ AI ì‘ì—… ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

# ================================
# í¬ë¡¤ë§ ì‘ì—… ê´€ë¦¬ API
# ================================

@router.get("/crawling/jobs")
async def get_crawling_jobs():
    """ğŸ“‹ í¬ë¡¤ë§ ì‘ì—… ëª©ë¡ ì¡°íšŒ"""
    try:
        logger.info("[API] í¬ë¡¤ë§ ì‘ì—… ëª©ë¡ ìš”ì²­ ì²˜ë¦¬ ì¤‘...")
        
        # ì‘ì—… ëª©ë¡ ë°˜í™˜
        logger.info(f"[API] í¬ë¡¤ë§ ì‘ì—… ëª©ë¡ ë°˜í™˜ - {len(crawling_jobs_store)}ê°œ")
        return {"jobs": [j.dict() for j in crawling_jobs_store]}
        
    except Exception as e:
        logger.error(f"[API] í¬ë¡¤ë§ ì‘ì—… ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"í¬ë¡¤ë§ ì‘ì—… ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@router.post("/crawling/jobs/{job_id}/{action}")
async def handle_job_action(job_id: str, action: str):
    """âš¡ í¬ë¡¤ë§ ì‘ì—… ì•¡ì…˜ ì²˜ë¦¬"""
    try:
        logger.info(f"[API] ì‘ì—… ì•¡ì…˜ ìš”ì²­ - JobID: {job_id}, Action: {action}")
        
        # ì‘ì—… ì°¾ê¸°
        job = next((j for j in crawling_jobs_store if j.id == job_id), None)
        if not job:
            raise HTTPException(status_code=404, detail=f"ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {job_id}")
        
        # ì•¡ì…˜ ìˆ˜í–‰
        if action == "start":
            job.status = "running"
        elif action == "pause":
            job.status = "paused"
        elif action == "stop":
            job.status = "stopped"
        elif action == "delete":
            crawling_jobs_store.remove(job)
            return {"message": f"ì‘ì—…ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤: {job_id}"}
        else:
            raise HTTPException(status_code=400, detail=f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì•¡ì…˜: {action}")
        
        logger.info(f"[API] ì‘ì—… ì•¡ì…˜ ì™„ë£Œ - JobID: {job_id}, ìƒˆ ìƒíƒœ: {job.status}")
        return {"message": f"ì‘ì—… {action} ì™„ë£Œ", "job": job.dict()}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[API] ì‘ì—… ì•¡ì…˜ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì‘ì—… ì•¡ì…˜ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

@router.post("/crawling/jobs")
async def create_crawling_job(job_data: Dict[str, Any]):
    """â• ìƒˆ í¬ë¡¤ë§ ì‘ì—… ìƒì„±"""
    try:
        logger.info(f"[API] ìƒˆ ì‘ì—… ìƒì„± ìš”ì²­ - {job_data}")
        
        # ìƒˆ ì‘ì—… ìƒì„±
        new_job = CrawlingJob(
            id=str(uuid.uuid4()),
            name=job_data.get("name", f"ì‘ì—… {len(crawling_jobs_store) + 1}"),
            status="scheduled",
            target_url=job_data.get("url", ""),
            crawler_tier=job_data.get("tier", "httpx"),
            created_at=datetime.now().isoformat(),
            scheduled_at=job_data.get("scheduled_at"),
            progress=0,
            items_collected=0
        )
        
        crawling_jobs_store.append(new_job)
        
        logger.info(f"[API] ìƒˆ ì‘ì—… ìƒì„± ì™„ë£Œ - JobID: {new_job.id}")
        return {"message": "ì‘ì—…ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤", "job": new_job.dict()}
        
    except Exception as e:
        logger.error(f"[API] ì‘ì—… ìƒì„± ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì‘ì—… ìƒì„± ì‹¤íŒ¨: {str(e)}")

# ================================
# ë°ì´í„° ê´€ë¦¬ API
# ================================

@router.get("/data/items")
async def get_data_items():
    """ğŸ“„ ë°ì´í„° í•­ëª© ëª©ë¡ ì¡°íšŒ"""
    try:
        logger.info("[API] ë°ì´í„° í•­ëª© ëª©ë¡ ìš”ì²­ ì²˜ë¦¬ ì¤‘...")
        
        logger.info(f"[API] ë°ì´í„° í•­ëª© ëª©ë¡ ë°˜í™˜ - {len(data_items_store)}ê°œ")
        return {"items": [d.dict() for d in data_items_store]}
        
    except Exception as e:
        logger.error(f"[API] ë°ì´í„° í•­ëª© ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë°ì´í„° í•­ëª© ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@router.post("/data/export")
async def export_data(export_request: Dict[str, Any]):
    """ğŸ“¤ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
    try:
        item_ids = export_request.get("itemIds", [])
        format_type = export_request.get("format", "csv")
        
        logger.info(f"[API] ë°ì´í„° ë‚´ë³´ë‚´ê¸° ìš”ì²­ - {len(item_ids)}ê°œ í•­ëª©, í˜•ì‹: {format_type}")
        
        # ì‹¤ì œë¡œëŠ” íŒŒì¼ì„ ìƒì„±í•˜ê³  ë‹¤ìš´ë¡œë“œ URL ì œê³µ
        download_url = f"/downloads/export_{uuid.uuid4().hex[:8]}.{format_type}"
        
        logger.info(f"[API] ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì™„ë£Œ - URL: {download_url}")
        return {
            "downloadUrl": download_url,
            "format": format_type,
            "itemCount": len(item_ids),
            "message": "ë‚´ë³´ë‚´ê¸°ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤"
        }
        
    except Exception as e:
        logger.error(f"[API] ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {str(e)}")

@router.post("/data/bulk/{action}")
async def handle_bulk_data_action(action: str, bulk_request: Dict[str, Any]):
    """ğŸ”„ ë°ì´í„° ì¼ê´„ ì‘ì—… ì²˜ë¦¬"""
    try:
        item_ids = bulk_request.get("itemIds", [])
        logger.info(f"[API] ì¼ê´„ {action} ìš”ì²­ - {len(item_ids)}ê°œ í•­ëª©")
        
        affected_count = 0
        
        if action == "delete":
            # ì‚­ì œ ì²˜ë¦¬
            global data_items_store
            data_items_store = [item for item in data_items_store if item.id not in item_ids]
            affected_count = len(item_ids)
        elif action == "reprocess":
            # ì¬ì²˜ë¦¬ ì²˜ë¦¬
            for item in data_items_store:
                if item.id in item_ids:
                    item.processing_status = "pending"
                    affected_count += 1
        elif action == "analyze":
            # AI ë¶„ì„ ì²˜ë¦¬
            for item in data_items_store:
                if item.id in item_ids:
                    item.ai_analyzed = True
                    affected_count += 1
        else:
            raise HTTPException(status_code=400, detail=f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì•¡ì…˜: {action}")
        
        logger.info(f"[API] ì¼ê´„ {action} ì™„ë£Œ - {affected_count}ê°œ í•­ëª© ì²˜ë¦¬ë¨")
        return {
            "success": True,
            "affected": affected_count,
            "message": f"{affected_count}ê°œ í•­ëª©ì— ëŒ€í•œ {action} ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[API] ì¼ê´„ ì‘ì—… ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì¼ê´„ ì‘ì—… ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

# ================================
# WebSocket ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
# ================================

@router.websocket("/ws/monitor")
async def websocket_endpoint(websocket: WebSocket):
    """ğŸ”´ ì‹¤ì‹œê°„ í¬ë¡¤ë§ ìƒíƒœ WebSocket"""
    await websocket.accept()
    logger.info("[WS] WebSocket ì—°ê²° ìˆ˜ë½ë¨")
    
    try:
        while True:
            # ì£¼ê¸°ì ìœ¼ë¡œ ìƒíƒœ ì—…ë°ì´íŠ¸ ì „ì†¡ (5ì´ˆë§ˆë‹¤)
            await asyncio.sleep(5)
            
            update_data = {
                "type": "status_update",
                "timestamp": datetime.now().isoformat(),
                "active_crawlers": len([c for c in crawler_status_store if c.status == 'running']),
                "total_jobs": len(crawling_jobs_store),
                "data_items": len(data_items_store)
            }
            
            await websocket.send_text(json.dumps(update_data))
            
    except WebSocketDisconnect:
        logger.info("[WS] WebSocket ì—°ê²° ì¢…ë£Œë¨")
    except Exception as e:
        logger.error(f"[WS] WebSocket ì˜¤ë¥˜: {str(e)}")

logger.info("ğŸš€ í¬ë¡¤ë§ ëŒ€ì‹œë³´ë“œ API ë¼ìš°í„° ë¡œë“œ ì™„ë£Œ")