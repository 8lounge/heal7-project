#!/usr/bin/env python3
"""
üß† AI Í∏∞Î∞ò ÌÅ¨Î°§Îü¨ ÏÑ†ÌÉù ÏóîÏßÑ (Gemini 2.0 ÌÜµÌï©)
URL Î∂ÑÏÑùÏùÑ ÌÜµÌïú ÏµúÏ†Å ÌÅ¨Î°§ÎßÅ ÎèÑÍµ¨ ÏûêÎèô ÏÑ†ÌÉù

üéØ ÌïµÏã¨ Í∏∞Îä•:
- URL Ìå®ÌÑ¥ Î∞è ÏΩòÌÖêÏ∏† ÌÉÄÏûÖ AI Î∂ÑÏÑù  
- 3Îã®Í≥Ñ ÎèÑÍµ¨ Ï≤¥Í≥Ñ ÏµúÏ†ÅÌôî (httpx ‚Üí httpx+beautifulsoup ‚Üí playwright)
- Ïã§ÏãúÍ∞Ñ ÌíàÏßà ÌèâÍ∞Ä Î∞è ÎèÑÍµ¨ Ï†ÑÌôò
- ÌïôÏäµ Í∏∞Î∞ò ÏÑ±Îä• ÏµúÏ†ÅÌôî

Author: HEAL7 Development Team  
Version: 1.0.0 (Gemini 2.0 Integration)
Date: 2025-09-03
"""

import asyncio
import json
import logging
import time
import re
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
from urllib.parse import urlparse, urljoin

# AI Î∂ÑÏÑùÍ∏∞ import
try:
    from ..multimodal.ai_analyzer import MultimodalAnalyzer, AIModel
except ImportError:
    # Ìè¥Î∞± Íµ¨ÌòÑ
    class AIModel:
        GEMINI_FLASH = "gemini_flash"
    
    class MultimodalAnalyzer:
        async def analyze_text(self, text, model, analysis_type):
            return {"success": False, "error": "AI Î∂ÑÏÑùÍ∏∞ ÎØ∏ÏÑ§Ïπò"}

from .crawlers.base_crawler import CrawlerType

logger = logging.getLogger(__name__)


class CrawlComplexity(Enum):
    """ÌÅ¨Î°§ÎßÅ Î≥µÏû°ÎèÑ Î†àÎ≤®"""
    SIMPLE = "simple"       # API ÎòêÎäî Ï†ïÏ†Å HTML (httpx)
    MODERATE = "moderate"   # HTML ÌååÏã± ÌïÑÏöî (httpx + beautifulsoup)
    COMPLEX = "complex"     # JavaScript Î†åÎçîÎßÅ ÌïÑÏöî (playwright)


class AIConfidence(Enum):
    """AI Î∂ÑÏÑù Ïã†Î¢∞ÎèÑ"""
    HIGH = "high"       # 90%+ Ïã†Î¢∞ÎèÑ
    MEDIUM = "medium"   # 70-89% Ïã†Î¢∞ÎèÑ  
    LOW = "low"         # 50-69% Ïã†Î¢∞ÎèÑ
    FALLBACK = "fallback"  # 50% ÎØ∏Îßå, Í∏∞Î≥∏ Í∑úÏπô ÏÇ¨Ïö©


@dataclass
class CrawlerRecommendation:
    """ÌÅ¨Î°§Îü¨ Ï∂îÏ≤ú Í≤∞Í≥º"""
    primary_crawler: CrawlerType
    fallback_crawler: CrawlerType
    complexity_level: CrawlComplexity
    confidence_score: float
    reasoning: str
    analysis_time_ms: float
    ai_analysis: Dict[str, Any] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "primary_crawler": self.primary_crawler.value,
            "fallback_crawler": self.fallback_crawler.value,
            "complexity_level": self.complexity_level.value,
            "confidence_score": self.confidence_score,
            "reasoning": self.reasoning,
            "analysis_time_ms": self.analysis_time_ms,
            "ai_analysis": self.ai_analysis
        }


@dataclass
class URLAnalysisContext:
    """URL Î∂ÑÏÑù Ïª®ÌÖçÏä§Ìä∏"""
    url: str
    domain: str
    path: str
    params: str
    initial_content: Optional[str] = None
    response_headers: Optional[Dict[str, str]] = None
    status_code: Optional[int] = None


class AICrawlerSelector:
    """üß† AI Í∏∞Î∞ò ÌÅ¨Î°§Îü¨ ÏÑ†ÌÉù ÏóîÏßÑ"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.AICrawlerSelector")
        self.ai_analyzer = None
        self._initialize_ai()
        
        # ÏÑ±Îä• ÌÜµÍ≥Ñ
        self.selection_stats = {
            "total_selections": 0,
            "ai_selections": 0,
            "fallback_selections": 0,
            "accuracy_scores": []
        }
        
        # ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ (ÎèÑÎ©îÏù∏Î≥Ñ ÏÑ±Í≥µ Ìå®ÌÑ¥)
        self.learning_patterns = {}
    
    def _initialize_ai(self):
        """AI Î∂ÑÏÑùÍ∏∞ Ï¥àÍ∏∞Ìôî"""
        try:
            self.ai_analyzer = MultimodalAnalyzer()
            self.logger.info("‚úÖ AI ÌÅ¨Î°§Îü¨ ÏÑ†ÌÉùÍ∏∞ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å (Gemini 2.0)")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è AI Î∂ÑÏÑùÍ∏∞ Ï¥àÍ∏∞Ìôî Ïã§Ìå®, Ìè¥Î∞± Î™®Îìú: {e}")
            self.ai_analyzer = None
    
    async def select_optimal_crawler(
        self, 
        url: str, 
        context: Optional[URLAnalysisContext] = None,
        use_ai: bool = True
    ) -> CrawlerRecommendation:
        """ÏµúÏ†Å ÌÅ¨Î°§Îü¨ ÏÑ†ÌÉù (AI + Ìú¥Î¶¨Ïä§Ìã± ÌïòÏù¥Î∏åÎ¶¨Îìú)"""
        start_time = time.time()
        
        try:
            # 1Îã®Í≥Ñ: URL Í∏∞Î≥∏ Î∂ÑÏÑù
            url_context = context or self._create_url_context(url)
            
            # 2Îã®Í≥Ñ: AI Î∂ÑÏÑù (ÌôúÏÑ±ÌôîÎêú Í≤ΩÏö∞)
            ai_result = None
            if use_ai and self.ai_analyzer:
                ai_result = await self._ai_url_analysis(url_context)
            
            # 3Îã®Í≥Ñ: ÏµúÏ¢Ö Ï∂îÏ≤ú Í≤∞Ï†ï
            recommendation = await self._make_final_recommendation(
                url_context, ai_result
            )
            
            recommendation.analysis_time_ms = (time.time() - start_time) * 1000
            
            # 4Îã®Í≥Ñ: ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏
            self._update_selection_stats(recommendation, ai_result is not None)
            
            self.logger.info(
                f"üéØ ÌÅ¨Î°§Îü¨ ÏÑ†ÌÉù ÏôÑÎ£å: {recommendation.primary_crawler.value} "
                f"(Ïã†Î¢∞ÎèÑ: {recommendation.confidence_score:.1f}%, "
                f"Î∂ÑÏÑùÏãúÍ∞Ñ: {recommendation.analysis_time_ms:.0f}ms)"
            )
            
            return recommendation
            
        except Exception as e:
            self.logger.error(f"‚ùå ÌÅ¨Î°§Îü¨ ÏÑ†ÌÉù Ïã§Ìå®: {e}")
            return self._create_fallback_recommendation(url, str(e))
    
    def _create_url_context(self, url: str) -> URLAnalysisContext:
        """URL Ïª®ÌÖçÏä§Ìä∏ ÏÉùÏÑ±"""
        parsed = urlparse(url)
        return URLAnalysisContext(
            url=url,
            domain=parsed.netloc,
            path=parsed.path,
            params=parsed.query
        )
    
    async def _ai_url_analysis(self, context: URLAnalysisContext) -> Optional[Dict[str, Any]]:
        """AI Í∏∞Î∞ò URL Î∂ÑÏÑù"""
        try:
            # Gemini Flash Î™®Îç∏Î°ú URL Ìå®ÌÑ¥ Î∂ÑÏÑù
            analysis_prompt = f"""
Ïõπ ÌÅ¨Î°§ÎßÅÏùÑ ÏúÑÌïú URL Î∂ÑÏÑùÏùÑ Ìï¥Ï£ºÏÑ∏Ïöî. 3Îã®Í≥Ñ ÎèÑÍµ¨ Ï≤¥Í≥Ñ Ï§ë ÏµúÏ†Å ÏÑ†ÌÉùÏùÑ ÏúÑÌï¥:

üéØ Î∂ÑÏÑù ÎåÄÏÉÅ URL: {context.url}
- ÎèÑÎ©îÏù∏: {context.domain} 
- Í≤ΩÎ°ú: {context.path}
- ÌååÎùºÎØ∏ÌÑ∞: {context.params}

üîç ÌåêÎã® Í∏∞Ï§Ä:
1. HTTPX (1Îã®Í≥Ñ): API ÏóîÎìúÌè¨Ïù∏Ìä∏, Ï†ïÏ†Å ÌååÏùº, Îã®Ïàú HTML
2. HTTPX + BeautifulSoup (2Îã®Í≥Ñ): HTML ÌååÏã± ÌïÑÏöî, Ï†ïÏ†Å ÏΩòÌÖêÏ∏†
3. Playwright (3Îã®Í≥Ñ): JavaScript Î†åÎçîÎßÅ, ÎèôÏ†Å ÏΩòÌÖêÏ∏†, Î≥µÏû°Ìïú ÏÉÅÌò∏ÏûëÏö©

üí° Î∂ÑÏÑù ÏöîÏ≤≠:
- complexity_level: "simple", "moderate", "complex" Ï§ë ÏÑ†ÌÉù
- confidence_score: 0-100 Ï†êÏàò 
- reasoning: ÏÑ†ÌÉù Ïù¥Ïú† (Ìïú Î¨∏Ïû•)
- requires_js: JavaScript ÌïÑÏöî Ïó¨Î∂Ä (boolean)
- is_api: API ÏóîÎìúÌè¨Ïù∏Ìä∏ Ïó¨Î∂Ä (boolean)
- content_type_prediction: ÏòàÏÉÅ ÏΩòÌÖêÏ∏† ÌÉÄÏûÖ

JSON ÌòïÌÉúÎ°ú ÏùëÎãµÌï¥Ï£ºÏÑ∏Ïöî.
"""
            
            result = await self.ai_analyzer.analyze_text(
                text=analysis_prompt,
                model=AIModel.GEMINI_FLASH,
                analysis_type="url_crawler_selection"
            )
            
            if result and result.get('success'):
                content = result.get('content', {})
                # JSON ÌååÏã± ÏãúÎèÑ
                if isinstance(content, str):
                    try:
                        content = json.loads(content)
                    except json.JSONDecodeError:
                        # JSONÏù¥ ÏïÑÎãå Í≤ΩÏö∞ ÌÇ§ÏõåÎìú Ï∂îÏ∂ú
                        content = self._extract_analysis_keywords(content)
                
                self.logger.debug(f"ü§ñ AI Î∂ÑÏÑù Í≤∞Í≥º: {content}")
                return content
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è AI Î∂ÑÏÑù Ïã§Ìå®: {e}")
        
        return None
    
    def _extract_analysis_keywords(self, text: str) -> Dict[str, Any]:
        """ÌÖçÏä§Ìä∏ÏóêÏÑú Î∂ÑÏÑù ÌÇ§ÏõåÎìú Ï∂îÏ∂ú"""
        analysis = {
            "complexity_level": "moderate",
            "confidence_score": 60.0,
            "reasoning": "ÌÇ§ÏõåÎìú Í∏∞Î∞ò Î∂ÑÏÑù",
            "requires_js": False,
            "is_api": False
        }
        
        text_lower = text.lower()
        
        # Î≥µÏû°ÎèÑ ÌåêÎã®
        if any(word in text_lower for word in ['api', 'json', 'simple', 'static']):
            analysis["complexity_level"] = "simple"
            analysis["confidence_score"] = 75.0
        elif any(word in text_lower for word in ['javascript', 'dynamic', 'complex', 'spa']):
            analysis["complexity_level"] = "complex"
            analysis["requires_js"] = True
            analysis["confidence_score"] = 80.0
        
        # API ÌåêÎã®
        if any(word in text_lower for word in ['api', 'rest', 'json', 'graphql']):
            analysis["is_api"] = True
            
        return analysis
    
    async def _make_final_recommendation(
        self, 
        context: URLAnalysisContext,
        ai_result: Optional[Dict[str, Any]]
    ) -> CrawlerRecommendation:
        """ÏµúÏ¢Ö ÌÅ¨Î°§Îü¨ Ï∂îÏ≤ú Í≤∞Ï†ï"""
        
        # AI Í≤∞Í≥ºÍ∞Ä ÏûàÎäî Í≤ΩÏö∞
        if ai_result:
            return self._ai_based_recommendation(context, ai_result)
        
        # Ìú¥Î¶¨Ïä§Ìã± Í∏∞Î∞ò Ìè¥Î∞±
        return self._heuristic_based_recommendation(context)
    
    def _ai_based_recommendation(
        self, 
        context: URLAnalysisContext, 
        ai_result: Dict[str, Any]
    ) -> CrawlerRecommendation:
        """AI Í≤∞Í≥º Í∏∞Î∞ò Ï∂îÏ≤ú"""
        
        complexity = ai_result.get("complexity_level", "moderate")
        confidence = ai_result.get("confidence_score", 60.0)
        reasoning = ai_result.get("reasoning", "AI Í∏∞Î∞ò Î∂ÑÏÑù")
        requires_js = ai_result.get("requires_js", False)
        is_api = ai_result.get("is_api", False)
        
        # ÌÅ¨Î°§Îü¨ ÏÑ†ÌÉù Î°úÏßÅ
        if is_api or complexity == "simple":
            primary = CrawlerType.HTTPX
            fallback = CrawlerType.PLAYWRIGHT
            complexity_level = CrawlComplexity.SIMPLE
        elif requires_js or complexity == "complex":
            primary = CrawlerType.PLAYWRIGHT
            fallback = CrawlerType.HTTPX
            complexity_level = CrawlComplexity.COMPLEX
        else:  # moderate
            primary = CrawlerType.HTTPX  # BeautifulSoupÎäî HTTPXÏóê Ìè¨Ìï®Îê®
            fallback = CrawlerType.PLAYWRIGHT
            complexity_level = CrawlComplexity.MODERATE
        
        return CrawlerRecommendation(
            primary_crawler=primary,
            fallback_crawler=fallback,
            complexity_level=complexity_level,
            confidence_score=confidence,
            reasoning=f"AI Î∂ÑÏÑù: {reasoning}",
            analysis_time_ms=0,  # ÎÇòÏ§ëÏóê ÏÑ§Ï†ïÎê®
            ai_analysis=ai_result
        )
    
    def _heuristic_based_recommendation(
        self, 
        context: URLAnalysisContext
    ) -> CrawlerRecommendation:
        """Ìú¥Î¶¨Ïä§Ìã± Í∏∞Î∞ò Ï∂îÏ≤ú (AI Ìè¥Î∞±)"""
        
        url_lower = context.url.lower()
        domain_lower = context.domain.lower()
        path_lower = context.path.lower()
        
        # API Ìå®ÌÑ¥ Í∞êÏßÄ
        if any(pattern in url_lower for pattern in [
            'api', '.json', '/rest/', '/graphql', '/v1/', '/v2/'
        ]):
            return CrawlerRecommendation(
                primary_crawler=CrawlerType.HTTPX,
                fallback_crawler=CrawlerType.PLAYWRIGHT,
                complexity_level=CrawlComplexity.SIMPLE,
                confidence_score=85.0,
                reasoning="API ÏóîÎìúÌè¨Ïù∏Ìä∏ Ìå®ÌÑ¥ Í∞êÏßÄ",
                analysis_time_ms=0
            )
        
        # Ï†ïÎ∂Ä ÏÇ¨Ïù¥Ìä∏ (Î≥µÏû°Ìï®)
        if '.go.kr' in domain_lower or '.gov' in domain_lower:
            return CrawlerRecommendation(
                primary_crawler=CrawlerType.PLAYWRIGHT,
                fallback_crawler=CrawlerType.HTTPX,
                complexity_level=CrawlComplexity.COMPLEX,
                confidence_score=80.0,
                reasoning="Ï†ïÎ∂Ä ÏÇ¨Ïù¥Ìä∏ - JavaScript Î†åÎçîÎßÅ ÌïÑÏöî",
                analysis_time_ms=0
            )
        
        # ÎèôÏ†Å ÏΩòÌÖêÏ∏† Ìå®ÌÑ¥
        if any(pattern in url_lower for pattern in [
            'spa', 'app', 'react', 'vue', 'angular', 'ajax'
        ]):
            return CrawlerRecommendation(
                primary_crawler=CrawlerType.PLAYWRIGHT,
                fallback_crawler=CrawlerType.HTTPX,
                complexity_level=CrawlComplexity.COMPLEX,
                confidence_score=75.0,
                reasoning="ÎèôÏ†Å Ïõπ Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò Ìå®ÌÑ¥ Í∞êÏßÄ",
                analysis_time_ms=0
            )
        
        # Í∏∞Î≥∏Í∞í: Ï§ëÍ∞Ñ Î≥µÏû°ÎèÑ
        return CrawlerRecommendation(
            primary_crawler=CrawlerType.HTTPX,  # BeautifulSoup Ìè¨Ìï®
            fallback_crawler=CrawlerType.PLAYWRIGHT,
            complexity_level=CrawlComplexity.MODERATE,
            confidence_score=65.0,
            reasoning="Ìú¥Î¶¨Ïä§Ìã± Í∏∞Î≥∏ Î∂ÑÏÑù - HTML ÌååÏã± Ï§ëÏã¨",
            analysis_time_ms=0
        )
    
    def _create_fallback_recommendation(self, url: str, error: str) -> CrawlerRecommendation:
        """ÏóêÎü¨ Ïãú Ìè¥Î∞± Ï∂îÏ≤ú"""
        return CrawlerRecommendation(
            primary_crawler=CrawlerType.HTTPX,
            fallback_crawler=CrawlerType.PLAYWRIGHT,
            complexity_level=CrawlComplexity.MODERATE,
            confidence_score=30.0,
            reasoning=f"Î∂ÑÏÑù Ïò§Î•òÎ°ú Ïù∏Ìïú Í∏∞Î≥∏Í∞í Ï†ÅÏö©: {error}",
            analysis_time_ms=0
        )
    
    def _update_selection_stats(self, recommendation: CrawlerRecommendation, used_ai: bool):
        """ÏÑ†ÌÉù ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏"""
        self.selection_stats["total_selections"] += 1
        if used_ai:
            self.selection_stats["ai_selections"] += 1
        else:
            self.selection_stats["fallback_selections"] += 1
        
        self.selection_stats["accuracy_scores"].append(recommendation.confidence_score)
        
        # ÏµúÍ∑º 100Í∞ú Ï†êÏàòÎßå Ïú†ÏßÄ
        if len(self.selection_stats["accuracy_scores"]) > 100:
            self.selection_stats["accuracy_scores"] = self.selection_stats["accuracy_scores"][-100:]
    
    async def evaluate_crawler_performance(
        self,
        url: str,
        recommended_crawler: CrawlerType,
        actual_result: Dict[str, Any]
    ) -> float:
        """ÌÅ¨Î°§Îü¨ ÏÑ±Îä• ÌèâÍ∞Ä Î∞è ÌïôÏäµ"""
        try:
            success = actual_result.get('success', False)
            response_time = actual_result.get('response_time', 0)
            content_length = len(actual_result.get('html', ''))
            
            # ÏÑ±Îä• Ï†êÏàò Í≥ÑÏÇ∞ (0-100)
            score = 0.0
            if success:
                score += 50.0  # Í∏∞Î≥∏ ÏÑ±Í≥µ Ï†êÏàò
                
                # ÏùëÎãµ ÏãúÍ∞Ñ ÌèâÍ∞Ä (Îπ†Î•ºÏàòÎ°ù Ï¢ãÏùå)
                if response_time < 2.0:
                    score += 25.0
                elif response_time < 5.0:
                    score += 15.0
                elif response_time < 10.0:
                    score += 5.0
                
                # ÏΩòÌÖêÏ∏† ÌíàÏßà ÌèâÍ∞Ä
                if content_length > 1000:
                    score += 25.0
                elif content_length > 100:
                    score += 15.0
                elif content_length > 0:
                    score += 5.0
            
            # ÎèÑÎ©îÏù∏Î≥Ñ ÌïôÏäµ Ìå®ÌÑ¥ ÏóÖÎç∞Ïù¥Ìä∏
            domain = urlparse(url).netloc
            if domain not in self.learning_patterns:
                self.learning_patterns[domain] = {
                    'crawler_scores': {},
                    'total_attempts': 0,
                    'successful_attempts': 0
                }
            
            pattern = self.learning_patterns[domain]
            crawler_key = recommended_crawler.value
            
            if crawler_key not in pattern['crawler_scores']:
                pattern['crawler_scores'][crawler_key] = []
            
            pattern['crawler_scores'][crawler_key].append(score)
            pattern['total_attempts'] += 1
            if success:
                pattern['successful_attempts'] += 1
            
            # ÏµúÍ∑º 10Í∞ú Ï†êÏàòÎßå Ïú†ÏßÄ
            if len(pattern['crawler_scores'][crawler_key]) > 10:
                pattern['crawler_scores'][crawler_key] = pattern['crawler_scores'][crawler_key][-10:]
            
            self.logger.debug(f"üìä ÌÅ¨Î°§Îü¨ ÏÑ±Îä• ÌèâÍ∞Ä: {crawler_key} ‚Üí {score:.1f}Ï†ê")
            return score
            
        except Exception as e:
            self.logger.error(f"‚ùå ÏÑ±Îä• ÌèâÍ∞Ä Ïã§Ìå®: {e}")
            return 0.0
    
    def get_selection_stats(self) -> Dict[str, Any]:
        """ÏÑ†ÌÉù ÌÜµÍ≥Ñ Ï°∞Ìöå"""
        accuracy_scores = self.selection_stats["accuracy_scores"]
        avg_accuracy = sum(accuracy_scores) / len(accuracy_scores) if accuracy_scores else 0.0
        
        return {
            "total_selections": self.selection_stats["total_selections"],
            "ai_selections": self.selection_stats["ai_selections"],
            "fallback_selections": self.selection_stats["fallback_selections"],
            "ai_usage_rate": (
                self.selection_stats["ai_selections"] / max(1, self.selection_stats["total_selections"]) * 100
            ),
            "average_accuracy": avg_accuracy,
            "learning_domains": len(self.learning_patterns),
            "ai_analyzer_status": "active" if self.ai_analyzer else "inactive"
        }


# Ï†ÑÏó≠ Ïù∏Ïä§ÌÑ¥Ïä§
_ai_selector = None

async def get_ai_crawler_selector() -> AICrawlerSelector:
    """AI ÌÅ¨Î°§Îü¨ ÏÑ†ÌÉùÍ∏∞ Ïù∏Ïä§ÌÑ¥Ïä§ Ï°∞Ìöå"""
    global _ai_selector
    if _ai_selector is None:
        _ai_selector = AICrawlerSelector()
    return _ai_selector


# Ìé∏Ïùò Ìï®ÏàòÎì§
async def select_crawler_for_url(url: str, use_ai: bool = True) -> CrawlerRecommendation:
    """URLÏóê ÏµúÏ†ÅÌôîÎêú ÌÅ¨Î°§Îü¨ ÏÑ†ÌÉù"""
    selector = await get_ai_crawler_selector()
    return await selector.select_optimal_crawler(url, use_ai=use_ai)


async def evaluate_crawl_result(
    url: str, 
    crawler: CrawlerType, 
    result: Dict[str, Any]
) -> float:
    """ÌÅ¨Î°§ÎßÅ Í≤∞Í≥º ÌèâÍ∞Ä Î∞è ÌïôÏäµ"""
    selector = await get_ai_crawler_selector()
    return await selector.evaluate_crawler_performance(url, crawler, result)