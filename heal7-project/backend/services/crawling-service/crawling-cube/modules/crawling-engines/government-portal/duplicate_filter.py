#!/usr/bin/env python3
"""
ì¤‘ë³µ ë°ì´í„° í•„í„°ë§ ì‹œìŠ¤í…œ
í•´ì‹œ ê¸°ë°˜ + ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ê²€ì¶œ
"""

import hashlib
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Set, Tuple, Optional
from dataclasses import dataclass, field
from difflib import SequenceMatcher
import re

logger = logging.getLogger(__name__)

@dataclass
class DuplicateDetectionResult:
    """ì¤‘ë³µ ê²€ì¶œ ê²°ê³¼"""
    is_duplicate: bool
    confidence_score: float  # 0.0-1.0
    duplicate_type: str  # 'exact', 'hash', 'content_similarity', 'title_similarity'
    original_record_id: Optional[str] = None
    similarity_details: Dict = field(default_factory=dict)

class AdvancedDuplicateFilter:
    """ê³ ê¸‰ ì¤‘ë³µ ë°ì´í„° í•„í„°"""
    
    def __init__(self):
        # í•´ì‹œ ìºì‹œ (ë©”ëª¨ë¦¬ ê¸°ë°˜)
        self.content_hashes: Set[str] = set()
        self.title_hashes: Set[str] = set()
        
        # ìœ ì‚¬ë„ ê²€ì‚¬ ì„¤ì •
        self.title_similarity_threshold = 0.85
        self.content_similarity_threshold = 0.80
        self.url_similarity_threshold = 0.95
        
        # ì •ê·œí™” íŒ¨í„´
        self.normalization_patterns = [
            (r'\s+', ' '),  # ë‹¤ì¤‘ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ
            (r'[^\w\sê°€-í£]', ''),  # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ)
            (r'^\s+|\s+$', ''),  # ì•ë’¤ ê³µë°± ì œê±°
        ]
        
        # ë¬´ì‹œí•  ë‹¨ì–´ë“¤ (ì¤‘ë³µ íŒì • ì‹œ ì œì™¸)
        self.stop_words = {
            'ê³µê³ ', 'ì‚¬ì—…', 'ì§€ì›', 'ì‹ ì²­', 'ëª¨ì§‘', 'ì•ˆë‚´', 'ê³µì§€',
            'ë…„ë„', 'ì›”', 'ì¼', 'ì‹œí–‰', 'ì ‘ìˆ˜', 'ë§ˆê°', 'ì¢…ë£Œ'
        }
        
        logger.info("ğŸ” ê³ ê¸‰ ì¤‘ë³µ ë°ì´í„° í•„í„° ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def detect_duplicate(self, new_record: Dict, existing_records: List[Dict] = None) -> DuplicateDetectionResult:
        """
        ì¢…í•©ì ì¸ ì¤‘ë³µ ê²€ì¶œ
        
        Args:
            new_record: ê²€ì‚¬í•  ì‹ ê·œ ë ˆì½”ë“œ
            existing_records: ê¸°ì¡´ ë ˆì½”ë“œë“¤ (ì„ íƒì )
        
        Returns:
            ì¤‘ë³µ ê²€ì¶œ ê²°ê³¼
        """
        # 1ë‹¨ê³„: ì™„ì „ ì¼ì¹˜ í•´ì‹œ ê²€ì‚¬
        exact_hash_result = await self._check_exact_hash_duplicate(new_record)
        if exact_hash_result.is_duplicate:
            return exact_hash_result
        
        # 2ë‹¨ê³„: ì œëª© í•´ì‹œ ê²€ì‚¬
        title_hash_result = await self._check_title_hash_duplicate(new_record)
        if title_hash_result.is_duplicate:
            return title_hash_result
        
        # 3ë‹¨ê³„: ë‚´ìš© ìœ ì‚¬ë„ ê²€ì‚¬
        if existing_records:
            similarity_result = await self._check_content_similarity_duplicate(new_record, existing_records)
            if similarity_result.is_duplicate:
                return similarity_result
        
        # 4ë‹¨ê³„: URL ìœ ì‚¬ë„ ê²€ì‚¬
        if existing_records:
            url_similarity_result = await self._check_url_similarity_duplicate(new_record, existing_records)
            if url_similarity_result.is_duplicate:
                return url_similarity_result
        
        # ì¤‘ë³µì´ ì•„ë‹˜
        return DuplicateDetectionResult(
            is_duplicate=False,
            confidence_score=0.0,
            duplicate_type='none'
        )
    
    async def _check_exact_hash_duplicate(self, record: Dict) -> DuplicateDetectionResult:
        """ì™„ì „ ì¼ì¹˜ í•´ì‹œ ì¤‘ë³µ ê²€ì‚¬"""
        content_hash = self._generate_content_hash(record)
        
        if content_hash in self.content_hashes:
            return DuplicateDetectionResult(
                is_duplicate=True,
                confidence_score=1.0,
                duplicate_type='exact',
                similarity_details={'content_hash': content_hash}
            )
        
        # ìƒˆë¡œìš´ í•´ì‹œ ë“±ë¡
        self.content_hashes.add(content_hash)
        
        return DuplicateDetectionResult(
            is_duplicate=False,
            confidence_score=0.0,
            duplicate_type='none'
        )
    
    async def _check_title_hash_duplicate(self, record: Dict) -> DuplicateDetectionResult:
        """ì œëª© í•´ì‹œ ì¤‘ë³µ ê²€ì‚¬"""
        title = record.get('title', '')
        if not title:
            return DuplicateDetectionResult(is_duplicate=False, confidence_score=0.0, duplicate_type='none')
        
        normalized_title = self._normalize_text(title)
        title_hash = hashlib.sha256(normalized_title.encode()).hexdigest()[:16]
        
        if title_hash in self.title_hashes:
            return DuplicateDetectionResult(
                is_duplicate=True,
                confidence_score=0.9,
                duplicate_type='title_hash',
                similarity_details={
                    'title_hash': title_hash,
                    'normalized_title': normalized_title
                }
            )
        
        # ìƒˆë¡œìš´ ì œëª© í•´ì‹œ ë“±ë¡
        self.title_hashes.add(title_hash)
        
        return DuplicateDetectionResult(
            is_duplicate=False,
            confidence_score=0.0,
            duplicate_type='none'
        )
    
    async def _check_content_similarity_duplicate(self, new_record: Dict, existing_records: List[Dict]) -> DuplicateDetectionResult:
        """ë‚´ìš© ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ê²€ì‚¬"""
        new_title = self._normalize_text(new_record.get('title', ''))
        new_content = self._normalize_text(new_record.get('description', ''))
        
        if not new_title:
            return DuplicateDetectionResult(is_duplicate=False, confidence_score=0.0, duplicate_type='none')
        
        max_similarity = 0.0
        best_match_record = None
        
        for existing_record in existing_records:
            existing_title = self._normalize_text(existing_record.get('title', ''))
            existing_content = self._normalize_text(existing_record.get('description', ''))
            
            # ì œëª© ìœ ì‚¬ë„
            title_similarity = SequenceMatcher(None, new_title, existing_title).ratio()
            
            # ë‚´ìš© ìœ ì‚¬ë„ (ìˆëŠ” ê²½ìš°)
            content_similarity = 0.0
            if new_content and existing_content:
                content_similarity = SequenceMatcher(None, new_content, existing_content).ratio()
            
            # ì¢…í•© ìœ ì‚¬ë„ (ì œëª© ê°€ì¤‘ì¹˜ 70%, ë‚´ìš© 30%)
            overall_similarity = title_similarity * 0.7 + content_similarity * 0.3
            
            if overall_similarity > max_similarity:
                max_similarity = overall_similarity
                best_match_record = existing_record
        
        # ì„ê³„ê°’ ì´ìƒì´ë©´ ì¤‘ë³µìœ¼ë¡œ íŒì •
        if max_similarity >= self.content_similarity_threshold:
            return DuplicateDetectionResult(
                is_duplicate=True,
                confidence_score=max_similarity,
                duplicate_type='content_similarity',
                original_record_id=best_match_record.get('id') if best_match_record else None,
                similarity_details={
                    'similarity_score': max_similarity,
                    'threshold': self.content_similarity_threshold,
                    'matching_title': best_match_record.get('title') if best_match_record else None
                }
            )
        
        return DuplicateDetectionResult(
            is_duplicate=False,
            confidence_score=max_similarity,
            duplicate_type='none'
        )
    
    async def _check_url_similarity_duplicate(self, new_record: Dict, existing_records: List[Dict]) -> DuplicateDetectionResult:
        """URL ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ê²€ì‚¬"""
        new_url = new_record.get('detail_url', '')
        if not new_url:
            return DuplicateDetectionResult(is_duplicate=False, confidence_score=0.0, duplicate_type='none')
        
        normalized_new_url = self._normalize_url(new_url)
        
        for existing_record in existing_records:
            existing_url = existing_record.get('detail_url', '')
            if not existing_url:
                continue
            
            normalized_existing_url = self._normalize_url(existing_url)
            url_similarity = SequenceMatcher(None, normalized_new_url, normalized_existing_url).ratio()
            
            if url_similarity >= self.url_similarity_threshold:
                return DuplicateDetectionResult(
                    is_duplicate=True,
                    confidence_score=url_similarity,
                    duplicate_type='url_similarity',
                    original_record_id=existing_record.get('id'),
                    similarity_details={
                        'url_similarity': url_similarity,
                        'new_url': normalized_new_url,
                        'existing_url': normalized_existing_url
                    }
                )
        
        return DuplicateDetectionResult(
            is_duplicate=False,
            confidence_score=0.0,
            duplicate_type='none'
        )
    
    def _generate_content_hash(self, record: Dict) -> str:
        """ë ˆì½”ë“œì˜ ì¢…í•© í•´ì‹œ ìƒì„±"""
        # í•µì‹¬ í•„ë“œë“¤ì„ ì¡°í•©í•´ì„œ í•´ì‹œ ìƒì„±
        key_fields = ['title', 'implementing_agency', 'agency', 'application_period', 'detail_url']
        content_parts = []
        
        for field in key_fields:
            value = record.get(field, '')
            if value and value != 'N/A':
                content_parts.append(self._normalize_text(str(value)))
        
        combined_content = '|'.join(content_parts)
        return hashlib.sha256(combined_content.encode()).hexdigest()[:32]
    
    def _normalize_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ê·œí™”"""
        if not text:
            return ''
        
        text = str(text).lower()
        
        # ì •ê·œí™” íŒ¨í„´ ì ìš©
        for pattern, replacement in self.normalization_patterns:
            text = re.sub(pattern, replacement, text)
        
        # ë¶ˆìš©ì–´ ì œê±°
        words = text.split()
        filtered_words = [word for word in words if word not in self.stop_words]
        
        return ' '.join(filtered_words)
    
    def _normalize_url(self, url: str) -> str:
        """URL ì •ê·œí™”"""
        if not url:
            return ''
        
        # URLì—ì„œ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±°
        url = url.split('?')[0]
        url = url.split('#')[0]
        
        # í”„ë¡œí† ì½œ ì œê±°
        url = re.sub(r'^https?://', '', url)
        
        # ë ìŠ¬ë˜ì‹œ ì œê±°
        url = url.rstrip('/')
        
        return url.lower()
    
    def add_processed_record(self, record: Dict):
        """ì²˜ë¦¬ëœ ë ˆì½”ë“œë¥¼ ìºì‹œì— ì¶”ê°€"""
        content_hash = self._generate_content_hash(record)
        self.content_hashes.add(content_hash)
        
        title = record.get('title', '')
        if title:
            normalized_title = self._normalize_text(title)
            title_hash = hashlib.sha256(normalized_title.encode()).hexdigest()[:16]
            self.title_hashes.add(title_hash)
    
    def get_filter_statistics(self) -> Dict:
        """í•„í„° í†µê³„ ì¡°íšŒ"""
        return {
            'content_hashes_count': len(self.content_hashes),
            'title_hashes_count': len(self.title_hashes),
            'thresholds': {
                'title_similarity': self.title_similarity_threshold,
                'content_similarity': self.content_similarity_threshold,
                'url_similarity': self.url_similarity_threshold
            },
            'stop_words_count': len(self.stop_words)
        }
    
    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        self.content_hashes.clear()
        self.title_hashes.clear()
        logger.info("ğŸ§¹ ì¤‘ë³µ í•„í„° ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")

# ì „ì—­ ì¤‘ë³µ í•„í„° ì¸ìŠ¤í„´ìŠ¤
duplicate_filter = AdvancedDuplicateFilter()