# ğŸ‡°ğŸ‡· í•œêµ­ ì •ë¶€ ì§€ì›ì‚¬ì—… í¬í„¸ ë¶„ì„ ë³´ê³ ì„œ v2.0

> **í”„ë¡œì íŠ¸**: Government Portal Intelligence System - í•œêµ­ í¬í„¸ ë¶„ì„  
> **ë²„ì „**: v2.0.0  
> **ì‘ì„±ì¼**: 2025-08-23  
> **ëª©ì **: í•œêµ­ ì£¼ìš” ì •ë¶€ ì§€ì›ì‚¬ì—… í¬í„¸ êµ¬ì¡° ë¶„ì„ ë° ìŠ¤í¬ë˜í•‘ ì „ëµ ìˆ˜ë¦½  

---

## ğŸ¯ **ë¶„ì„ ëŒ€ìƒ í¬í„¸**

### ğŸ“Š **1ì°¨ íƒ€ê²Ÿ í¬í„¸ (ìš°ì„ ìˆœìœ„ High)**

| ìˆœìœ„ | í¬í„¸ëª… | URL | ì¤‘ìš”ë„ | ì—…ë°ì´íŠ¸ ë¹ˆë„ | ìŠ¤í¬ë˜í•‘ ë‚œì´ë„ |
|------|--------|-----|---------|---------------|----------------|
| 1 | ê¸°ì—…ë§ˆë‹¹ | bizinfo.go.kr | â­â­â­â­â­ | ë§¤ì¼ | ì¤‘ê°„ |
| 2 | K-Startup | k-startup.go.kr | â­â­â­â­â­ | ë§¤ì¼ | ë†’ìŒ |
| 3 | ì •ë¶€24 | gov.kr | â­â­â­â­ | ì£¼ê°„ | ë‚®ìŒ |
| 4 | ì˜¨ë¼ì¸ì •ì±…ì†Œí†µ | policy.go.kr | â­â­â­ | ì£¼ê°„ | ë‚®ìŒ |

### ğŸ“Š **2ì°¨ í™•ì¥ í¬í„¸ (ìš°ì„ ìˆœìœ„ Medium)**

| í¬í„¸ëª… | URL | ì „ë¬¸ë¶„ì•¼ | íŠ¹ì´ì‚¬í•­ |
|--------|-----|---------|----------|
| ì¤‘ì†Œê¸°ì—…ì§„í¥ê³µë‹¨ | kosmes.or.kr | ì¤‘ì†Œê¸°ì—… ì§€ì› | API ì œê³µ |
| ê¸°ìˆ ë³´ì¦ê¸°ê¸ˆ | kibo.or.kr | ê¸°ìˆ ê¸ˆìœµ | ë¡œê·¸ì¸ í•„ìš” |
| ì°½ì—…ë„· | changupnet.go.kr | ì°½ì—… ìƒíƒœê³„ | ì»¤ë®¤ë‹ˆí‹° ê¸°ëŠ¥ |
| ìˆ˜ì¶œë°”ìš°ì²˜ | export.go.kr | ìˆ˜ì¶œ ì§€ì› | ë‹¤êµ­ì–´ ì§€ì› |

---

## ğŸ” **í¬í„¸ë³„ ìƒì„¸ ë¶„ì„**

### ğŸ¢ **1. ê¸°ì—…ë§ˆë‹¹ (bizinfo.go.kr) ë¶„ì„**

#### **ğŸ“‹ ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„**
```yaml
ê¸°ë³¸ ì •ë³´:
  - ìš´ì˜ê¸°ê´€: ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€, ì¤‘ì†Œê¸°ì—…ì§„í¥ê³µë‹¨
  - ì£¼ìš” ê¸°ëŠ¥: ì •ë¶€ ì§€ì›ì‚¬ì—… í†µí•© ì•ˆë‚´
  - ì›” ë°©ë¬¸ì: ì•½ 200ë§Œëª…
  - ëª¨ë°”ì¼ ëŒ€ì‘: ë°˜ì‘í˜• ì›¹

ì£¼ìš” ì„¹ì…˜:
  ì§€ì›ì‚¬ì—…:
    - URL: /web/lay1/biz/PBIZ_0000000000000.do
    - ë¶„ë¥˜: ìê¸ˆ, íŒë¡œ, ì°½ì—…, ê¸°ìˆ ê°œë°œ, í•´ì™¸ì§„ì¶œ
    - ë°ì´í„° í˜•íƒœ: ë¦¬ìŠ¤íŠ¸ + ìƒì„¸í˜ì´ì§€
    
  ê³µëª¨ì‚¬ì—…:
    - URL: /web/lay1/contest/PCNT_0000000000000.do
    - íŠ¹ì§•: ë§ˆê°ì„ë°• í‘œì‹œ, ì‹ ì²­í˜„í™© ê³µê°œ
    - ì—…ë°ì´íŠ¸: ë§¤ì¼ ì˜¤ì „ 9ì‹œ

ê¸°ìˆ ì  íŠ¹ì§•:
  - í”„ë¡ íŠ¸ì—”ë“œ: JSP + JavaScript
  - ê²€ìƒ‰: SOLR ê¸°ë°˜ ì „ë¬¸ê²€ìƒ‰
  - ì„¸ì…˜ê´€ë¦¬: JSESSIONID ì¿ í‚¤
  - CSRF ë³´í˜¸: í† í° ê¸°ë°˜
```

#### **ğŸ•·ï¸ ìŠ¤í¬ë˜í•‘ ì „ëµ**
```python
class BizinfoScraper:
    """ê¸°ì—…ë§ˆë‹¹ ì „ìš© ì§€ëŠ¥í˜• ìŠ¤í¬ë˜í¼"""
    
    def __init__(self):
        self.base_url = 'https://www.bizinfo.go.kr'
        self.session = aiohttp.ClientSession()
        self.rate_limiter = RateLimiter(requests_per_minute=20)  # ì„œë²„ ë¶€í•˜ ê³ ë ¤
        
    async def scrape_support_programs(self, category: str = 'all') -> List[Dict]:
        """ì§€ì›ì‚¬ì—… ëª©ë¡ ìŠ¤í¬ë˜í•‘"""
        
        # 1. ë©”ì¸ ëª©ë¡ í˜ì´ì§€ ì ‘ê·¼
        list_url = f"{self.base_url}/web/lay1/biz/PBIZ_0000000000000.do"
        
        # 2. ì¹´í…Œê³ ë¦¬ë³„ í•„í„° ì ìš©
        params = {
            'searchCondition': 'TITLE',
            'searchKeyword': '',
            'bizTycd': category if category != 'all' else '',
            'pageIndex': 1
        }
        
        programs = []
        page = 1
        
        while True:
            params['pageIndex'] = page
            
            # Rate limiting ì ìš©
            await self.rate_limiter.acquire()
            
            async with self.session.get(list_url, params=params) as response:
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                # 3. í”„ë¡œê·¸ë¨ ëª©ë¡ ì¶”ì¶œ
                program_items = soup.select('.business-item, .biz-item')
                
                if not program_items:
                    break
                
                for item in program_items:
                    program_data = await self.extract_program_basic_info(item)
                    
                    # 4. ìƒì„¸ ì •ë³´ ì¶”ì¶œ
                    detail_info = await self.scrape_program_detail(program_data['detail_url'])
                    program_data.update(detail_info)
                    
                    programs.append(program_data)
            
            page += 1
            if page > 100:  # ì•ˆì „ì¥ì¹˜
                break
        
        return programs
    
    async def extract_program_basic_info(self, item_element) -> Dict:
        """ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ"""
        
        title = item_element.select_one('.title, h3')
        title = title.get_text().strip() if title else 'N/A'
        
        agency = item_element.select_one('.agency, .org')
        agency = agency.get_text().strip() if agency else 'N/A'
        
        period = item_element.select_one('.period, .date')
        period = period.get_text().strip() if period else 'N/A'
        
        detail_link = item_element.select_one('a[href]')
        detail_url = None
        if detail_link:
            href = detail_link.get('href')
            detail_url = urljoin(self.base_url, href)
        
        return {
            'title': title,
            'agency': agency,
            'application_period': period,
            'detail_url': detail_url,
            'scraped_at': datetime.now().isoformat()
        }
    
    async def scrape_program_detail(self, detail_url: str) -> Dict:
        """í”„ë¡œê·¸ë¨ ìƒì„¸ ì •ë³´ ìŠ¤í¬ë˜í•‘"""
        
        if not detail_url:
            return {}
        
        await self.rate_limiter.acquire()
        
        async with self.session.get(detail_url) as response:
            html = await response.text()
            soup = BeautifulSoup(html, 'html.parser')
            
            # ìƒì„¸ ì •ë³´ ì¶”ì¶œ
            details = {
                'program_id': self.extract_program_id(detail_url),
                'support_details': self.extract_support_details(soup),
                'target_audience': self.extract_target_audience(soup),
                'required_documents': self.extract_required_documents(soup),
                'evaluation_criteria': self.extract_evaluation_criteria(soup),
                'contact_info': self.extract_contact_info(soup),
                'attachments': self.extract_attachments(soup)
            }
            
            return details
    
    def extract_support_details(self, soup) -> Dict:
        """ì§€ì› ë‚´ìš© ì¶”ì¶œ"""
        
        support_section = soup.select_one('.support-content, .biz-content')
        if not support_section:
            return {}
        
        # AIë¥¼ ì‚¬ìš©í•œ êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ
        content_text = support_section.get_text(strip=True)
        
        # GPT-4oë¥¼ ì‚¬ìš©í•´ êµ¬ì¡°í™”
        extraction_prompt = f"""
        ë‹¤ìŒ ì§€ì›ì‚¬ì—… ë‚´ìš©ì—ì„œ í•µì‹¬ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”:
        
        ë‚´ìš©: {content_text}
        
        ì¶”ì¶œí•  ì •ë³´:
        - ì§€ì›ê¸ˆì•¡ (ê¸ˆì•¡ ë° ë‹¨ìœ„)
        - ì§€ì›ê¸°ê°„ (ê°œì›” ìˆ˜)
        - ì§€ì›ë°©ì‹ (ìœµì/ì§€ì›ê¸ˆ/ì„¸ì•¡ê³µì œ ë“±)
        - íŠ¹ë³„ì¡°ê±´ (ìˆë‹¤ë©´)
        """
        
        # ì‹¤ì œ ìš´ì˜ì‹œì—ëŠ” AI ëª¨ë¸ í˜¸ì¶œ
        return {
            'support_amount': 'TBD',
            'support_period': 'TBD',
            'support_type': 'TBD',
            'special_conditions': []
        }
```

### ğŸš€ **2. K-Startup (k-startup.go.kr) ë¶„ì„**

#### **ğŸ“‹ ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„**
```yaml
ê¸°ë³¸ ì •ë³´:
  - ìš´ì˜ê¸°ê´€: ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€
  - ì£¼ìš” ê¸°ëŠ¥: ì°½ì—… ì§€ì› ì¢…í•© í”Œë«í¼
  - íŠ¹ì§•: SPA(Single Page Application) êµ¬ì¡°
  - API: REST API ì¼ë¶€ ì œê³µ

ì£¼ìš” ì„¹ì…˜:
  ì‚¬ì—…ê³µê³ :
    - URL: /homepage/powerup/business/list.do
    - íŠ¹ì§•: Ajax ë¡œë”©, ë¬´í•œìŠ¤í¬ë¡¤
    - í•„í„°: ëª¨ì§‘ìƒíƒœ, ì§€ì›ë¶„ì•¼, ì§€ì—­
    
  ì°½ì—…êµìœ¡:
    - URL: /homepage/academy/education/list.do
    - íŠ¹ì§•: ì˜¨/ì˜¤í”„ë¼ì¸ êµ¬ë¶„
    - ì‹ ì²­í˜„í™©: ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸

ê¸°ìˆ ì  íŠ¹ì§•:
  - í”„ë¡ íŠ¸ì—”ë“œ: React.js + Redux
  - API: /api/v1/ ì—”ë“œí¬ì¸íŠ¸
  - ì¸ì¦: JWT í† í° (ì¼ë¶€ ê¸°ëŠ¥)
  - ë³´ì•ˆ: reCAPTCHA, Rate Limiting
```

#### **ğŸ•·ï¸ ìŠ¤í¬ë˜í•‘ ì „ëµ (ê³ ë‚œì´ë„)**
```python
class KStartupScraper:
    """K-Startup ì „ìš© SPA ìŠ¤í¬ë˜í¼"""
    
    def __init__(self):
        self.base_url = 'https://www.k-startup.go.kr'
        self.api_base = f"{self.base_url}/api/v1"
        self.browser = None  # Playwright ë¸Œë¼ìš°ì €
        
    async def initialize_browser(self):
        """ë¸Œë¼ìš°ì € ì´ˆê¸°í™” (SPA ëŒ€ì‘)"""
        from playwright.async_api import async_playwright
        
        p = await async_playwright().start()
        self.browser = await p.chromium.launch(headless=True)
        
        # User-Agent ë° ê¸°íƒ€ í—¤ë” ì„¤ì •
        context = await self.browser.new_context(
            user_agent='Mozilla/5.0 (compatible; PaperworkAI/1.0; +http://paperwork.heal7.com/bot)',
            viewport={'width': 1920, 'height': 1080}
        )
        
        return context
    
    async def scrape_business_announcements(self) -> List[Dict]:
        """ì‚¬ì—…ê³µê³  ìŠ¤í¬ë˜í•‘ (SPA ëŒ€ì‘)"""
        
        context = await self.initialize_browser()
        page = await context.new_page()
        
        # 1. í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
        await page.goto(f"{self.base_url}/homepage/powerup/business/list.do")
        await page.wait_for_load_state('networkidle')
        
        # 2. Ajax ë°ì´í„° ë¡œë”© ëŒ€ê¸°
        await page.wait_for_selector('.business-list, .announce-list', timeout=30000)
        
        announcements = []
        
        # 3. ë¬´í•œìŠ¤í¬ë¡¤ ì²˜ë¦¬
        while True:
            # í˜„ì¬ í˜ì´ì§€ì˜ í•­ëª©ë“¤ ì¶”ì¶œ
            items = await page.query_selector_all('.business-item, .announce-item')
            
            for item in items:
                announcement_data = await self.extract_announcement_data(item)
                if announcement_data not in announcements:  # ì¤‘ë³µ ì œê±°
                    announcements.append(announcement_data)
            
            # 4. ë” ë§ì€ í•­ëª© ë¡œë“œ ì‹œë„
            load_more_btn = await page.query_selector('.load-more, .btn-more')
            if load_more_btn:
                await load_more_btn.click()
                await page.wait_for_timeout(2000)  # ë¡œë”© ëŒ€ê¸°
            else:
                break
        
        await context.close()
        return announcements
    
    async def extract_announcement_data(self, item_element) -> Dict:
        """ê³µê³  ë°ì´í„° ì¶”ì¶œ"""
        
        title = await item_element.query_selector('.title, h3')
        title_text = await title.inner_text() if title else 'N/A'
        
        status = await item_element.query_selector('.status, .state')
        status_text = await status.inner_text() if status else 'N/A'
        
        period = await item_element.query_selector('.period, .date')
        period_text = await period.inner_text() if period else 'N/A'
        
        # ìƒì„¸ ë§í¬ ì¶”ì¶œ
        link = await item_element.query_selector('a[href]')
        detail_url = None
        if link:
            href = await link.get_attribute('href')
            detail_url = urljoin(self.base_url, href)
        
        return {
            'title': title_text.strip(),
            'status': status_text.strip(),
            'period': period_text.strip(),
            'detail_url': detail_url,
            'portal': 'k_startup',
            'scraped_at': datetime.now().isoformat()
        }
    
    async def extract_api_data(self) -> List[Dict]:
        """API ì—”ë“œí¬ì¸íŠ¸ í™œìš© (ê°€ëŠ¥í•œ ê²½ìš°)"""
        
        # ê³µê°œ API í™•ì¸
        api_endpoints = [
            '/api/v1/business/announcements',
            '/api/v1/programs/list',
            '/api/v1/education/courses'
        ]
        
        api_data = []
        
        for endpoint in api_endpoints:
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(f"{self.api_base}{endpoint}") as response:
                        if response.status == 200:
                            data = await response.json()
                            api_data.extend(data.get('data', []))
            except Exception as e:
                print(f"API ì ‘ê·¼ ì‹¤íŒ¨ {endpoint}: {e}")
                continue
        
        return api_data
```

---

## ğŸ¤– **AI ê¸°ë°˜ ì½˜í…ì¸  ë¶„ì„ ì „ëµ**

### ğŸ“Š **íŒ¨í„´ ì¸ì‹ ì‹œìŠ¤í…œ**
```python
class KoreanPortalPatternAnalyzer:
    """í•œêµ­ í¬í„¸ íŠ¹í™” íŒ¨í„´ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.korean_patterns = {
            # í•œêµ­ íŠ¹í™” í‚¤ì›Œë“œ íŒ¨í„´
            'support_types': [
                'ìœµì', 'ì§€ì›ê¸ˆ', 'ë³´ì¡°ê¸ˆ', 'ì„¸ì•¡ê³µì œ', 'ë°”ìš°ì²˜',
                'ë©˜í† ë§', 'êµìœ¡', 'ì»¨ì„¤íŒ…', 'ê³µê°„ì œê³µ', 'ë„¤íŠ¸ì›Œí‚¹'
            ],
            'target_keywords': [
                'ì¤‘ì†Œê¸°ì—…', 'ìŠ¤íƒ€íŠ¸ì—…', 'ì°½ì—…ê¸°ì—…', 'ì†Œìƒê³µì¸',
                'ì˜ˆë¹„ì°½ì—…ì', 'ì²­ë…„ì°½ì—…', 'ì—¬ì„±ì°½ì—…', 'ì‹œë‹ˆì–´ì°½ì—…'
            ],
            'document_types': [
                'ì‚¬ì—…ê³„íšì„œ', 'ì‹ ì²­ì„œ', 'ì œì•ˆì„œ', 'ê³„íšì„œ',
                'ë³´ê³ ì„œ', 'ì¦ë¹™ì„œë¥˜', 'ì²¨ë¶€ì„œë¥˜'
            ],
            'evaluation_criteria': [
                'ì‚¬ì—…ì„±', 'ê¸°ìˆ ì„±', 'ì‹œì¥ì„±', 'ì‹¤í˜„ê°€ëŠ¥ì„±',
                'í˜ì‹ ì„±', 'ì°¨ë³„ì„±', 'ì§€ì†ê°€ëŠ¥ì„±', 'í™•ì¥ì„±'
            ]
        }
    
    async def analyze_korean_program_structure(self, program_data: Dict) -> Dict:
        """í•œêµ­ ì§€ì›ì‚¬ì—… êµ¬ì¡° ë¶„ì„"""
        
        analysis_prompt = f"""
        ë‹¤ìŒ í•œêµ­ ì •ë¶€ ì§€ì›ì‚¬ì—…ì„ ë¶„ì„í•˜ì—¬ í‘œì¤€í™”ëœ êµ¬ì¡°ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:
        
        í”„ë¡œê·¸ë¨ ì •ë³´:
        - ì œëª©: {program_data.get('title', '')}
        - ì£¼ê´€ê¸°ê´€: {program_data.get('agency', '')}
        - ì§€ì›ë‚´ìš©: {program_data.get('support_details', '')}
        - ëŒ€ìƒ: {program_data.get('target_audience', '')}
        
        í•œêµ­ ì •ë¶€ ì§€ì›ì‚¬ì—…ì˜ ì¼ë°˜ì ì¸ íŒ¨í„´ì„ ê³ ë ¤í•˜ì—¬ ë‹¤ìŒì„ ë¶„ì„í•´ì£¼ì„¸ìš”:
        
        1. ì§€ì›ì‚¬ì—… ìœ í˜• ë¶„ë¥˜:
           - ìê¸ˆì§€ì›í˜• (ìœµì, ë³´ì¡°ê¸ˆ, íˆ¬ì)
           - ì„œë¹„ìŠ¤ì§€ì›í˜• (ì»¨ì„¤íŒ…, êµìœ¡, ë©˜í† ë§)  
           - ì¸í”„ë¼ì§€ì›í˜• (ê³µê°„, ì¥ë¹„, ë„¤íŠ¸ì›Œí¬)
           
        2. ì‹ ì²­ ë³µì¡ë„ (1-5ì ):
           - 1ì : ê°„ë‹¨í•œ ì‹ ì²­ì„œë§Œ í•„ìš”
           - 3ì : ì‚¬ì—…ê³„íšì„œ + ì¦ë¹™ì„œë¥˜
           - 5ì : ìƒì„¸í•œ ê¸°ìˆ /ì‚¬ì—…ê³„íšì„œ + ë©´ì ‘/ë°œí‘œ
           
        3. í•„ìˆ˜ ì‹ ì²­ì„œë¥˜ ì˜ˆì¸¡:
           - ê¸°ë³¸ì„œë¥˜ (ì‹ ì²­ì„œ, ì‚¬ì—…ìë“±ë¡ì¦ ë“±)
           - í•µì‹¬ì„œë¥˜ (ì‚¬ì—…ê³„íšì„œ, ê¸°ìˆ ê°œë°œê³„íšì„œ ë“±)
           - ì¦ë¹™ì„œë¥˜ (ì¬ë¬´ì œí‘œ, íŠ¹í—ˆì¦, ì¶”ì²œì„œ ë“±)
           
        4. í‰ê°€ê¸°ì¤€ ê°€ì¤‘ì¹˜ ì˜ˆì¸¡:
           - ì‚¬ì—…ì„±/ì‹œì¥ì„± (%)
           - ê¸°ìˆ ì„±/í˜ì‹ ì„± (%)
           - ì‹¤í–‰ì—­ëŸ‰ (%)
           - ê¸°íƒ€ (%)
           
        5. ìœ ì‚¬ í”„ë¡œê·¸ë¨ê³¼ì˜ ì°¨ë³„ì 
        
        JSON í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”í•˜ì—¬ ë°˜í™˜í•´ì£¼ì„¸ìš”.
        """
        
        # ì‹¤ì œ ìš´ì˜ì‹œ AI ëª¨ë¸ í˜¸ì¶œ
        analysis_result = await self.call_ai_analysis(analysis_prompt)
        
        return {
            'program_id': program_data.get('program_id'),
            'analysis_result': analysis_result,
            'korean_specific_features': await self.identify_korean_features(program_data),
            'template_recommendations': await self.recommend_templates(analysis_result)
        }
    
    async def identify_korean_features(self, program_data: Dict) -> Dict:
        """í•œêµ­ íŠ¹í™” ìš”ì†Œ ì‹ë³„"""
        
        korean_features = {
            'government_hierarchy': self.identify_government_level(program_data['agency']),
            'regional_focus': self.identify_regional_focus(program_data),
            'industry_specialization': self.identify_industry_focus(program_data),
            'startup_stage_focus': self.identify_startup_stage(program_data),
            'unique_requirements': self.identify_unique_korean_requirements(program_data)
        }
        
        return korean_features
    
    def identify_government_level(self, agency: str) -> str:
        """ì •ë¶€ ê¸°ê´€ ë ˆë²¨ ì‹ë³„"""
        
        if any(keyword in agency for keyword in ['ë¶€', 'ì²­', 'ì²˜']):
            return 'central_government'  # ì¤‘ì•™ì •ë¶€
        elif any(keyword in agency for keyword in ['ì‹œ', 'ë„', 'êµ°', 'êµ¬']):
            return 'local_government'   # ì§€ë°©ì •ë¶€  
        elif any(keyword in agency for keyword in ['ê³µë‹¨', 'ì§„í¥ì›', 'ì„¼í„°']):
            return 'public_agency'      # ê³µê³µê¸°ê´€
        else:
            return 'other'
```

---

## ğŸ“ˆ **ìŠ¤í¬ë˜í•‘ ì„±ëŠ¥ ë° ì•ˆì •ì„±**

### âš¡ **ì„±ëŠ¥ ìµœì í™” ì „ëµ**
```python
class PerformanceOptimizedScraper:
    """ì„±ëŠ¥ ìµœì í™”ëœ ìŠ¤í¬ë˜í¼"""
    
    def __init__(self):
        self.concurrent_limit = 5  # ë™ì‹œ ìš”ì²­ ì œí•œ
        self.retry_policy = RetryPolicy(max_attempts=3, backoff_factor=2)
        self.cache = TTLCache(maxsize=1000, ttl=3600)  # 1ì‹œê°„ ìºì‹œ
        
    async def scrape_with_performance_optimization(self, portal_configs: List[Dict]) -> Dict:
        """ì„±ëŠ¥ ìµœì í™”ëœ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"""
        
        # 1. ë™ì‹œì„± ì œì–´
        semaphore = asyncio.Semaphore(self.concurrent_limit)
        
        # 2. íƒœìŠ¤í¬ ìƒì„±
        tasks = []
        for config in portal_configs:
            task = self.scrape_single_portal_with_semaphore(semaphore, config)
            tasks.append(task)
        
        # 3. ë³‘ë ¬ ì‹¤í–‰ ë° ê²°ê³¼ ìˆ˜ì§‘
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # 4. ê²°ê³¼ ì •ë¦¬ ë° ì˜¤ë¥˜ ì²˜ë¦¬
        successful_results = []
        failed_results = []
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                failed_results.append({
                    'portal': portal_configs[i]['portal_id'],
                    'error': str(result)
                })
            else:
                successful_results.append(result)
        
        return {
            'successful_count': len(successful_results),
            'failed_count': len(failed_results),
            'total_programs_found': sum(len(r.get('programs', [])) for r in successful_results),
            'results': successful_results,
            'errors': failed_results,
            'execution_time': time.time() - start_time
        }
```

---

## ğŸš€ **êµ¬í˜„ ìš°ì„ ìˆœìœ„**

### ğŸ“… **1ë‹¨ê³„: ê¸°ì—…ë§ˆë‹¹ ìŠ¤í¬ë˜í¼ (1ì£¼)**
- [ ] ê¸°ë³¸ ìŠ¤í¬ë˜í•‘ ë¡œì§ êµ¬í˜„
- [ ] ë°ì´í„° êµ¬ì¡°í™” ë° ì €ì¥
- [ ] ì˜¤ë¥˜ ì²˜ë¦¬ ë° ì•ˆì •ì„± í™•ë³´
- [ ] ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ìµœì í™”

### ğŸ“… **2ë‹¨ê³„: K-Startup ìŠ¤í¬ë˜í¼ (1ì£¼)**  
- [ ] SPA ëŒ€ì‘ ë¸Œë¼ìš°ì € ìë™í™”
- [ ] API ì—”ë“œí¬ì¸íŠ¸ í™œìš© ê²€í† 
- [ ] ë™ì  ì½˜í…ì¸  ì²˜ë¦¬
- [ ] ë°ì´í„° í†µí•© ë° ì •ê·œí™”

### ğŸ“… **3ë‹¨ê³„: AI ë¶„ì„ ì‹œìŠ¤í…œ (1ì£¼)**
- [ ] í•œêµ­ íŠ¹í™” íŒ¨í„´ ë¶„ì„ê¸° êµ¬í˜„
- [ ] ìë™ í…œí”Œë¦¿ ìƒì„± ë¡œì§
- [ ] í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ
- [ ] ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

---

**ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸**: í•œêµ­ ì •ë¶€ í¬í„¸ì˜ íŠ¹ì„±ìƒ **ì •í˜•í™”ëœ êµ¬ì¡°**ì™€ **í‘œì¤€í™”ëœ ìš©ì–´**ë¥¼ í™œìš©í•˜ë©´ ë†’ì€ ì •í™•ë„ì˜ ìë™í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤. íŠ¹íˆ "ì‚¬ì—…ê³„íšì„œ", "ì‹ ì²­ì„œ", "ì§€ì›ë‚´ìš©" ë“±ì˜ í‚¤ì›Œë“œê°€ ì¼ê´€ë˜ê²Œ ì‚¬ìš©ë˜ì–´ íŒ¨í„´ ì¸ì‹ì— ë§¤ìš° ìœ ë¦¬í•©ë‹ˆë‹¤!

*ğŸ“ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì‹¤ì œ ìŠ¤í¬ë˜í•‘ ëª¨ë“ˆ ê°œë°œì— ì°©ìˆ˜í•˜ê² ìŠµë‹ˆë‹¤!*