#!/usr/bin/env python3
"""
ğŸ¤– AI Research Engine - ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„ ë° ìˆ˜ì§‘ ì „ëµ ìƒì„±
AI ëª¨ë¸ì„ í™œìš©í•œ ë™ì  ìŠ¤í¬ë˜í•‘ ì „ëµ ê°œë°œ ì—”ì§„

Author: HEAL7 Development Team
Version: 1.0.0
Date: 2025-08-29
"""

import asyncio
import json
import logging
import re
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from enum import Enum

import aiohttp
import asyncpg
from bs4 import BeautifulSoup, NavigableString
from urllib.parse import urljoin, urlparse


logger = logging.getLogger(__name__)


class StrategyConfidence(Enum):
    """ì „ëµ ì‹ ë¢°ë„ ìˆ˜ì¤€"""
    HIGH = "high"       # 95%+ ì‹ ë¢°ë„
    MEDIUM = "medium"   # 80-94% ì‹ ë¢°ë„
    LOW = "low"         # 60-79% ì‹ ë¢°ë„
    UNCERTAIN = "uncertain"  # 60% ë¯¸ë§Œ


@dataclass
class SiteAnalysisResult:
    """ì‚¬ì´íŠ¸ ë¶„ì„ ê²°ê³¼"""
    site_url: str
    site_title: str = ""
    site_type: str = ""  # 'government', 'news', 'board', 'listing'
    content_structure: Dict[str, Any] = None
    data_patterns: List[Dict] = None
    recommended_selectors: Dict[str, str] = None
    pagination_info: Dict[str, Any] = None
    confidence_score: float = 0.0
    analysis_timestamp: str = ""
    
    def __post_init__(self):
        if self.content_structure is None:
            self.content_structure = {}
        if self.data_patterns is None:
            self.data_patterns = []
        if self.recommended_selectors is None:
            self.recommended_selectors = {}
        if self.pagination_info is None:
            self.pagination_info = {}
        if not self.analysis_timestamp:
            self.analysis_timestamp = datetime.now().isoformat()


@dataclass
class CollectionStrategy:
    """ìˆ˜ì§‘ ì „ëµ ë°ì´í„° êµ¬ì¡°"""
    strategy_id: str
    site_id: str
    site_url: str
    
    # í•µì‹¬ ì„ íƒì
    selectors: Dict[str, str] = None
    
    # í˜ì´ì§€ë„¤ì´ì…˜ ì „ëµ
    pagination: Dict[str, Any] = None
    
    # ë°ì´í„° ì¶”ì¶œ ê·œì¹™
    extraction_rules: Dict[str, Any] = None
    
    # ì„±ëŠ¥ ì„¤ì •
    performance_config: Dict[str, Any] = None
    
    # ë©”íƒ€ë°ì´í„°
    confidence: StrategyConfidence = StrategyConfidence.UNCERTAIN
    created_by: str = "ai_research_engine"
    created_at: str = ""
    last_tested_at: str = ""
    success_rate: float = 0.0
    
    def __post_init__(self):
        if self.selectors is None:
            self.selectors = {}
        if self.pagination is None:
            self.pagination = {}
        if self.extraction_rules is None:
            self.extraction_rules = {}
        if self.performance_config is None:
            self.performance_config = {
                "delay_between_requests": 2.0,
                "max_concurrent": 3,
                "timeout": 30
            }
        if not self.created_at:
            self.created_at = datetime.now().isoformat()


class AIResearchEngine:
    """ğŸ¤– AI ê¸°ë°˜ ì›¹ì‚¬ì´íŠ¸ ë¶„ì„ ë° ì „ëµ ìƒì„± ì—”ì§„"""
    
    def __init__(self, gemini_api_key: str = None, claude_api_key: str = None):
        # API í‚¤ ê´€ë¦¬ìì—ì„œ ì‹¤ì œ í‚¤ ë¡œë“œ
        try:
            from ..config.api_keys_config import create_api_keys_manager
            self.api_manager = create_api_keys_manager()
            
            # ì‹¤ì œ API í‚¤ ì„¤ì • (ë§¤ê°œë³€ìˆ˜ â†’ ê´€ë¦¬ì â†’ None)
            self.gemini_api_key = (gemini_api_key or 
                                 self.api_manager.get_gemini_api_key())
            self.claude_api_key = (claude_api_key or 
                                 self.api_manager.get_claude_api_key())
            
            # API í‚¤ ìƒíƒœ ë¡œê¹…
            status = self.api_manager.get_api_status()
            if status['has_any_valid_keys']:
                logger.info("ğŸ”‘ ì‹¤ì œ API í‚¤ ë¡œë“œ ì™„ë£Œ")
            else:
                logger.warning("âš ï¸ ì‹¤ì œ API í‚¤ ì—†ìŒ - ë¡œì»¬ íŒ¨í„´ ëª¨ë“œë¡œ ì‹¤í–‰")
                
        except ImportError:
            # í´ë°±: ê¸°ë³¸ ì„¤ì •
            logger.warning("API í‚¤ ê´€ë¦¬ì ë¡œë“œ ì‹¤íŒ¨ - ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")
            self.gemini_api_key = gemini_api_key
            self.claude_api_key = claude_api_key
        
        # HTTP ì„¸ì…˜
        self.session = None
        
        # AI ëª¨ë¸ ë™ì  ì„¤ì • - ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤ì— ë”°ë¼
        self.ai_models = {}
        
        # Gemini Flash 2.0 (Primary)
        if self.gemini_api_key and self.gemini_api_key != "your-gemini-api-key":
            self.ai_models["primary"] = {
                "name": "gemini-2.0-flash-exp",
                "provider": "google",
                "api_key": self.gemini_api_key,
                "max_tokens": 8000,
                "temperature": 0.1,
                "enabled": True
            }
        
        # Claude (Fallback)  
        if self.claude_api_key and self.claude_api_key != "your-claude-api-key":
            model_key = "fallback" if "primary" in self.ai_models else "primary"
            self.ai_models[model_key] = {
                "name": "claude-3-sonnet",
                "provider": "anthropic", 
                "api_key": self.claude_api_key,
                "max_tokens": 4000,
                "temperature": 0.1,
                "enabled": True
            }
        
        # ë¡œì»¬ íŒ¨í„´ (Always available)
        self.ai_models["local"] = {
            "name": "pattern-based",
            "provider": "local",
            "api_key": None,
            "enabled": True
        }
        
        # í™œì„± ëª¨ë¸ ë¡œê¹…
        enabled_models = [k for k, v in self.ai_models.items() if v.get("enabled", False)]
        logger.info(f"ğŸ¤– í™œì„± AI ëª¨ë¸: {', '.join(enabled_models)}")
        
        # í˜„ì¬ í™œì„± ëª¨ë¸
        self.current_model = "primary"
        
        # ë¶„ì„ í…œí”Œë¦¿ë“¤
        self.analysis_prompts = {
            "structure_analysis": self._get_structure_analysis_prompt(),
            "selector_generation": self._get_selector_generation_prompt(),
            "pagination_analysis": self._get_pagination_analysis_prompt()
        }
        
        # ì‚¬ì´íŠ¸ íƒ€ì…ë³„ íŒ¨í„´
        self.site_patterns = {
            "government_portal": {
                "indicators": ["go.kr", "ì •ë¶€", "ê³µê³ ", "ì§€ì›ì‚¬ì—…", "ì‹ ì²­"],
                "common_selectors": {
                    "item_container": [".board-list tr", ".list-item", ".program-item"],
                    "title": [".title a", ".subject a", "td:first-child a"],
                    "agency": [".agency", ".organ", ".institution"]
                }
            },
            "news_board": {
                "indicators": ["ë‰´ìŠ¤", "ê¸°ì‚¬", "ë³´ë„ìë£Œ", "ì–¸ë¡ "],
                "common_selectors": {
                    "item_container": [".news-list li", ".article-list .item"],
                    "title": [".headline a", ".title a"],
                    "date": [".date", ".publish-date"]
                }
            }
        }
    
    async def initialize(self):
        """AI ì—°êµ¬ ì—”ì§„ ì´ˆê¸°í™”"""
        logger.info("ğŸ¤– AI Research Engine ì´ˆê¸°í™” ì‹œì‘")
        
        # HTTP ì„¸ì…˜ ìƒì„±
        connector = aiohttp.TCPConnector(limit=5, ttl_dns_cache=300)
        timeout = aiohttp.ClientTimeout(total=60)
        
        self.session = aiohttp.ClientSession(
            headers={
                'User-Agent': 'HEAL7-AI-Research-Bot/1.0 (+https://heal7.com/ai-research)',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive'
            },
            connector=connector,
            timeout=timeout
        )
        
        logger.info("âœ… AI Research Engine ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.session:
            await self.session.close()
        logger.info("ğŸ›‘ AI Research Engine ì¢…ë£Œ")
    
    async def analyze_website_structure(self, url: str) -> SiteAnalysisResult:
        """ğŸ” ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ì¢…í•© ë¶„ì„"""
        logger.info(f"ğŸ” ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„ ì‹œì‘: {url}")
        
        try:
            # 1. ê¸°ë³¸ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            html_content = await self._fetch_page_content(url)
            if not html_content:
                raise Exception("í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # 2. ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            soup = BeautifulSoup(html_content, 'html.parser')
            site_title = self._extract_site_title(soup)
            
            # 3. ì‚¬ì´íŠ¸ íƒ€ì… ì¶”ë¡ 
            site_type = self._infer_site_type(url, soup)
            
            # 4. ì»¨í…ì¸  êµ¬ì¡° ë¶„ì„
            content_structure = await self._analyze_content_structure(soup, url)
            
            # 5. ë°ì´í„° íŒ¨í„´ ì‹ë³„
            data_patterns = await self._identify_data_patterns(soup, site_type)
            
            # 6. AI ëª¨ë¸ë¡œ ì„ íƒì ìƒì„±
            recommended_selectors = await self._generate_ai_selectors(html_content, url, site_type)
            
            # 7. í˜ì´ì§€ë„¤ì´ì…˜ ë¶„ì„
            pagination_info = await self._analyze_pagination(soup, url)
            
            # 8. ì‹ ë¢°ë„ ê³„ì‚°
            confidence_score = self._calculate_analysis_confidence(
                content_structure, data_patterns, recommended_selectors
            )
            
            result = SiteAnalysisResult(
                site_url=url,
                site_title=site_title,
                site_type=site_type,
                content_structure=content_structure,
                data_patterns=data_patterns,
                recommended_selectors=recommended_selectors,
                pagination_info=pagination_info,
                confidence_score=confidence_score
            )
            
            logger.info(f"âœ… ì‚¬ì´íŠ¸ ë¶„ì„ ì™„ë£Œ: {url} (ì‹ ë¢°ë„: {confidence_score:.1%})")
            return result
            
        except Exception as e:
            logger.error(f"âŒ ì‚¬ì´íŠ¸ ë¶„ì„ ì‹¤íŒ¨ {url}: {str(e)}")
            # ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ ë°˜í™˜
            return SiteAnalysisResult(
                site_url=url,
                confidence_score=0.0,
                content_structure={"error": str(e)}
            )
    
    async def generate_collection_strategy(
        self, 
        analysis_result: SiteAnalysisResult,
        target_data_types: List[str] = None
    ) -> CollectionStrategy:
        """ğŸ“‹ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìˆ˜ì§‘ ì „ëµ ìƒì„±"""
        
        if target_data_types is None:
            target_data_types = ["title", "agency", "category", "date", "url"]
        
        logger.info(f"ğŸ“‹ ìˆ˜ì§‘ ì „ëµ ìƒì„±: {analysis_result.site_url}")
        
        try:
            # 1. ê¸°ë³¸ ì„ íƒì ë§µí•‘
            selectors = self._map_target_selectors(
                analysis_result.recommended_selectors, 
                target_data_types
            )
            
            # 2. í˜ì´ì§€ë„¤ì´ì…˜ ì „ëµ
            pagination = self._create_pagination_strategy(analysis_result.pagination_info)
            
            # 3. ì¶”ì¶œ ê·œì¹™ ìƒì„±
            extraction_rules = self._create_extraction_rules(
                analysis_result.data_patterns,
                target_data_types
            )
            
            # 4. ì„±ëŠ¥ ìµœì í™” ì„¤ì •
            performance_config = self._optimize_performance_config(analysis_result.site_type)
            
            # 5. ì‹ ë¢°ë„ ë“±ê¸‰ ê²°ì •
            confidence = self._determine_confidence_level(analysis_result.confidence_score)
            
            # 6. ì „ëµ ID ìƒì„±
            strategy_id = self._generate_strategy_id(analysis_result.site_url)
            site_id = self._extract_site_id(analysis_result.site_url)
            
            strategy = CollectionStrategy(
                strategy_id=strategy_id,
                site_id=site_id,
                site_url=analysis_result.site_url,
                selectors=selectors,
                pagination=pagination,
                extraction_rules=extraction_rules,
                performance_config=performance_config,
                confidence=confidence
            )
            
            logger.info(f"âœ… ì „ëµ ìƒì„± ì™„ë£Œ: {strategy_id} (ì‹ ë¢°ë„: {confidence.value})")
            return strategy
            
        except Exception as e:
            logger.error(f"âŒ ì „ëµ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            # ê¸°ë³¸ ì „ëµ ë°˜í™˜
            return CollectionStrategy(
                strategy_id=f"fallback_{int(datetime.now().timestamp())}",
                site_id=self._extract_site_id(analysis_result.site_url),
                site_url=analysis_result.site_url,
                confidence=StrategyConfidence.UNCERTAIN
            )
    
    async def _fetch_page_content(self, url: str) -> str:
        """í˜ì´ì§€ ì»¨í…ì¸  ê°€ì ¸ì˜¤ê¸°"""
        try:
            async with self.session.get(url) as response:
                if response.status == 200:
                    return await response.text()
                else:
                    logger.warning(f"HTTP {response.status}: {url}")
                    return ""
        except Exception as e:
            logger.error(f"í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ {url}: {str(e)}")
            return ""
    
    def _extract_site_title(self, soup: BeautifulSoup) -> str:
        """ì‚¬ì´íŠ¸ ì œëª© ì¶”ì¶œ"""
        title_element = soup.find('title')
        if title_element:
            return title_element.get_text(strip=True)
        
        # ëŒ€ì•ˆ ì œëª© ì¶”ì¶œ
        for selector in ['h1', '.site-title', '.logo', '.brand']:
            element = soup.select_one(selector)
            if element:
                return element.get_text(strip=True)
        
        return "Unknown Site"
    
    def _infer_site_type(self, url: str, soup: BeautifulSoup) -> str:
        """ì‚¬ì´íŠ¸ íƒ€ì… ì¶”ë¡ """
        url_lower = url.lower()
        page_text = soup.get_text().lower()
        
        # ì •ë¶€ í¬í„¸ íŒ¨í„´
        if ('go.kr' in url_lower or 
            any(keyword in page_text for keyword in ['ì •ë¶€', 'ê³µê³ ', 'ì§€ì›ì‚¬ì—…', 'ì‹ ì²­ê¸°ê°„'])):
            return "government_portal"
        
        # ë‰´ìŠ¤ ì‚¬ì´íŠ¸ íŒ¨í„´
        if (any(keyword in url_lower for keyword in ['news', 'press']) or
            any(keyword in page_text for keyword in ['ë‰´ìŠ¤', 'ê¸°ì‚¬', 'ë³´ë„ìë£Œ'])):
            return "news_board"
        
        # ì¼ë°˜ ê²Œì‹œíŒ íŒ¨í„´
        if soup.find('table') or soup.select('.board-list, .list-board'):
            return "bulletin_board"
        
        # ëª©ë¡í˜• ì‚¬ì´íŠ¸
        if soup.select('ul li, ol li') and len(soup.select('ul li, ol li')) > 10:
            return "listing_site"
        
        return "unknown"
    
    async def _analyze_content_structure(self, soup: BeautifulSoup, url: str) -> Dict[str, Any]:
        """ì»¨í…ì¸  êµ¬ì¡° ë¶„ì„"""
        structure = {}
        
        # í…Œì´ë¸” êµ¬ì¡° ë¶„ì„
        tables = soup.find_all('table')
        if tables:
            structure['has_tables'] = True
            structure['table_count'] = len(tables)
            structure['main_table_headers'] = []
            
            # ë©”ì¸ í…Œì´ë¸”ì˜ í—¤ë” ì¶”ì¶œ
            main_table = max(tables, key=lambda t: len(t.find_all('tr')))
            headers = main_table.find_all(['th', 'td'])[:5]  # ì²« 5ê°œ í—¤ë”ë§Œ
            structure['main_table_headers'] = [h.get_text(strip=True) for h in headers]
        
        # ë¦¬ìŠ¤íŠ¸ êµ¬ì¡° ë¶„ì„
        lists = soup.select('ul, ol')
        if lists:
            structure['has_lists'] = True
            structure['list_count'] = len(lists)
            
            # ê°€ì¥ ê¸´ ë¦¬ìŠ¤íŠ¸ ë¶„ì„
            longest_list = max(lists, key=lambda l: len(l.find_all('li')))
            structure['max_list_items'] = len(longest_list.find_all('li'))
        
        # ì¹´ë“œ/ì•„ì´í…œ êµ¬ì¡° ë¶„ì„
        item_selectors = ['.item', '.card', '.post', '.entry', '.program']
        for selector in item_selectors:
            items = soup.select(selector)
            if items and len(items) >= 3:  # ìµœì†Œ 3ê°œ ì´ìƒ
                structure['has_items'] = True
                structure['item_selector'] = selector
                structure['item_count'] = len(items)
                break
        
        # í¼ êµ¬ì¡° ë¶„ì„
        forms = soup.find_all('form')
        if forms:
            structure['has_forms'] = True
            structure['form_count'] = len(forms)
        
        return structure
    
    async def _identify_data_patterns(self, soup: BeautifulSoup, site_type: str) -> List[Dict]:
        """ë°ì´í„° íŒ¨í„´ ì‹ë³„"""
        patterns = []
        
        # ì‚¬ì´íŠ¸ íƒ€ì…ë³„ íŒ¨í„´ ì‹ë³„
        if site_type == "government_portal":
            patterns.extend(self._identify_government_patterns(soup))
        elif site_type == "news_board":
            patterns.extend(self._identify_news_patterns(soup))
        elif site_type == "bulletin_board":
            patterns.extend(self._identify_board_patterns(soup))
        
        # ê³µí†µ íŒ¨í„´ ì‹ë³„
        patterns.extend(self._identify_common_patterns(soup))
        
        return patterns
    
    def _identify_government_patterns(self, soup: BeautifulSoup) -> List[Dict]:
        """ì •ë¶€ í¬í„¸ íŒ¨í„´ ì‹ë³„"""
        patterns = []
        
        # ê³µê³  ì œëª© íŒ¨í„´
        title_patterns = []
        for selector in ['.title a', '.subject a', 'td:first-child a']:
            elements = soup.select(selector)
            if len(elements) >= 3:
                sample_texts = [e.get_text(strip=True) for e in elements[:3]]
                if any('ì§€ì›' in text or 'ì‚¬ì—…' in text or 'ê³µê³ ' in text for text in sample_texts):
                    title_patterns.append({
                        'selector': selector,
                        'sample_count': len(elements),
                        'confidence': 0.8,
                        'samples': sample_texts
                    })
        
        if title_patterns:
            patterns.append({
                'type': 'government_titles',
                'patterns': title_patterns
            })
        
        # ê¸°ê´€ëª… íŒ¨í„´
        agency_patterns = []
        for selector in ['.agency', '.organ', '.institution', 'td:nth-child(3)']:
            elements = soup.select(selector)
            if len(elements) >= 3:
                sample_texts = [e.get_text(strip=True) for e in elements[:3]]
                if any('ë¶€' in text or 'ì²­' in text or 'ì›' in text for text in sample_texts):
                    agency_patterns.append({
                        'selector': selector,
                        'sample_count': len(elements),
                        'confidence': 0.7,
                        'samples': sample_texts
                    })
        
        if agency_patterns:
            patterns.append({
                'type': 'government_agencies',
                'patterns': agency_patterns
            })
        
        return patterns
    
    def _identify_news_patterns(self, soup: BeautifulSoup) -> List[Dict]:
        """ë‰´ìŠ¤ ì‚¬ì´íŠ¸ íŒ¨í„´ ì‹ë³„"""
        patterns = []
        
        # ë‰´ìŠ¤ í—¤ë“œë¼ì¸ íŒ¨í„´ (êµ¬í˜„ í•„ìš”)
        # ë‚ ì§œ íŒ¨í„´ (êµ¬í˜„ í•„ìš”)
        
        return patterns
    
    def _identify_board_patterns(self, soup: BeautifulSoup) -> List[Dict]:
        """ê²Œì‹œíŒ íŒ¨í„´ ì‹ë³„"""
        patterns = []
        
        # ê²Œì‹œíŒ íŠ¹í™” íŒ¨í„´ (êµ¬í˜„ í•„ìš”)
        
        return patterns
    
    def _identify_common_patterns(self, soup: BeautifulSoup) -> List[Dict]:
        """ê³µí†µ íŒ¨í„´ ì‹ë³„"""
        patterns = []
        
        # ë§í¬ íŒ¨í„´
        links = soup.find_all('a', href=True)
        if len(links) >= 5:
            patterns.append({
                'type': 'links',
                'count': len(links),
                'internal_links': len([l for l in links if not l['href'].startswith('http')]),
                'external_links': len([l for l in links if l['href'].startswith('http')])
            })
        
        # ë‚ ì§œ íŒ¨í„´
        date_patterns = re.findall(r'\d{4}[-/.]\d{1,2}[-/.]\d{1,2}', soup.get_text())
        if date_patterns:
            patterns.append({
                'type': 'dates',
                'count': len(date_patterns),
                'samples': date_patterns[:3]
            })
        
        return patterns
    
    async def _generate_ai_selectors(self, html_content: str, url: str, site_type: str) -> Dict[str, str]:
        """AI ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì„ íƒì ìƒì„±"""
        try:
            # HTMLì„ ë¶„ì„ ê°€ëŠ¥í•œ í¬ê¸°ë¡œ ì¶•ì•½
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ë¶ˆí•„ìš”í•œ ìŠ¤í¬ë¦½íŠ¸/ìŠ¤íƒ€ì¼ ì œê±°
            for tag in soup(['script', 'style', 'meta', 'link']):
                tag.decompose()
            
            # ì£¼ìš” ì»¨í…ì¸  ì˜ì—­ë§Œ ì¶”ì¶œ (ì²˜ìŒ 5000ì)
            clean_html = str(soup)[:5000]
            
            # AI í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._create_selector_prompt(clean_html, url, site_type)
            
            # AI ëª¨ë¸ í˜¸ì¶œ (ì‹¤ì œ API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜)
            ai_response = await self._call_ai_model(prompt)
            
            # AI ì‘ë‹µ íŒŒì‹±
            selectors = self._parse_ai_selector_response(ai_response)
            
            logger.info(f"AI ì„ íƒì ìƒì„± ì™„ë£Œ: {len(selectors)}ê°œ ì„ íƒì")
            return selectors
            
        except Exception as e:
            logger.error(f"AI ì„ íƒì ìƒì„± ì‹¤íŒ¨: {str(e)}")
            # ê¸°ë³¸ ì„ íƒì ë°˜í™˜
            return self._get_fallback_selectors(site_type)
    
    def _create_selector_prompt(self, html_content: str, url: str, site_type: str) -> str:
        """AI ëª¨ë¸ìš© ì„ íƒì ìƒì„± í”„ë¡¬í”„íŠ¸"""
        return f"""
ì›¹í˜ì´ì§€ êµ¬ì¡° ë¶„ì„ ë° ë°ì´í„° ì¶”ì¶œ ì„ íƒì ìƒì„± ìš”ì²­

URL: {url}
ì‚¬ì´íŠ¸ íƒ€ì…: {site_type}

HTML êµ¬ì¡°:
```html
{html_content}
```

ìš”ì²­ì‚¬í•­:
1. ì´ ì›¹í˜ì´ì§€ì—ì„œ ë°˜ë³µì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ë°ì´í„° í•­ëª©ë“¤ì„ ì‹ë³„
2. ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆëŠ” CSS ì„ íƒìë¥¼ ìƒì„±:
   - title: ì œëª©/í—¤ë“œë¼ì¸
   - agency: ê¸°ê´€ëª…/ë°œí–‰ì²˜
   - category: ì¹´í…Œê³ ë¦¬/ë¶„ë¥˜
   - date: ë‚ ì§œ/ì‹œê°„
   - url: ìƒì„¸ í˜ì´ì§€ ë§í¬
   - content: ë‚´ìš©/ìš”ì•½

3. ê° ì„ íƒìëŠ” ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì´ê³  ì•ˆì •ì ì´ì–´ì•¼ í•¨
4. ì—¬ëŸ¬ ê°œì˜ ëŒ€ì•ˆ ì„ íƒìë¥¼ ì œì‹œ (ìš°ì„ ìˆœìœ„ ìˆœ)

ì‘ë‹µ í˜•ì‹ (JSON):
{{
    "title": ["selector1", "selector2", "selector3"],
    "agency": ["selector1", "selector2"],
    "category": ["selector1"],
    "date": ["selector1", "selector2"],
    "url": ["selector1"],
    "content": ["selector1"],
    "container": ["selector1", "selector2"],
    "confidence": 0.85,
    "notes": "ë¶„ì„ ì°¸ê³ ì‚¬í•­"
}}
"""
    
    async def _call_ai_model(self, prompt: str, model_preference: str = "primary") -> str:
        """ğŸ¤– AI ëª¨ë¸ API í˜¸ì¶œ (Gemini Flash 2.0 ìš°ì„ , í´ë°± ì§€ì›)"""
        
        # ëª¨ë¸ ìš°ì„ ìˆœìœ„ ì„¤ì •
        models_to_try = []
        if model_preference == "primary":
            models_to_try = ["primary", "fallback", "local"]
        elif model_preference == "fallback":
            models_to_try = ["fallback", "local"]
        else:
            models_to_try = ["local"]
        
        last_error = None
        
        for model_key in models_to_try:
            try:
                model_config = self.ai_models[model_key]
                logger.info(f"ğŸ¤– AI ëª¨ë¸ ì‹œë„: {model_config['name']} ({model_config['provider']})")
                
                if model_config["provider"] == "google":
                    return await self._call_gemini_model(prompt, model_config)
                elif model_config["provider"] == "anthropic":
                    return await self._call_claude_model(prompt, model_config)
                elif model_config["provider"] == "local":
                    return await self._call_local_pattern_model(prompt)
                    
            except Exception as e:
                last_error = e
                logger.warning(f"âŒ {model_config['name']} ëª¨ë¸ ì‹¤íŒ¨: {str(e)}")
                continue
        
        # ëª¨ë“  ëª¨ë¸ ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ë°œìƒ
        raise Exception(f"ëª¨ë“  AI ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨. ë§ˆì§€ë§‰ ì˜¤ë¥˜: {last_error}")
    
    async def _call_gemini_model(self, prompt: str, model_config: Dict) -> str:
        """ğŸš€ Gemini Flash 2.0 API í˜¸ì¶œ"""
        try:
            # Gemini API ì—”ë“œí¬ì¸íŠ¸
            api_url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_config['name']}:generateContent"
            
            headers = {
                "Content-Type": "application/json",
                "x-goog-api-key": model_config["api_key"]
            }
            
            payload = {
                "contents": [{
                    "parts": [{
                        "text": prompt
                    }]
                }],
                "generationConfig": {
                    "temperature": model_config["temperature"],
                    "maxOutputTokens": model_config["max_tokens"],
                    "candidateCount": 1
                }
            }
            
            async with self.session.post(api_url, json=payload, headers=headers) as response:
                if response.status == 200:
                    result = await response.json()
                    
                    # Gemini ì‘ë‹µ êµ¬ì¡°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    if "candidates" in result and len(result["candidates"]) > 0:
                        candidate = result["candidates"][0]
                        if "content" in candidate and "parts" in candidate["content"]:
                            text_content = candidate["content"]["parts"][0].get("text", "")
                            logger.info("âœ… Gemini Flash 2.0 ì‘ë‹µ ì„±ê³µ")
                            return text_content
                    
                    raise Exception("Gemini ì‘ë‹µ êµ¬ì¡° íŒŒì‹± ì‹¤íŒ¨")
                else:
                    error_text = await response.text()
                    raise Exception(f"Gemini API ì˜¤ë¥˜ {response.status}: {error_text}")
                    
        except Exception as e:
            logger.error(f"âŒ Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            raise
    
    async def _call_claude_model(self, prompt: str, model_config: Dict) -> str:
        """ğŸ”„ Claude API í˜¸ì¶œ (í´ë°±)"""
        try:
            # Claude API ì—”ë“œí¬ì¸íŠ¸
            api_url = "https://api.anthropic.com/v1/messages"
            
            headers = {
                "Content-Type": "application/json",
                "x-api-key": model_config["api_key"],
                "anthropic-version": "2023-06-01"
            }
            
            payload = {
                "model": model_config["name"],
                "max_tokens": model_config["max_tokens"],
                "temperature": model_config["temperature"],
                "messages": [{
                    "role": "user",
                    "content": prompt
                }]
            }
            
            async with self.session.post(api_url, json=payload, headers=headers) as response:
                if response.status == 200:
                    result = await response.json()
                    
                    # Claude ì‘ë‹µ êµ¬ì¡°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    if "content" in result and len(result["content"]) > 0:
                        text_content = result["content"][0].get("text", "")
                        logger.info("âœ… Claude API ì‘ë‹µ ì„±ê³µ (í´ë°±)")
                        return text_content
                    
                    raise Exception("Claude ì‘ë‹µ êµ¬ì¡° íŒŒì‹± ì‹¤íŒ¨")
                else:
                    error_text = await response.text()
                    raise Exception(f"Claude API ì˜¤ë¥˜ {response.status}: {error_text}")
                    
        except Exception as e:
            logger.error(f"âŒ Claude API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            raise
    
    async def _call_local_pattern_model(self, prompt: str) -> str:
        """ğŸ  ë¡œì»¬ íŒ¨í„´ ê¸°ë°˜ ëª¨ë¸ (ìµœì¢… í´ë°±)"""
        logger.info("ğŸ  ë¡œì»¬ íŒ¨í„´ ê¸°ë°˜ ë¶„ì„ ì‹¤í–‰")
        
        # ê°„ë‹¨í•œ íŒ¨í„´ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
        # ì‹¤ì œë¡œëŠ” HTML êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ íŒ¨í„´ ë§¤ì¹­
        local_response = {
            "title": [".title a", ".subject a", "td:first-child a", "h1 a", "h2 a"],
            "agency": [".agency", ".organ", ".institution", ".publisher", ".author"],
            "category": [".category", ".type", ".ë¶„ë¥˜", ".classification"],
            "date": [".date", ".regDate", ".ë“±ë¡ì¼", ".created", ".published"],
            "url": [".title a", ".subject a", "a[href*='view']"],
            "content": [".content", ".summary", ".ìš”ì•½", ".description"],
            "container": ["table tbody tr", ".list-item", ".board-item", "li", ".item"],
            "confidence": 0.60,
            "notes": "ë¡œì»¬ íŒ¨í„´ ê¸°ë°˜ ë¶„ì„ ê²°ê³¼ (í´ë°± ëª¨ë“œ)"
        }
        
        return json.dumps(local_response, ensure_ascii=False)
    
    def _parse_ai_selector_response(self, ai_response: str) -> Dict[str, str]:
        """AI ì‘ë‹µì—ì„œ ì„ íƒì íŒŒì‹±"""
        try:
            data = json.loads(ai_response)
            
            # ê° í•„ë“œì˜ ì²« ë²ˆì§¸ ì„ íƒìë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©
            selectors = {}
            for field, selector_list in data.items():
                if field not in ['confidence', 'notes'] and isinstance(selector_list, list):
                    selectors[field] = selector_list[0] if selector_list else ""
            
            return selectors
            
        except Exception as e:
            logger.error(f"AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            return {}
    
    def _get_fallback_selectors(self, site_type: str) -> Dict[str, str]:
        """ê¸°ë³¸ ì„ íƒì ë°˜í™˜"""
        if site_type == "government_portal":
            return {
                "title": ".title a",
                "agency": ".agency",
                "category": ".category",
                "date": ".date",
                "url": ".title a",
                "container": "table tbody tr"
            }
        else:
            return {
                "title": "a",
                "container": "li, tr"
            }
    
    async def _analyze_pagination(self, soup: BeautifulSoup, url: str) -> Dict[str, Any]:
        """í˜ì´ì§€ë„¤ì´ì…˜ ë¶„ì„"""
        pagination_info = {
            "has_pagination": False,
            "method": "none",
            "selectors": {},
            "max_pages": 1
        }
        
        # í˜ì´ì§€ ë²ˆí˜¸ ë§í¬ ì°¾ê¸°
        page_links = soup.select('a[href*="page"], a[href*="pageNum"], a[href*="currentPage"]')
        if page_links:
            pagination_info["has_pagination"] = True
            pagination_info["method"] = "link_based"
            pagination_info["selectors"]["next_page"] = "a:contains('ë‹¤ìŒ'), a:contains('Next'), a:contains('>')"
            
            # ìµœëŒ€ í˜ì´ì§€ ìˆ˜ ì¶”ì •
            try:
                page_numbers = []
                for link in page_links:
                    text = link.get_text(strip=True)
                    if text.isdigit():
                        page_numbers.append(int(text))
                
                if page_numbers:
                    pagination_info["max_pages"] = max(page_numbers)
            except:
                pass
        
        # ë²„íŠ¼ ê¸°ë°˜ í˜ì´ì§€ë„¤ì´ì…˜
        next_buttons = soup.select('button:contains("ë‹¤ìŒ"), input[value*="ë‹¤ìŒ"], .next, .page-next')
        if next_buttons and not page_links:
            pagination_info["has_pagination"] = True
            pagination_info["method"] = "button_based"
            pagination_info["selectors"]["next_button"] = ".next, .page-next, button:contains('ë‹¤ìŒ')"
        
        return pagination_info
    
    def _calculate_analysis_confidence(
        self, 
        content_structure: Dict, 
        data_patterns: List[Dict], 
        selectors: Dict[str, str]
    ) -> float:
        """ë¶„ì„ ì‹ ë¢°ë„ ê³„ì‚°"""
        confidence = 0.0
        
        # ì»¨í…ì¸  êµ¬ì¡° ì ìˆ˜ (40%)
        structure_score = 0.0
        if content_structure.get('has_tables'):
            structure_score += 0.3
        if content_structure.get('has_lists'):
            structure_score += 0.2
        if content_structure.get('has_items'):
            structure_score += 0.3
        
        # ë°ì´í„° íŒ¨í„´ ì ìˆ˜ (30%)
        pattern_score = min(len(data_patterns) * 0.1, 0.3)
        
        # ì„ íƒì í’ˆì§ˆ ì ìˆ˜ (30%)
        selector_score = min(len(selectors) * 0.05, 0.3)
        
        confidence = structure_score + pattern_score + selector_score
        return min(confidence, 1.0)
    
    # ì „ëµ ìƒì„± ê´€ë ¨ ë©”ì†Œë“œë“¤
    
    def _map_target_selectors(self, ai_selectors: Dict[str, str], target_types: List[str]) -> Dict[str, str]:
        """ëª©í‘œ ë°ì´í„° íƒ€ì…ì— ë§ëŠ” ì„ íƒì ë§¤í•‘"""
        mapped = {}
        for target in target_types:
            if target in ai_selectors:
                mapped[target] = ai_selectors[target]
        return mapped
    
    def _create_pagination_strategy(self, pagination_info: Dict[str, Any]) -> Dict[str, Any]:
        """í˜ì´ì§€ë„¤ì´ì…˜ ì „ëµ ìƒì„±"""
        if not pagination_info.get('has_pagination'):
            return {"enabled": False}
        
        return {
            "enabled": True,
            "method": pagination_info.get('method', 'link_based'),
            "selectors": pagination_info.get('selectors', {}),
            "max_pages": pagination_info.get('max_pages', 10),
            "delay_between_pages": 3.0
        }
    
    def _create_extraction_rules(self, data_patterns: List[Dict], target_types: List[str]) -> Dict[str, Any]:
        """ë°ì´í„° ì¶”ì¶œ ê·œì¹™ ìƒì„±"""
        rules = {
            "text_cleaning": {
                "strip_whitespace": True,
                "remove_newlines": True,
                "normalize_spaces": True
            },
            "validation": {
                "min_title_length": 5,
                "max_title_length": 200,
                "required_fields": ["title"]
            },
            "post_processing": {
                "url_resolution": "absolute",
                "date_parsing": "auto",
                "text_encoding": "utf-8"
            }
        }
        
        return rules
    
    def _optimize_performance_config(self, site_type: str) -> Dict[str, Any]:
        """ì‚¬ì´íŠ¸ íƒ€ì…ë³„ ì„±ëŠ¥ ìµœì í™” ì„¤ì •"""
        base_config = {
            "delay_between_requests": 2.0,
            "max_concurrent": 3,
            "timeout": 30,
            "max_retries": 2
        }
        
        # ì •ë¶€ ì‚¬ì´íŠ¸ëŠ” ë” ë³´ìˆ˜ì ìœ¼ë¡œ
        if site_type == "government_portal":
            base_config["delay_between_requests"] = 3.0
            base_config["max_concurrent"] = 2
        
        return base_config
    
    def _determine_confidence_level(self, confidence_score: float) -> StrategyConfidence:
        """ì‹ ë¢°ë„ ì ìˆ˜ë¥¼ ë“±ê¸‰ìœ¼ë¡œ ë³€í™˜"""
        if confidence_score >= 0.95:
            return StrategyConfidence.HIGH
        elif confidence_score >= 0.80:
            return StrategyConfidence.MEDIUM
        elif confidence_score >= 0.60:
            return StrategyConfidence.LOW
        else:
            return StrategyConfidence.UNCERTAIN
    
    def _generate_strategy_id(self, url: str) -> str:
        """ì „ëµ ID ìƒì„±"""
        import hashlib
        url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
        timestamp = int(datetime.now().timestamp())
        return f"strategy_{url_hash}_{timestamp}"
    
    def _extract_site_id(self, url: str) -> str:
        """URLì—ì„œ ì‚¬ì´íŠ¸ ID ì¶”ì¶œ"""
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        
        # ì„œë¸Œë„ë©”ì¸ ì œê±°
        domain_parts = domain.split('.')
        if len(domain_parts) >= 2:
            return '_'.join(domain_parts[-2:]).replace('.', '_')
        
        return domain.replace('.', '_')
    
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë©”ì†Œë“œë“¤
    
    def _get_structure_analysis_prompt(self) -> str:
        """êµ¬ì¡° ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿"""
        return """
ì›¹í˜ì´ì§€ì˜ HTML êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

1. ì£¼ìš” ì»¨í…ì¸  ì˜ì—­ ì‹ë³„
2. ë°ì´í„° ë¦¬ìŠ¤íŠ¸ êµ¬ì¡° (í…Œì´ë¸”, ëª©ë¡, ì¹´ë“œ ë“±)
3. ë°˜ë³µë˜ëŠ” ì•„ì´í…œ íŒ¨í„´
4. ë„¤ë¹„ê²Œì´ì…˜ ë° í˜ì´ì§€ë„¤ì´ì…˜ êµ¬ì¡°

ë¶„ì„ ëŒ€ìƒ HTML:
{html_content}

ì‘ë‹µì€ êµ¬ì¡°í™”ëœ JSON í˜•íƒœë¡œ ì œê³µí•´ì£¼ì„¸ìš”.
"""
    
    def _get_selector_generation_prompt(self) -> str:
        """ì„ íƒì ìƒì„±ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿"""
        return """
ì œê³µëœ HTML êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ë°ì´í„° ì¶”ì¶œì„ ìœ„í•œ ìµœì ì˜ CSS ì„ íƒìë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ëª©í‘œ ë°ì´í„°:
- ì œëª©/í—¤ë“œë¼ì¸
- ê¸°ê´€ëª…/ë°œí–‰ì²˜  
- ì¹´í…Œê³ ë¦¬/ë¶„ë¥˜
- ë‚ ì§œ
- ë§í¬ URL
- ë‚´ìš©/ìš”ì•½

ê° ì„ íƒìëŠ” ì•ˆì •ì ì´ê³  êµ¬ì²´ì ì´ì–´ì•¼ í•˜ë©°, ì‚¬ì´íŠ¸ ë³€ê²½ì— ëŒ€í•œ ë‚´ì„±ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
"""
    
    def _get_pagination_analysis_prompt(self) -> str:
        """í˜ì´ì§€ë„¤ì´ì…˜ ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿"""
        return """
ì›¹í˜ì´ì§€ì˜ í˜ì´ì§€ë„¤ì´ì…˜ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

1. í˜ì´ì§€ë„¤ì´ì…˜ ì¡´ì¬ ì—¬ë¶€
2. í˜ì´ì§€ë„¤ì´ì…˜ ë°©ì‹ (ë§í¬, ë²„íŠ¼, AJAX ë“±)
3. ë‹¤ìŒ í˜ì´ì§€ ì´ë™ ë°©ë²•
4. ì „ì²´ í˜ì´ì§€ ìˆ˜ (ì¶”ì • ê°€ëŠ¥í•œ ê²½ìš°)

ë¶„ì„ ê²°ê³¼ë¥¼ JSON í˜•íƒœë¡œ ì œê³µí•´ì£¼ì„¸ìš”.
"""


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤

def create_ai_research_engine(ai_api_key: str = None) -> AIResearchEngine:
    """AI ì—°êµ¬ ì—”ì§„ ìƒì„± ë° ì´ˆê¸°í™”"""
    engine = AIResearchEngine(ai_api_key)
    return engine


async def analyze_site_and_generate_strategy(
    url: str, 
    target_data_types: List[str] = None,
    ai_api_key: str = None
) -> Tuple[SiteAnalysisResult, CollectionStrategy]:
    """ì‚¬ì´íŠ¸ ë¶„ì„ ë° ì „ëµ ìƒì„± ì›ìŠ¤í†± í•¨ìˆ˜"""
    engine = create_ai_research_engine(ai_api_key)
    
    try:
        await engine.initialize()
        
        # 1. ì‚¬ì´íŠ¸ ë¶„ì„
        analysis_result = await engine.analyze_website_structure(url)
        
        # 2. ì „ëµ ìƒì„±
        strategy = await engine.generate_collection_strategy(analysis_result, target_data_types)
        
        return analysis_result, strategy
        
    finally:
        await engine.close()


# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜
async def test_ai_research_engine():
    """AI ì—°êµ¬ ì—”ì§„ í…ŒìŠ¤íŠ¸"""
    test_urls = [
        "https://www.bizinfo.go.kr/web/lay1/bbs/S1T122C128/AS/list.do",
        "https://www.k-startup.go.kr/web/contents/bizListPage.do"
    ]
    
    for url in test_urls:
        logger.info(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹œì‘: {url}")
        
        try:
            analysis, strategy = await analyze_site_and_generate_strategy(url)
            
            logger.info(f"âœ… ë¶„ì„ ì™„ë£Œ")
            logger.info(f"   - ì‚¬ì´íŠ¸ íƒ€ì…: {analysis.site_type}")
            logger.info(f"   - ì‹ ë¢°ë„: {analysis.confidence_score:.1%}")
            logger.info(f"   - ì¶”ì²œ ì„ íƒì: {len(analysis.recommended_selectors)}ê°œ")
            logger.info(f"   - ì „ëµ ì‹ ë¢°ë„: {strategy.confidence.value}")
            
        except Exception as e:
            logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")


if __name__ == "__main__":
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_ai_research_engine())