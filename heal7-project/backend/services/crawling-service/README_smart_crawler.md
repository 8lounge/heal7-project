# ğŸ§  HEAL7 Smart Crawler System

3-Tier ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì‹œìŠ¤í…œê³¼ ë©€í‹°ëª¨ë‹¬ AI ë¶„ì„ í†µí•© ì†”ë£¨ì…˜

## ğŸ¯ ì£¼ìš” íŠ¹ì§•

### ğŸ•·ï¸ 3-Tier í¬ë¡¤ë§ ì•„í‚¤í…ì²˜
- **Tier 1**: `httpx` - ë¹ ë¥¸ ì •ì  ì‚¬ì´íŠ¸ ë° API
- **Tier 2**: `playwright` - ë™ì  ì½˜í…ì¸  ë° JavaScript ë Œë”ë§
- **Tier 3**: `selenium + undetected` - Anti-bot ìš°íšŒ ë° ìµœëŒ€ í˜¸í™˜ì„±

### ğŸ¤– ë©€í‹°ëª¨ë‹¬ AI ë¶„ì„
- **Gemini Flash** (1ìˆœìœ„): ë¬´ë£Œ, ë¹ ë¦„
- **GPT-4o** (2ìˆœìœ„): ê°•ë ¥í•œ ë¹„ì „ ëŠ¥ë ¥
- **Claude Sonnet** (3ìˆœìœ„): ë¬¸ì„œ ì´í•´ íŠ¹í™”

### ğŸ§  ì§€ëŠ¥í˜• ê¸°ëŠ¥
- ìë™ í´ë°± ì‹œìŠ¤í…œ
- ë„ë©”ì¸ë³„ ìµœì  í¬ë¡¤ëŸ¬ í•™ìŠµ
- ì„±ëŠ¥ ê¸°ë°˜ ìš°ì„ ìˆœìœ„ ì¡°ì •
- ì‹¤ì‹œê°„ ìƒíƒœ ëª¨ë‹ˆí„°ë§

## ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
crawling-service/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ crawlers/
â”‚   â”‚   â”œâ”€â”€ base_crawler.py          # í¬ë¡¤ëŸ¬ ê¸°ë³¸ í´ë˜ìŠ¤
â”‚   â”‚   â”œâ”€â”€ httpx_crawler.py         # Tier 1: httpx
â”‚   â”‚   â”œâ”€â”€ playwright_crawler.py    # Tier 2: Playwright
â”‚   â”‚   â””â”€â”€ selenium_crawler.py      # Tier 3: Selenium
â”‚   â””â”€â”€ smart_crawler.py             # í†µí•© ì§€ëŠ¥í˜• í¬ë¡¤ëŸ¬
â”œâ”€â”€ multimodal/
â”‚   â”œâ”€â”€ ai_analyzer.py               # ë©€í‹°ëª¨ë‹¬ AI ë¶„ì„
â”‚   â””â”€â”€ document_processor.py        # ë¬¸ì„œ ì²˜ë¦¬
â”œâ”€â”€ demo_smart_crawler.py            # ë°ëª¨ ë° í…ŒìŠ¤íŠ¸
â””â”€â”€ requirements.txt                 # ì˜ì¡´ì„±
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. ì˜ì¡´ì„± ì„¤ì¹˜
```bash
pip install -r requirements.txt

# Playwright ë¸Œë¼ìš°ì € ì„¤ì¹˜
playwright install chromium
```

### 2. API í‚¤ ì„¤ì • (ì„ íƒì‚¬í•­)
```bash
export GEMINI_API_KEY="your-gemini-api-key"
export OPENAI_API_KEY="your-openai-api-key"  
export ANTHROPIC_API_KEY="your-anthropic-api-key"
```

### 3. ë°ëª¨ ì‹¤í–‰
```bash
python demo_smart_crawler.py
```

## ğŸ’» ê¸°ë³¸ ì‚¬ìš©ë²•

### ë‹¨ì¼ URL í¬ë¡¤ë§
```python
from core.smart_crawler import SmartCrawler, CrawlStrategy

# í¬ë¡¤ëŸ¬ ìƒì„± ë° ì´ˆê¸°í™”
crawler = SmartCrawler()
await crawler.initialize()

# ìë™ ì „ëµìœ¼ë¡œ í¬ë¡¤ë§
result = await crawler.crawl("https://example.com", strategy=CrawlStrategy.AUTO)

if result.success:
    print(f"ì„±ê³µ! í¬ë¡¤ëŸ¬: {result.crawler_used.value}")
    print(f"HTML í¬ê¸°: {len(result.html)} bytes")
else:
    print(f"ì‹¤íŒ¨: {result.error}")

await crawler.cleanup()
```

### ë°°ì¹˜ í¬ë¡¤ë§
```python
urls = [
    "https://api.github.com/users/octocat",
    "https://httpbin.org/json",
    "https://example.com"
]

results = await crawler.batch_crawl(urls, max_concurrent=3)

for result in results:
    print(f"{result.url}: {'âœ…' if result.success else 'âŒ'}")
```

### ì „ëµë³„ í¬ë¡¤ë§
```python
# ë¹ ë¥¸ í¬ë¡¤ë§ (httpx ìš°ì„ )
result = await crawler.crawl(url, strategy=CrawlStrategy.FAST)

# ë Œë”ë§ í•„ìš” (playwright ìš°ì„ )  
result = await crawler.crawl(url, strategy=CrawlStrategy.RENDER)

# ìŠ¤í…”ìŠ¤ ëª¨ë“œ (selenium ìš°ì„ )
result = await crawler.crawl(url, strategy=CrawlStrategy.STEALTH)
```

## ğŸ¨ ë©€í‹°ëª¨ë‹¬ AI ë¶„ì„

### ì´ë¯¸ì§€ ë¶„ì„
```python
from multimodal.ai_analyzer import MultimodalAnalyzer

analyzer = MultimodalAnalyzer()
await analyzer.initialize()

# ìŠ¤í¬ë¦°ìƒ· ë¶„ì„
result = await analyzer.analyze_webpage_screenshot("screenshot.png")
print(result['content'])

# í…Œì´ë¸” ì¶”ì¶œ
table_data = await analyzer.extract_table_from_image("table_image.png")
print(table_data['parsed_tables'])
```

### ë¬¸ì„œ ì²˜ë¦¬
```python
from multimodal.document_processor import DocumentProcessor

processor = DocumentProcessor()
await processor.initialize()

# PDF ë¬¸ì„œ ì²˜ë¦¬
result = await processor.process_document("document.pdf", include_ai_analysis=True)

if result.success:
    print(f"í…ìŠ¤íŠ¸ ë¸”ë¡: {len(result.text_content)}")
    print(f"í…Œì´ë¸”: {len(result.tables)}")
    print(f"AI ìš”ì•½: {result.ai_summary}")
```

## ğŸ”§ ê³ ê¸‰ ê¸°ëŠ¥

### í¬ë¡¤ëŸ¬ ì„¤ì •
```python
from core.crawlers import CrawlConfig

config = CrawlConfig(
    url="https://example.com",
    timeout=30,
    screenshot=True,
    wait_for_load=True,
    stealth_mode=True
)

result = await crawler.crawl(config)
```

### ìƒíƒœ ëª¨ë‹ˆí„°ë§
```python
# ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
health = await crawler.health_check()
print(f"ì‹œìŠ¤í…œ ìƒíƒœ: {health['system_healthy']}")

# í¬ë¡¤ëŸ¬ë³„ í†µê³„
for crawler_type in crawler.crawlers:
    stats = crawler.get_crawler_stats(crawler_type)
    print(f"{crawler_type.value}: {stats['success_rate']:.1%} ì„±ê³µë¥ ")

# ì„±ëŠ¥ ìµœì í™”
crawler.optimize_performance()
```

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### í¬ë¡¤ëŸ¬ë³„ íŠ¹ì„±
| í¬ë¡¤ëŸ¬ | ì†ë„ | JavaScript | Anti-bot | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ |
|--------|------|------------|----------|---------------|
| httpx | â­â­â­â­â­ | âŒ | âŒ | ë‚®ìŒ |
| playwright | â­â­â­ | âœ… | â­â­ | ì¤‘ê°„ |
| selenium | â­â­ | âœ… | â­â­â­â­â­ | ë†’ìŒ |

### AI ëª¨ë¸ë³„ íŠ¹ì„±
| ëª¨ë¸ | ì†ë„ | ë¹„ìš© | ì •í™•ë„ | íŠ¹í™” ë¶„ì•¼ |
|------|------|------|--------|-----------|
| Gemini Flash | â­â­â­â­â­ | ë¬´ë£Œ | â­â­â­â­ | ë²”ìš© |
| GPT-4o | â­â­â­ | ìœ ë£Œ | â­â­â­â­â­ | ì´ë¯¸ì§€/ì°¨íŠ¸ |
| Claude Sonnet | â­â­â­â­ | ìœ ë£Œ | â­â­â­â­â­ | ë¬¸ì„œ/í…Œì´ë¸” |

## ğŸ” íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì¼ë°˜ì ì¸ ë¬¸ì œ

**Q: í¬ë¡¤ë§ì´ ë„ˆë¬´ ëŠë ¤ìš”**
```python
# ë¹ ë¥¸ ì „ëµ ì‚¬ìš©
result = await crawler.crawl(url, strategy=CrawlStrategy.FAST)

# ë˜ëŠ” httpx ì§ì ‘ ì‚¬ìš©
from core.crawlers import HttpxCrawler
httpx_crawler = HttpxCrawler()
await httpx_crawler.initialize()
result = await httpx_crawler.crawl(CrawlConfig(url=url))
```

**Q: Bot ê°ì§€ë¡œ ì°¨ë‹¨ë˜ì–´ìš”**
```python
# ìŠ¤í…”ìŠ¤ ì „ëµ ì‚¬ìš©
result = await crawler.crawl(url, strategy=CrawlStrategy.STEALTH)

# ë˜ëŠ” selenium ì§ì ‘ ì‚¬ìš©
from core.crawlers import SeleniumCrawler
selenium_crawler = SeleniumCrawler(use_undetected=True)
result = await selenium_crawler.solve_captcha(CrawlConfig(url=url))
```

**Q: JavaScript ë Œë”ë§ì´ ì•ˆ ë¼ìš”**
```python
# ë Œë”ë§ ì „ëµ ì‚¬ìš©
result = await crawler.crawl(url, strategy=CrawlStrategy.RENDER)

# ë˜ëŠ” ëŒ€ê¸° ì„¤ì • ì¶”ê°€
config = CrawlConfig(
    url=url,
    wait_for_load=True,
    wait_for_selector=".main-content"
)
```

### ë¡œê·¸ ë ˆë²¨ ì¡°ì •
```python
import logging
logging.getLogger('core.smart_crawler').setLevel(logging.DEBUG)
logging.getLogger('multimodal').setLevel(logging.INFO)
```

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™” íŒ

1. **ì ì ˆí•œ ì „ëµ ì„ íƒ**: APIëŠ” FAST, ë™ì  ì‚¬ì´íŠ¸ëŠ” RENDER
2. **ë°°ì¹˜ ì²˜ë¦¬ í™œìš©**: ëŒ€ëŸ‰ URLì€ `batch_crawl()` ì‚¬ìš©
3. **AI ë¶„ì„ ì„ íƒì  ì‚¬ìš©**: í•„ìš”í•œ ê²½ìš°ë§Œ AI ë¶„ì„ í™œì„±í™”
4. **ìºì‹œ í™œìš©**: ê°™ì€ ë„ë©”ì¸ì€ ìë™ìœ¼ë¡œ ìµœì  í¬ë¡¤ëŸ¬ í•™ìŠµ
5. **ë¦¬ì†ŒìŠ¤ ì •ë¦¬**: ì‚¬ìš© í›„ ë°˜ë“œì‹œ `cleanup()` í˜¸ì¶œ

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

1. ìƒˆë¡œìš´ í¬ë¡¤ëŸ¬ ì¶”ê°€
2. AI ëª¨ë¸ í†µí•©
3. ë¬¸ì„œ í˜•ì‹ ì§€ì› í™•ì¥
4. ì„±ëŠ¥ ìµœì í™”
5. í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¶”ê°€

## ğŸ“„ ë¼ì´ì„ ìŠ¤

MIT License - ììœ ë¡­ê²Œ ì‚¬ìš©í•˜ì„¸ìš”!

---

**ğŸ¯ HEAL7 Smart Crawler System** - ì°¨ì„¸ëŒ€ ì§€ëŠ¥í˜• ì›¹ í¬ë¡¤ë§ ì†”ë£¨ì…˜